{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SIMLAN, Simulation for Multi-Camera Robotics (5.1.5) This simulation environment, based on the Ignition Gazebo simulator and ROS 2, resembles a Volvo Trucks warehouse and serves as a playground for rapid prototyping and testing of systems that rely on a multi-camera setup for perception, monitoring, localization or even navigation. This project is inspired by GPSS (Generic photo-based sensor system) that utilizes ceiling-mounted cameras, deep learning and computer vision algorithms, and very simple transport robots. [ \ud83d\udcf9 GPSS demo ] \ud83d\udd17 Online Documentation \ud83d\udcc3 PDF Documentation SIMLAN Features Ignition Gazebo Library of assets Real-World environment inspired design (camera position and warehouse layout) ROS 2 interfaces (Humble and Jazzy) Simple GPSS (Generic Photo-based Sensor System) navigation Multi-Robot localization and navigation using Nav2 ArUco marker localization Inverse Perspective Mapping (Bird's-Eye view projection) Multi-Sensor Support (LiDAR, RGB camera, semantic segmentation, depth) Geofencing for safe zones and safe stop on collision Humanoid worker model Panda arm robotic arm Deep-learning-based human pose capture and replay \ud83d\udcf9 Click the YouTube link below to view the SIMLAN demo video: Installation [ \ud83d\udcf9 Demo ] Dependencies Ubuntu 24.04: use the instructions in dependencies.md#linux-dependencies to install Docker and ensure that your Linux user account has docker access. Attention : Make sure to restart the computer (for the changes in group membership to take effect) before proceeding to the next step. Windows 11: use the instructions in dependencies.md#windows-dependencies to install dependencies. Production environment : follow installation procedure used in .devcontainer/Dockerfile to install dependencies. Development environment : To improve collaboration we use VS Code and Docker as explained in this instruction and docker files . Install Visual Studio Code (VS Code) and open the project folder. VS Code will prompt you to install the required extension dependencies. Make sure the Dev containers extension is installed. Reopen the project in VS Code, and you will be prompted to rebuild the container. Accept the prompt; this process may take a few minutes. Once VS Code is connected to Docker (as shown in the image below), open the terminal and run the following commands: (if you don't see this try to build manually in VS Code by pressing Ctrl + Shift + P and select Dev Containers: Rebuild and Reopen in Container .) Quick Start The best place to learn about the various features, start different components, and understand the project structure is ./control.sh . Attention : The following commands (using ./control.sh ) are executed in a separate terminal tab inside VS Code. To kill all the relevant processes (related to Gazebo and ROS 2), delete build files, delete recorded images and rosbag files using the following command: ./control.sh clean To clean up and build the project: ./control.sh build (optionally, in VS Code you can click on Terminal-> Run Task/Run Build Task or use Ctrl + Shift + B ) GPSS controls (pallet trucks, aruco) [ \ud83d\udcf9 Demo ] It is possible for the cameras to detect ArUco markers on the floor and publish their location to TF, both relative to the camera, and the ArUcos transform from origin. The package ./camera_utility/aruco_localization contains the code for handling ArUco detection. You can also use Nav2 to make a robot_agent (that can be either robot/pallet_truck) navigate by itself to a goal position. You can find the code in simulation/pallet_truck/pallet_truck_navigation Run these three in separate terminals ./control.sh gpss # spawn the simulation, robot_agents and GPSS ArUco detection ./control.sh nav ROBOTS # spawn map server, and separate nav2 stack in a separate namespace for each robot_agent ./control.sh send_goal robot_agent_1 2.0 1.0 # send navigation goal to nav2 stack for robot_agent_1 camera_enabled_ids in config.sh specifies which cameras are enabled in the GPSS system for ArUco code detection and bird's-eye view (Inverse Perspective Mapping). ./control.sh birdeye Running the command above, the system publishes projected camera images to /static_agent/camera_X/image_projected and the final stitched output to the /projected_images_stitched topic that you can inspect using Rviz. RITA controls (humanoid, robotic arm) [ \ud83d\udcf9 Demo ] To spawn a human worker run the following command ./control.sh sim ./control.sh humanoid We employ a deep neural network to learn the mapping between human pose and humanoid robot motion. The model takes 33 MediaPipe pose landmarks as input and predicts corresponding robot joint positions. Read more about it Humanoid Utilities Arm controls Spawn the Panda arm inside SIMLAN and instruct it to pick and place a box around with the following commands: ./control sim ./control panda ./control plan_panda_motion Testing Integration tests can be found inside of the integration_tests/test/ package. Running the tests helps maintain the project's quality. For more information about how the tests are set up, check out the package README . To run all tests, run the following command: ./control.sh build ./control.sh test Customized startup In config.sh it is possible to customize your scenarios. From there you can edit what world you want to run, how many cameras you want enabled, and also edit Humanoid-related properties. Modifying these variables is preferred, rather than modifying the control.sh file. Headless gazebo In config.sh there is a variable headless_gazebo that you can set to \"true\" or \"false\". Setting it to true means will run without a window though the simulation will still run as normal. This can be useful when visualization is redundant. Setting it to false means the gazebo window will be visible. World fidelity in the config.sh script, you can adjust the world fidelity default : Contains the default world with maximum objects medium : Based on default but boxes are removed light : Based on medium but shelves are removed empty : Everything except the ground is removed Older versions gz_classic_humble branch contains code for Gazebo Classic (Gazebo11) that has reached end-of-life (EOL). ign_humble branch contains code for ROS 2 Humble & Ignition Gazebo , an earlier version of this repository. Documentation You can build the online documentation page or a PDF file by running scripts in resources/build-documentation . control.sh script is a shortcut to run different launch scripts, please also see these diagram . config.sh contains information about which world is loaded, which cameras are active, and what and where the robots are spawned. Marp Markdown Presentation Configuration Generation Bringup and launch files Pallet Truck Navigation Documentation Camera Utilities and notebooks : ( Extrinsic/Intrinsic calibrations and Projection ) Humanoid Utilities (pose2motion) CHANGELOG.md credits.md LICENSE (apache 2) contributing.md ISSUES.md : known issues and advanced features simulation/ : ROS2 packages Simulation and Warehouse Specification (fidelity) Camera and Birdeye Configuration Building Gazebo models (Blender/Phobos) Objects Specifications Warehouse Specification Pallet truck bringup Aruco Localization Documentation Geofencing and Collision safe stop Visualize Real Data requires data from Volvo Humanoid bringup humanoid_robot simulation Humanoid Control Research Funding This work was carried out within these research projects: The SMILE IV project financed by Vinnova, FFI, Fordonsstrategisk forskning och innovation under the grant number 2023-00789. The EUREKA ITEA4 ArtWork - The smart and connected worker financed by Vinnova under the grant number 2023-00970. INFOTIV AB Dyno-robotics RISE Research Institutes of Sweden CHALMERS Volvo Group SIMLAN project was started and is currently maintained by Hamid Ebadi . To see a complete list of contributors see the changelog .","title":"SIMLAN, Vision-Based Multi-Camera Robotics Simulation"},{"location":"#simlan-simulation-for-multi-camera-robotics-515","text":"This simulation environment, based on the Ignition Gazebo simulator and ROS 2, resembles a Volvo Trucks warehouse and serves as a playground for rapid prototyping and testing of systems that rely on a multi-camera setup for perception, monitoring, localization or even navigation. This project is inspired by GPSS (Generic photo-based sensor system) that utilizes ceiling-mounted cameras, deep learning and computer vision algorithms, and very simple transport robots. [ \ud83d\udcf9 GPSS demo ] \ud83d\udd17 Online Documentation \ud83d\udcc3 PDF Documentation","title":"SIMLAN, Simulation for Multi-Camera Robotics (5.1.5)"},{"location":"#simlan-features","text":"Ignition Gazebo Library of assets Real-World environment inspired design (camera position and warehouse layout) ROS 2 interfaces (Humble and Jazzy) Simple GPSS (Generic Photo-based Sensor System) navigation Multi-Robot localization and navigation using Nav2 ArUco marker localization Inverse Perspective Mapping (Bird's-Eye view projection) Multi-Sensor Support (LiDAR, RGB camera, semantic segmentation, depth) Geofencing for safe zones and safe stop on collision Humanoid worker model Panda arm robotic arm Deep-learning-based human pose capture and replay \ud83d\udcf9 Click the YouTube link below to view the SIMLAN demo video:","title":"SIMLAN Features"},{"location":"#installation-demo","text":"","title":"Installation [\ud83d\udcf9 Demo]"},{"location":"#dependencies","text":"Ubuntu 24.04: use the instructions in dependencies.md#linux-dependencies to install Docker and ensure that your Linux user account has docker access. Attention : Make sure to restart the computer (for the changes in group membership to take effect) before proceeding to the next step. Windows 11: use the instructions in dependencies.md#windows-dependencies to install dependencies. Production environment : follow installation procedure used in .devcontainer/Dockerfile to install dependencies. Development environment : To improve collaboration we use VS Code and Docker as explained in this instruction and docker files . Install Visual Studio Code (VS Code) and open the project folder. VS Code will prompt you to install the required extension dependencies. Make sure the Dev containers extension is installed. Reopen the project in VS Code, and you will be prompted to rebuild the container. Accept the prompt; this process may take a few minutes. Once VS Code is connected to Docker (as shown in the image below), open the terminal and run the following commands: (if you don't see this try to build manually in VS Code by pressing Ctrl + Shift + P and select Dev Containers: Rebuild and Reopen in Container .)","title":"Dependencies"},{"location":"#quick-start","text":"The best place to learn about the various features, start different components, and understand the project structure is ./control.sh . Attention : The following commands (using ./control.sh ) are executed in a separate terminal tab inside VS Code. To kill all the relevant processes (related to Gazebo and ROS 2), delete build files, delete recorded images and rosbag files using the following command: ./control.sh clean To clean up and build the project: ./control.sh build (optionally, in VS Code you can click on Terminal-> Run Task/Run Build Task or use Ctrl + Shift + B )","title":"Quick Start"},{"location":"#gpss-controls-pallet-trucks-aruco-demo","text":"It is possible for the cameras to detect ArUco markers on the floor and publish their location to TF, both relative to the camera, and the ArUcos transform from origin. The package ./camera_utility/aruco_localization contains the code for handling ArUco detection. You can also use Nav2 to make a robot_agent (that can be either robot/pallet_truck) navigate by itself to a goal position. You can find the code in simulation/pallet_truck/pallet_truck_navigation Run these three in separate terminals ./control.sh gpss # spawn the simulation, robot_agents and GPSS ArUco detection ./control.sh nav ROBOTS # spawn map server, and separate nav2 stack in a separate namespace for each robot_agent ./control.sh send_goal robot_agent_1 2.0 1.0 # send navigation goal to nav2 stack for robot_agent_1 camera_enabled_ids in config.sh specifies which cameras are enabled in the GPSS system for ArUco code detection and bird's-eye view (Inverse Perspective Mapping). ./control.sh birdeye Running the command above, the system publishes projected camera images to /static_agent/camera_X/image_projected and the final stitched output to the /projected_images_stitched topic that you can inspect using Rviz.","title":"GPSS controls (pallet trucks, aruco) [\ud83d\udcf9 Demo]"},{"location":"#rita-controls-humanoid-robotic-arm-demo","text":"To spawn a human worker run the following command ./control.sh sim ./control.sh humanoid We employ a deep neural network to learn the mapping between human pose and humanoid robot motion. The model takes 33 MediaPipe pose landmarks as input and predicts corresponding robot joint positions. Read more about it Humanoid Utilities","title":"RITA controls (humanoid, robotic arm) [\ud83d\udcf9 Demo]"},{"location":"#arm-controls","text":"Spawn the Panda arm inside SIMLAN and instruct it to pick and place a box around with the following commands: ./control sim ./control panda ./control plan_panda_motion","title":"Arm controls"},{"location":"#testing","text":"Integration tests can be found inside of the integration_tests/test/ package. Running the tests helps maintain the project's quality. For more information about how the tests are set up, check out the package README . To run all tests, run the following command: ./control.sh build ./control.sh test","title":"Testing"},{"location":"#customized-startup","text":"In config.sh it is possible to customize your scenarios. From there you can edit what world you want to run, how many cameras you want enabled, and also edit Humanoid-related properties. Modifying these variables is preferred, rather than modifying the control.sh file.","title":"Customized startup"},{"location":"#headless-gazebo","text":"In config.sh there is a variable headless_gazebo that you can set to \"true\" or \"false\". Setting it to true means will run without a window though the simulation will still run as normal. This can be useful when visualization is redundant. Setting it to false means the gazebo window will be visible.","title":"Headless gazebo"},{"location":"#world-fidelity","text":"in the config.sh script, you can adjust the world fidelity default : Contains the default world with maximum objects medium : Based on default but boxes are removed light : Based on medium but shelves are removed empty : Everything except the ground is removed","title":"World fidelity"},{"location":"#older-versions","text":"gz_classic_humble branch contains code for Gazebo Classic (Gazebo11) that has reached end-of-life (EOL). ign_humble branch contains code for ROS 2 Humble & Ignition Gazebo , an earlier version of this repository.","title":"Older versions"},{"location":"#documentation","text":"You can build the online documentation page or a PDF file by running scripts in resources/build-documentation . control.sh script is a shortcut to run different launch scripts, please also see these diagram . config.sh contains information about which world is loaded, which cameras are active, and what and where the robots are spawned. Marp Markdown Presentation Configuration Generation Bringup and launch files Pallet Truck Navigation Documentation Camera Utilities and notebooks : ( Extrinsic/Intrinsic calibrations and Projection ) Humanoid Utilities (pose2motion) CHANGELOG.md credits.md LICENSE (apache 2) contributing.md ISSUES.md : known issues and advanced features simulation/ : ROS2 packages Simulation and Warehouse Specification (fidelity) Camera and Birdeye Configuration Building Gazebo models (Blender/Phobos) Objects Specifications Warehouse Specification Pallet truck bringup Aruco Localization Documentation Geofencing and Collision safe stop Visualize Real Data requires data from Volvo Humanoid bringup humanoid_robot simulation Humanoid Control","title":"Documentation"},{"location":"#research-funding","text":"This work was carried out within these research projects: The SMILE IV project financed by Vinnova, FFI, Fordonsstrategisk forskning och innovation under the grant number 2023-00789. The EUREKA ITEA4 ArtWork - The smart and connected worker financed by Vinnova under the grant number 2023-00970. INFOTIV AB Dyno-robotics RISE Research Institutes of Sweden CHALMERS Volvo Group SIMLAN project was started and is currently maintained by Hamid Ebadi . To see a complete list of contributors see the changelog .","title":"Research Funding"},{"location":"CHANGELOG/","text":"Changelog v5.1.5: ur10, refactor: Team/Tech Lead: Hamid Ebadi add simulation real time factor to config : Hamid Ebadi introducing UR10 & Robotiq gripper: Markus and Elias (experimental) RITA models: Markus and Elias (experimental) refactor control.sh : Hamid Ebadi refactor build scripts : Hamid Ebadi add dynorobotics documentation : Hamid Ebadi 5.1.4: Code cleanup (Feb 2026) - Sebastian Olsson (Dyno Robotics) Remove SPAWN_JACKAL variable. Use ./control.sh jackal to spawn the jackal instead. Move replay_data directory inside the simulation/ folder. Example trajectory and images inside replay_data to test the visualize_real_data package. Add static transform for visualizing images and markers while running prepare.launch.py. Integration test for scenario_manager. Remove jackal dependency in pallet_truck. Remove emojis and weird symbols from scenario_manager README. 5.1.3: Time filtering and initial heading for replaying data (Jan 2026) - Sebastian Olsson (Dyno Robotics) Retrieve code that was pushed on feature/scenario-replay. Able to filter out timestamps to process in the .json files with start_time and end_time parameters. Set initial heading for the orientation_faker.py with initial_heading parameter. More clear listing of all .json files in replay_data/ as a table. 5.1.2: Port Dyno Robotics features to Jazzy (Jan 2026) - Sebastian Olsson (Dyno Robotics) Add SPAWN_JACKAL variable in config.sh to spawn jackal (can also be spawned with ./control.sh jackal ). Re-add jackal to ros_dependencies.repos so it does not require to clone the repo inside the simulation folder. Remove scenario_execution from ros_dependencies.repos, install with apt in Dockerfile instead. Re-add pytrees in ros_dependencies.repos. Update version to 2.3.x. Add command in control.sh to run scenario_manager and run a collision case. Add drop_nuke.py file which runs on ./control.sh kill due to kill command previously not killing all ros-related ghost processes. Add ground_truth odom publisher to pallet_trucks (used by scenario_manager package to determine speed and position of pallet_trucks). Add .gitignored replay_data folder so images and trajectories can be used more easily instead of saving inside the install/share directory which deleted all data if a rebuild was performed. Add ignore_orientation parameter when replaying a scenario. Add rviz_config variable in config.sh to easily change which rviz file the sim is started with. Minor docs/README update to scenario_manager and visualize_real_data package. 5.1.1: Cleanup and Documentation (Jan 2026) - Team/Tech Lead: Hamid Ebadi Headless gazebo: P\u00e4r Aronsson Major cleanup, restructuring and documentation: Hamid Ebadi Improve ml prediction pipeline: P\u00e4r Aronsson 5.0.0: Refactor MoCap pipeline and sensors (Dec 2025) - Team/Tech Lead: Hamid Ebadi Working demo video scenario: Hamid Ebadi, P\u00e4r Aronsson Refactor and improve prediction pipeline: P\u00e4r Aronsson Review and update the mocap architecture: P\u00e4r Aronsson, Hamid Ebadi Pipeline for prediction on real data, images, and videos: P\u00e4r Aronsson Improve motion player: P\u00e4r Aronsson New demo scenario: P\u00e4r Aronsson Unify \"single\" and \"multi\" pipeline: P\u00e4r Aronsson, Hamid Ebadi Documentation and presentation (mkdocs, pandas): Hamid Ebadi 4.0.0: Improvements (Nov 2025) - Team/Tech Lead: Hamid Ebadi System integration tests: P\u00e4r Aronsson Overall system diagrams (launch files and bringups): Anton Stigemyr Hill PyTorch model for humanoid MoCap: P\u00e4r Aronsson Improved documentation and AutoGluon configuration: Marwa Naili Demo scenario: Anton Stigemyr Hill Semantic segmentation, depth, color camera sensors: Anton Stigemyr Hill, P\u00e4r Aronsson Humanoid navigation: Anton Stigemyr Hill Rework GPSS/ArUco navigation: Anton Stigemyr Hill 3.0.0: Jazzy, humanoid (Oct 2025) - Team/Tech Lead: Hamid Ebadi Collision sensor: Anton Stigemyr Hill Automatically generated parameter-files for robot_agents and world fidelity: Anton Stigemyr Hill Shared map and obstacle generation for robot_agents: Anton Stigemyr Hill Geofencing and safety stop: David Espedalen Humanoid motion capture: Casparsson and Siyu Yi, Hamid Ebadi Humanoid integration (pymoveit2 to moveit_py): Siyu Yi, P\u00e4r Aronsson Mocap (humanoid) with multi-camera training pipeline: Siyu Yi, P\u00e4r Aronsson Panda arm integration: P\u00e4r Aronsson Dyno-robotics trajectory replay integration, scenario: Sebastian Olsson 2.1.0: Namespace cleanup and multi-agent navigation (Aug 2025) - Team/Tech Lead: Hamid Ebadi Implemented generic robot-agent control supporting multiple meshes (e.g., pallet truck, forklift) and distinct ArUco IDs: P\u00e4r Aronsson Robot-agent and navigation2 (nav2) namespace: P\u00e4r Aronsson Multi-agent navigation capabilities using GPSS: P\u00e4r Aronsson 2.0.0: GPSS (May 2025) - Team/Tech Lead: Hamid Ebadi Upgrade Gazebo from classic to ignition, adapting new sensors: Nazeeh Alhosary ArUco_localization, added localization and nav2 to new robot: P\u00e4r Aronsson Navigation: P\u00e4r Aronsson Bird's Eye View ros package: Converting projection.ipynb to camera_bird_eye_view: Hamid Ebadi, P\u00e4r Aronsson Deprecated: InfoBot, replaced with Pallet_truck (based on Jackal by Clearpath Robotics) 1.0.6: Collision (Feb 2025) Add scenario execution library for ros2 Add action servers for set_speed, teleport_robot and collision Add Node for TTC calculation Add package for custom messages 1.0.5: Delivery 4 (Dec 2024) D3.5 Disentanglement: One-parameter Object Movements. Trajectory visualization: Hamid Ebadi feat: added DynoWaitFor to make sure Jackal is launched last (synchronisation issue) simlan_bringup pkg created: Christoffer Johannesson 1.0.4: Delivery 3 (Oct 2024) SMILE-IV and ArtWork projects contributions Updated camera extrinsic, fixing OpenCV camera calibration to Gazebo simulator conversion: Erik Brorsson, Volvo D3.4 Out distribution data collection: Hamid Ebadi D3.3 In distribution data collection: Hamid Ebadi D3.2 Images for stitching: Hamid Ebadi D3.1 Dataset draft for HH: Hamid Ebadi CI/CD and Kubernetes integration by Filip Melberg, Vasiliki Kostara Jackal integration: Christoffer Johannesson, Hjalmar Ruscck Docker/vscode Disable GPU support by default POC for camera projection to pixel coordinates (Jupyter notebook): Hamid Ebadi Camera image dump and image stitching: Hamid Ebadi 1.0.3: Jackal Robot (Mar 2024) - Christoffer Johannesson Added dyno fork of jackal repo from Clearpath Robotics. Updated to Humble, added bringup and support for namespacing. Jackal can be spawned in Gazebo and controlled through the keyboard. Added .devcontainer folder with Dockerfile and devcontainer.json to set up project container in VS Code. Added docker-compose to link all needed files and set environment variables. Added .vscode folder with settings and tasks for easy building of the project. Updated README with info on how to use Docker setup in VS Code, and some features to make it easy to share the same setup with others. Features include: python3 dependency install with pip, cloning of other git repositories and how to make changes to those repositories. 1.0.2: Delivery 2 (Feb 2024) Volvo warehouse 0.0.1: Hamid Ebadi Volvo camera calibration in Gazebo 0.0.1: Hamid Ebadi Integrate Infobot_agent 0.0.2: InfoBot differential-drive AMR (Autonomous Mobile Robot) URDF and ROS launcher (GOPAL and forklift): Hamid Ebadi Integrate Infobot_cartographer 2.1.5: cartographer for creating PGM maps Integrate nav2_commander 0.0.2: ROS package to command Infobot where the destination is: Hamid Ebadi Integrate Infobot_navigation2 2.1.5: Standard Nav2 stack launcher: Hamid Ebadi Integrate Infobot_teleop 0.0.2: Teleoperation for InfoBot 1.0.1: Delivery 1 (Dec 2023) - Team/Tech Lead: Hamid Ebadi Basic warehouse model 1.0.0: Anders B\u00e4ckelie CAD modelling (eur-pallet, boxes, shelf, support_pole, traffic-cone, steel_drum) 1.0.0: Jacob Rohdin Physics (collision, inertia), visuals and Gazebo compatible mesh creation 1.0.0: Anders B\u00e4ckelie Walking actor using scripted trajectories 1.0.0: Anders B\u00e4ckelie Infobot_Gazebo_environment 1.0.0: ROS2 launcher to start Gazebo world: Hamid Ebadi static_agent_launcher 1.0.0: Camera and ArUco tags: Hamid Ebadi camera-viewer 1.0.0: Python code to get Gazebo camera feed: Hamid Ebadi","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"","title":"Changelog"},{"location":"CHANGELOG/#v515-ur10-refactor-teamtech-lead-hamid-ebadi","text":"add simulation real time factor to config : Hamid Ebadi introducing UR10 & Robotiq gripper: Markus and Elias (experimental) RITA models: Markus and Elias (experimental) refactor control.sh : Hamid Ebadi refactor build scripts : Hamid Ebadi add dynorobotics documentation : Hamid Ebadi","title":"v5.1.5: ur10, refactor: Team/Tech Lead: Hamid Ebadi"},{"location":"CHANGELOG/#514-code-cleanup-feb-2026-sebastian-olsson-dyno-robotics","text":"Remove SPAWN_JACKAL variable. Use ./control.sh jackal to spawn the jackal instead. Move replay_data directory inside the simulation/ folder. Example trajectory and images inside replay_data to test the visualize_real_data package. Add static transform for visualizing images and markers while running prepare.launch.py. Integration test for scenario_manager. Remove jackal dependency in pallet_truck. Remove emojis and weird symbols from scenario_manager README.","title":"5.1.4: Code cleanup (Feb 2026) - Sebastian Olsson (Dyno Robotics)"},{"location":"CHANGELOG/#513-time-filtering-and-initial-heading-for-replaying-data-jan-2026-sebastian-olsson-dyno-robotics","text":"Retrieve code that was pushed on feature/scenario-replay. Able to filter out timestamps to process in the .json files with start_time and end_time parameters. Set initial heading for the orientation_faker.py with initial_heading parameter. More clear listing of all .json files in replay_data/ as a table.","title":"5.1.3: Time filtering and initial heading for replaying data (Jan 2026) - Sebastian Olsson (Dyno Robotics)"},{"location":"CHANGELOG/#512-port-dyno-robotics-features-to-jazzy-jan-2026-sebastian-olsson-dyno-robotics","text":"Add SPAWN_JACKAL variable in config.sh to spawn jackal (can also be spawned with ./control.sh jackal ). Re-add jackal to ros_dependencies.repos so it does not require to clone the repo inside the simulation folder. Remove scenario_execution from ros_dependencies.repos, install with apt in Dockerfile instead. Re-add pytrees in ros_dependencies.repos. Update version to 2.3.x. Add command in control.sh to run scenario_manager and run a collision case. Add drop_nuke.py file which runs on ./control.sh kill due to kill command previously not killing all ros-related ghost processes. Add ground_truth odom publisher to pallet_trucks (used by scenario_manager package to determine speed and position of pallet_trucks). Add .gitignored replay_data folder so images and trajectories can be used more easily instead of saving inside the install/share directory which deleted all data if a rebuild was performed. Add ignore_orientation parameter when replaying a scenario. Add rviz_config variable in config.sh to easily change which rviz file the sim is started with. Minor docs/README update to scenario_manager and visualize_real_data package.","title":"5.1.2: Port Dyno Robotics features to Jazzy (Jan 2026) - Sebastian Olsson (Dyno Robotics)"},{"location":"CHANGELOG/#511-cleanup-and-documentation-jan-2026-teamtech-lead-hamid-ebadi","text":"Headless gazebo: P\u00e4r Aronsson Major cleanup, restructuring and documentation: Hamid Ebadi Improve ml prediction pipeline: P\u00e4r Aronsson","title":"5.1.1: Cleanup and Documentation (Jan 2026) - Team/Tech Lead: Hamid Ebadi"},{"location":"CHANGELOG/#500-refactor-mocap-pipeline-and-sensors-dec-2025-teamtech-lead-hamid-ebadi","text":"Working demo video scenario: Hamid Ebadi, P\u00e4r Aronsson Refactor and improve prediction pipeline: P\u00e4r Aronsson Review and update the mocap architecture: P\u00e4r Aronsson, Hamid Ebadi Pipeline for prediction on real data, images, and videos: P\u00e4r Aronsson Improve motion player: P\u00e4r Aronsson New demo scenario: P\u00e4r Aronsson Unify \"single\" and \"multi\" pipeline: P\u00e4r Aronsson, Hamid Ebadi Documentation and presentation (mkdocs, pandas): Hamid Ebadi","title":"5.0.0: Refactor MoCap pipeline and sensors (Dec 2025) - Team/Tech Lead: Hamid Ebadi"},{"location":"CHANGELOG/#400-improvements-nov-2025-teamtech-lead-hamid-ebadi","text":"System integration tests: P\u00e4r Aronsson Overall system diagrams (launch files and bringups): Anton Stigemyr Hill PyTorch model for humanoid MoCap: P\u00e4r Aronsson Improved documentation and AutoGluon configuration: Marwa Naili Demo scenario: Anton Stigemyr Hill Semantic segmentation, depth, color camera sensors: Anton Stigemyr Hill, P\u00e4r Aronsson Humanoid navigation: Anton Stigemyr Hill Rework GPSS/ArUco navigation: Anton Stigemyr Hill","title":"4.0.0: Improvements (Nov 2025) - Team/Tech Lead: Hamid Ebadi"},{"location":"CHANGELOG/#300-jazzy-humanoid-oct-2025-teamtech-lead-hamid-ebadi","text":"Collision sensor: Anton Stigemyr Hill Automatically generated parameter-files for robot_agents and world fidelity: Anton Stigemyr Hill Shared map and obstacle generation for robot_agents: Anton Stigemyr Hill Geofencing and safety stop: David Espedalen Humanoid motion capture: Casparsson and Siyu Yi, Hamid Ebadi Humanoid integration (pymoveit2 to moveit_py): Siyu Yi, P\u00e4r Aronsson Mocap (humanoid) with multi-camera training pipeline: Siyu Yi, P\u00e4r Aronsson Panda arm integration: P\u00e4r Aronsson Dyno-robotics trajectory replay integration, scenario: Sebastian Olsson","title":"3.0.0: Jazzy, humanoid (Oct 2025) - Team/Tech Lead: Hamid Ebadi"},{"location":"CHANGELOG/#210-namespace-cleanup-and-multi-agent-navigation-aug-2025-teamtech-lead-hamid-ebadi","text":"Implemented generic robot-agent control supporting multiple meshes (e.g., pallet truck, forklift) and distinct ArUco IDs: P\u00e4r Aronsson Robot-agent and navigation2 (nav2) namespace: P\u00e4r Aronsson Multi-agent navigation capabilities using GPSS: P\u00e4r Aronsson","title":"2.1.0: Namespace cleanup and multi-agent navigation (Aug 2025) - Team/Tech Lead: Hamid Ebadi"},{"location":"CHANGELOG/#200-gpss-may-2025-teamtech-lead-hamid-ebadi","text":"Upgrade Gazebo from classic to ignition, adapting new sensors: Nazeeh Alhosary ArUco_localization, added localization and nav2 to new robot: P\u00e4r Aronsson Navigation: P\u00e4r Aronsson Bird's Eye View ros package: Converting projection.ipynb to camera_bird_eye_view: Hamid Ebadi, P\u00e4r Aronsson Deprecated: InfoBot, replaced with Pallet_truck (based on Jackal by Clearpath Robotics)","title":"2.0.0: GPSS (May 2025) - Team/Tech Lead: Hamid Ebadi"},{"location":"CHANGELOG/#106-collision-feb-2025","text":"Add scenario execution library for ros2 Add action servers for set_speed, teleport_robot and collision Add Node for TTC calculation Add package for custom messages","title":"1.0.6: Collision (Feb 2025)"},{"location":"CHANGELOG/#105-delivery-4-dec-2024","text":"D3.5 Disentanglement: One-parameter Object Movements. Trajectory visualization: Hamid Ebadi feat: added DynoWaitFor to make sure Jackal is launched last (synchronisation issue) simlan_bringup pkg created: Christoffer Johannesson","title":"1.0.5: Delivery 4 (Dec 2024)"},{"location":"CHANGELOG/#104-delivery-3-oct-2024","text":"SMILE-IV and ArtWork projects contributions Updated camera extrinsic, fixing OpenCV camera calibration to Gazebo simulator conversion: Erik Brorsson, Volvo D3.4 Out distribution data collection: Hamid Ebadi D3.3 In distribution data collection: Hamid Ebadi D3.2 Images for stitching: Hamid Ebadi D3.1 Dataset draft for HH: Hamid Ebadi CI/CD and Kubernetes integration by Filip Melberg, Vasiliki Kostara Jackal integration: Christoffer Johannesson, Hjalmar Ruscck Docker/vscode Disable GPU support by default POC for camera projection to pixel coordinates (Jupyter notebook): Hamid Ebadi Camera image dump and image stitching: Hamid Ebadi","title":"1.0.4: Delivery 3 (Oct 2024)"},{"location":"CHANGELOG/#103-jackal-robot-mar-2024-christoffer-johannesson","text":"Added dyno fork of jackal repo from Clearpath Robotics. Updated to Humble, added bringup and support for namespacing. Jackal can be spawned in Gazebo and controlled through the keyboard. Added .devcontainer folder with Dockerfile and devcontainer.json to set up project container in VS Code. Added docker-compose to link all needed files and set environment variables. Added .vscode folder with settings and tasks for easy building of the project. Updated README with info on how to use Docker setup in VS Code, and some features to make it easy to share the same setup with others. Features include: python3 dependency install with pip, cloning of other git repositories and how to make changes to those repositories.","title":"1.0.3: Jackal Robot (Mar 2024) - Christoffer Johannesson"},{"location":"CHANGELOG/#102-delivery-2-feb-2024","text":"Volvo warehouse 0.0.1: Hamid Ebadi Volvo camera calibration in Gazebo 0.0.1: Hamid Ebadi Integrate Infobot_agent 0.0.2: InfoBot differential-drive AMR (Autonomous Mobile Robot) URDF and ROS launcher (GOPAL and forklift): Hamid Ebadi Integrate Infobot_cartographer 2.1.5: cartographer for creating PGM maps Integrate nav2_commander 0.0.2: ROS package to command Infobot where the destination is: Hamid Ebadi Integrate Infobot_navigation2 2.1.5: Standard Nav2 stack launcher: Hamid Ebadi Integrate Infobot_teleop 0.0.2: Teleoperation for InfoBot","title":"1.0.2: Delivery 2 (Feb 2024)"},{"location":"CHANGELOG/#101-delivery-1-dec-2023-teamtech-lead-hamid-ebadi","text":"Basic warehouse model 1.0.0: Anders B\u00e4ckelie CAD modelling (eur-pallet, boxes, shelf, support_pole, traffic-cone, steel_drum) 1.0.0: Jacob Rohdin Physics (collision, inertia), visuals and Gazebo compatible mesh creation 1.0.0: Anders B\u00e4ckelie Walking actor using scripted trajectories 1.0.0: Anders B\u00e4ckelie Infobot_Gazebo_environment 1.0.0: ROS2 launcher to start Gazebo world: Hamid Ebadi static_agent_launcher 1.0.0: Camera and ArUco tags: Hamid Ebadi camera-viewer 1.0.0: Python code to get Gazebo camera feed: Hamid Ebadi","title":"1.0.1: Delivery 1 (Dec 2023) - Team/Tech Lead: Hamid Ebadi"},{"location":"ISSUES/","text":"Advanced Features Filtering log output In config.sh you can set the level of logs you want outputted into the terminal. By default it is set to \"info\" to allow all logs. Possible values are: \"debug\", \"info\", \"warn\", and \"error\". Setting it to \"warn\" filters out all debug and info messages. Additionally, to filter out specific lines you can add the phrase you want filtered inside log_blacklist.txt and setting the log_level flag to \"warn\" or \"error\" will start filtering out all phrases found in the blacklist. Aruco detection If you want to add the TF links between the cameras and the ArUco markers without running the gpss command, you can run the following command. This is primarily useful for debugging, as gpss runs this as well. ./control.sh aruco_detection Finally, to view the bird's-eye perspective from each camera, run the following command and open rviz . Then, navigate to the left panel and under \"Camera\" change the Topic /static_agents/camera_XXX/image_projected to visualize the corresponding camera feed: Running ROS2 commands: To avoid conflict between ros nodes in the same network, after each build a new random ROS2 domain is created. This means that you need to adjust this random domain before running any ros2 commands. For convenience you can use ./control.sh cmd YOUR_ROS2_COMMANDS Here is an example of extracting tf2 hierarchy ./control.sh cmd ros2 run tf2_tools view_frames These features and commands are under development and not fully supported yet and therefore are subject to change. Cartographer : With both Gazebo and rviz running you can start creating a map of a robot's surroundings. To start: ./control.sh cartographer Teleop If you want to control any robot (pallet truck, humanoid, etc.) manually you can run the following command. Remember to specify what robot you want to control by adding its namespace as argument, i.e. ./control.sh teleop pallet_truck_1 ./control.sh teleop ${YOUR_ROBOT_NAMESPACE} To move the humanoid around in the simulator ./control.sh teleop ${YOUR_HUMANOID_NAMESPACE} Scenarios In scenarios.sh you can run predefined scenarios of the project. At the bottom of the file, commands are shown how to run it; otherwise each scenario is referenced by a number. Currently there are 3 scenarios and to run them, run this command in the shell Before running scenario 1 that uses GPSS cameras, make sure that CAMERA_ENABLED_IDS in config.sh has the list of GPSS cameras. Note: A previous successful setup where all scenarios worked used these cameras: '160 161 162 163 164 165 166 167 168 169 170'. If robots are not moving in your simulation, it might be that there is no camera looking at the robot. ./control.sh build ./scenarios.sh 1 # navigation using GPSS cameras (real time factor need to be updated) ./scenarios.sh 2 # Humanoid and robotic arm ./scenarios.sh 3 # Humanoid navigation without GPSS cameras (navigate_w_replanning_and_recovery_robot_agent_X.xml need to be updated ) ./scenarios.sh 4 # Record the video demo If you want to see the planning route for all agents, load scenario_3_planning.rviz in Rviz. Keep in mind to change real_time_factor in simulation/simlan_gazebo_environment/worlds/ign_simlan_factory.world.xacro to small values to slow down the simulator (e.g. 0.05-0.1) before building the project. ./control.sh build Jackal If you want to control the Jackal, you add the following lines into the control.sh: elif [[ \"$*\" == *\"jackal_teleop\"* ]] then ros2 launch dyno_jackal_bringup keyboard_steering.launch.py And then run: ./control.sh jackal_teleop These ones did not work, so we put them here in issues elif [[ \"$*\" == *\"move_object\"* ]] then ros2 run object_mover move_object elif [[ \"$*\" == *\"scenario\"* ]] then ros2 launch scenario_execution_ros scenario_launch.py scenario:=simulation/scenario_manager/scenarios/test.osc ISSUES gazebo_ros2_control [gzserver-1] [ERROR] [1727951819.671190014] [jackal.gazebo_ros2_control]: controller manager doesn't have an update_rate parameter No solution. OpenAL [gzserver-1] [Err] [OpenAL.cc:84] Unable to open audio device[default] Related to support for audio inside a Docker container. It will not be resolved. Command failed: docker compose If you have any issue with docker incompatibility (e.g. Error: Command failed: docker compose ... ), make sure that docker compose or docker-compose is set correctly in the settings. In docker-compose.yaml , uncomment the factory_simulation_nvidia section: factory_simulation_nvidia: <<: *research-base container_name: factory_simulation_nvidia runtime: nvidia deploy: resources: reservations: devices: - driver: nvidia count: \"all\" capabilities: [compute,utility,graphics,display] and update .devcontainer/devcontainer.json : { \"name\": \"ROS2 RESEARCH CONTAINER\", \"dockerComposeFile\": \"../docker-compose.yaml\", \"service\": \"factory_simulation_nvidia ... Missing nvidia docker runtime Solution: https://stackoverflow.com/questions/59008295/add-nvidia-runtime-to-docker-runtimes Docker nvidia-container-cli: requirement error If you get this error when building the docker container: nvidia-container-cli: requirement error: unsatisfied condition: cuda>=12.6, please update your driver to a newer version, or use an earlier cuda container: unknown A solution to try is first: re-install nvidia-container-toolkit. Or, if that does not work, update your nvidia-drivers. GLIBC_2 issue Sometimes the wrong nvidia-container-toolkit results in this issue below: $/usr/lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /usr/lib/x86_64-linux-gnu/libGLdispatch.so.0) $/usr/lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /usr/lib/x86_64-linux-gnu/libGLX.so.0) If so, try downgrading nvidia-container- toolkit . You can use these commands: $sudo apt-get remove --purge nvidia-container-toolkit nvidia-container-toolkit-base libnvidia-container* $sudo apt-get update $sudo apt-get install nvidia-container-toolkit=1.17.4-1 nvidia-container-toolkit-base=1.17.4-1 libnvidia-container1=1.17.4-1 libnvidia-container-tools=1.17.4-1 docker issues Make sure docker is installed correctly by following these two instructions: Install docker using the convenience script Linux post-installation steps for Docker Engine","title":"Issues"},{"location":"ISSUES/#advanced-features","text":"","title":"Advanced Features"},{"location":"ISSUES/#filtering-log-output","text":"In config.sh you can set the level of logs you want outputted into the terminal. By default it is set to \"info\" to allow all logs. Possible values are: \"debug\", \"info\", \"warn\", and \"error\". Setting it to \"warn\" filters out all debug and info messages. Additionally, to filter out specific lines you can add the phrase you want filtered inside log_blacklist.txt and setting the log_level flag to \"warn\" or \"error\" will start filtering out all phrases found in the blacklist.","title":"Filtering log output"},{"location":"ISSUES/#aruco-detection","text":"If you want to add the TF links between the cameras and the ArUco markers without running the gpss command, you can run the following command. This is primarily useful for debugging, as gpss runs this as well. ./control.sh aruco_detection Finally, to view the bird's-eye perspective from each camera, run the following command and open rviz . Then, navigate to the left panel and under \"Camera\" change the Topic /static_agents/camera_XXX/image_projected to visualize the corresponding camera feed:","title":"Aruco detection"},{"location":"ISSUES/#running-ros2-commands","text":"To avoid conflict between ros nodes in the same network, after each build a new random ROS2 domain is created. This means that you need to adjust this random domain before running any ros2 commands. For convenience you can use ./control.sh cmd YOUR_ROS2_COMMANDS Here is an example of extracting tf2 hierarchy ./control.sh cmd ros2 run tf2_tools view_frames These features and commands are under development and not fully supported yet and therefore are subject to change. Cartographer : With both Gazebo and rviz running you can start creating a map of a robot's surroundings. To start: ./control.sh cartographer","title":"Running ROS2 commands:"},{"location":"ISSUES/#teleop","text":"If you want to control any robot (pallet truck, humanoid, etc.) manually you can run the following command. Remember to specify what robot you want to control by adding its namespace as argument, i.e. ./control.sh teleop pallet_truck_1 ./control.sh teleop ${YOUR_ROBOT_NAMESPACE} To move the humanoid around in the simulator ./control.sh teleop ${YOUR_HUMANOID_NAMESPACE}","title":"Teleop"},{"location":"ISSUES/#scenarios","text":"In scenarios.sh you can run predefined scenarios of the project. At the bottom of the file, commands are shown how to run it; otherwise each scenario is referenced by a number. Currently there are 3 scenarios and to run them, run this command in the shell Before running scenario 1 that uses GPSS cameras, make sure that CAMERA_ENABLED_IDS in config.sh has the list of GPSS cameras. Note: A previous successful setup where all scenarios worked used these cameras: '160 161 162 163 164 165 166 167 168 169 170'. If robots are not moving in your simulation, it might be that there is no camera looking at the robot. ./control.sh build ./scenarios.sh 1 # navigation using GPSS cameras (real time factor need to be updated) ./scenarios.sh 2 # Humanoid and robotic arm ./scenarios.sh 3 # Humanoid navigation without GPSS cameras (navigate_w_replanning_and_recovery_robot_agent_X.xml need to be updated ) ./scenarios.sh 4 # Record the video demo If you want to see the planning route for all agents, load scenario_3_planning.rviz in Rviz. Keep in mind to change real_time_factor in simulation/simlan_gazebo_environment/worlds/ign_simlan_factory.world.xacro to small values to slow down the simulator (e.g. 0.05-0.1) before building the project. ./control.sh build","title":"Scenarios"},{"location":"ISSUES/#jackal","text":"If you want to control the Jackal, you add the following lines into the control.sh: elif [[ \"$*\" == *\"jackal_teleop\"* ]] then ros2 launch dyno_jackal_bringup keyboard_steering.launch.py And then run: ./control.sh jackal_teleop These ones did not work, so we put them here in issues elif [[ \"$*\" == *\"move_object\"* ]] then ros2 run object_mover move_object elif [[ \"$*\" == *\"scenario\"* ]] then ros2 launch scenario_execution_ros scenario_launch.py scenario:=simulation/scenario_manager/scenarios/test.osc","title":"Jackal"},{"location":"ISSUES/#issues","text":"","title":"ISSUES"},{"location":"ISSUES/#gazebo_ros2_control","text":"[gzserver-1] [ERROR] [1727951819.671190014] [jackal.gazebo_ros2_control]: controller manager doesn't have an update_rate parameter No solution.","title":"gazebo_ros2_control"},{"location":"ISSUES/#openal","text":"[gzserver-1] [Err] [OpenAL.cc:84] Unable to open audio device[default] Related to support for audio inside a Docker container. It will not be resolved.","title":"OpenAL"},{"location":"ISSUES/#command-failed-docker-compose","text":"If you have any issue with docker incompatibility (e.g. Error: Command failed: docker compose ... ), make sure that docker compose or docker-compose is set correctly in the settings. In docker-compose.yaml , uncomment the factory_simulation_nvidia section: factory_simulation_nvidia: <<: *research-base container_name: factory_simulation_nvidia runtime: nvidia deploy: resources: reservations: devices: - driver: nvidia count: \"all\" capabilities: [compute,utility,graphics,display] and update .devcontainer/devcontainer.json : { \"name\": \"ROS2 RESEARCH CONTAINER\", \"dockerComposeFile\": \"../docker-compose.yaml\", \"service\": \"factory_simulation_nvidia ...","title":"Command failed: docker compose"},{"location":"ISSUES/#missing-nvidia-docker-runtime","text":"Solution: https://stackoverflow.com/questions/59008295/add-nvidia-runtime-to-docker-runtimes","title":"Missing nvidia docker runtime"},{"location":"ISSUES/#docker-nvidia-container-cli-requirement-error","text":"If you get this error when building the docker container: nvidia-container-cli: requirement error: unsatisfied condition: cuda>=12.6, please update your driver to a newer version, or use an earlier cuda container: unknown A solution to try is first: re-install nvidia-container-toolkit. Or, if that does not work, update your nvidia-drivers.","title":"Docker nvidia-container-cli: requirement error"},{"location":"ISSUES/#glibc_2-issue","text":"Sometimes the wrong nvidia-container-toolkit results in this issue below: $/usr/lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /usr/lib/x86_64-linux-gnu/libGLdispatch.so.0) $/usr/lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /usr/lib/x86_64-linux-gnu/libGLX.so.0) If so, try downgrading nvidia-container- toolkit . You can use these commands: $sudo apt-get remove --purge nvidia-container-toolkit nvidia-container-toolkit-base libnvidia-container* $sudo apt-get update $sudo apt-get install nvidia-container-toolkit=1.17.4-1 nvidia-container-toolkit-base=1.17.4-1 libnvidia-container1=1.17.4-1 libnvidia-container-tools=1.17.4-1","title":"GLIBC_2 issue"},{"location":"ISSUES/#docker-issues","text":"Make sure docker is installed correctly by following these two instructions: Install docker using the convenience script Linux post-installation steps for Docker Engine","title":"docker issues"},{"location":"contributing/","text":"Contributing Legal Notice: When contributing to this project, you must agree that you have authored 100% of the content, that you have the necessary rights to the content, and that the content you contribute may be provided under the project license. We try to use: Docker, vscode + dev-container extension to develop inside a container Markdown for documentation Conventional Commits for commit messages pre-commit for git pre-commit hooks and basic QA Semantic Versioning for versioning Draw.io for drawing diagrams for documentation Technical Avoid optimization unless you have a clear, measurable performance problem, as premature optimization can lead to overly complex and unreadable code. PEP 20 \u2013 The Zen of Python Simple is better than complex. Readability counts. If the implementation is hard to explain, it's a bad idea. Please be generous in giving credit when using an image or a piece of code created by others. Add a link to the original author in credits.md . Avoid pushing binary files such as images, models, etc. (use .gitignore), especially if they are constantly changing. git Try to avoid breaking the main branch, but don't be obsessed with having a perfect merge request. We can always revert back to the working version or fix the issues. That is why we are using git . Follow this simple git process: git checkout main git pull git checkout -b \"branch_name\" # update files git commit -m \"conventional_commit_type: conventional_commit_message\" git push # Create a pull request. Common commit types: feat: Introduces a new feature. fix: Patches a bug or issue. chore: Routine maintenance or changes that don't affect the app's functionality. docs: Documentation changes. Conventional commit message: A conventional commit message uses the imperative mood in the subject line Example of feat(auth): add login functionality fix(ui): resolve button color issue https://www.conventionalcommits.org Misc Don't use absolute paths. Your code should run correctly both inside the VS Code Dev Container and independently outside of it. If you need to modify a configuration file at build time, use relative paths from the project root directory (avoid using \"..\" as it becomes hard to see which scripts are using a specific directory). For ROS2 resources, use get_package_share_directory(project_name) to locate package files. When adding a new feature, add it to control.sh. Otherwise, control.sh should remain unchanged. Any configuration that requires modification should be defined in config.sh . Documentation Documentation is done in a markdown file. Try to keep our documentation close to your code. Keep the documentation short and clear. # : Only used once for the title of the project. ## : usually once in each markdown file and usually shares semantics with the markdown filename. ### : Different topics, try to split your documentation into at least 4 topics. #### : subtopics. Try to avoid when you can use simple paragraphs. Follow this subset of Markdown tags.","title":"Contributing"},{"location":"contributing/#contributing","text":"Legal Notice: When contributing to this project, you must agree that you have authored 100% of the content, that you have the necessary rights to the content, and that the content you contribute may be provided under the project license. We try to use: Docker, vscode + dev-container extension to develop inside a container Markdown for documentation Conventional Commits for commit messages pre-commit for git pre-commit hooks and basic QA Semantic Versioning for versioning Draw.io for drawing diagrams for documentation","title":"Contributing"},{"location":"contributing/#technical","text":"Avoid optimization unless you have a clear, measurable performance problem, as premature optimization can lead to overly complex and unreadable code. PEP 20 \u2013 The Zen of Python Simple is better than complex. Readability counts. If the implementation is hard to explain, it's a bad idea. Please be generous in giving credit when using an image or a piece of code created by others. Add a link to the original author in credits.md . Avoid pushing binary files such as images, models, etc. (use .gitignore), especially if they are constantly changing.","title":"Technical"},{"location":"contributing/#git","text":"Try to avoid breaking the main branch, but don't be obsessed with having a perfect merge request. We can always revert back to the working version or fix the issues. That is why we are using git . Follow this simple git process: git checkout main git pull git checkout -b \"branch_name\" # update files git commit -m \"conventional_commit_type: conventional_commit_message\" git push # Create a pull request. Common commit types: feat: Introduces a new feature. fix: Patches a bug or issue. chore: Routine maintenance or changes that don't affect the app's functionality. docs: Documentation changes. Conventional commit message: A conventional commit message uses the imperative mood in the subject line Example of feat(auth): add login functionality fix(ui): resolve button color issue https://www.conventionalcommits.org","title":"git"},{"location":"contributing/#misc","text":"Don't use absolute paths. Your code should run correctly both inside the VS Code Dev Container and independently outside of it. If you need to modify a configuration file at build time, use relative paths from the project root directory (avoid using \"..\" as it becomes hard to see which scripts are using a specific directory). For ROS2 resources, use get_package_share_directory(project_name) to locate package files. When adding a new feature, add it to control.sh. Otherwise, control.sh should remain unchanged. Any configuration that requires modification should be defined in config.sh .","title":"Misc"},{"location":"contributing/#documentation","text":"Documentation is done in a markdown file. Try to keep our documentation close to your code. Keep the documentation short and clear. # : Only used once for the title of the project. ## : usually once in each markdown file and usually shares semantics with the markdown filename. ### : Different topics, try to split your documentation into at least 4 topics. #### : subtopics. Try to avoid when you can use simple paragraphs. Follow this subset of Markdown tags.","title":"Documentation"},{"location":"credits/","text":"Licenses and Credits The majority of code for the pallet truck comes from these turtlebot3 and turtlebot3_simulations repositories (in humble-devel branch) with Apache-2.0 license and we continue using the same license: The Turtlebot3 robot project belongs to robotis.com and accessible in git repository . The authors and maintainers of the original packages that all credits go to are: Darby Lim Pyo Will Son Ryan Shim In October 2023, Hamid Ebadi renamed the package owner information and name for turtlebot3 projects to avoid dependency issues any naming confusion with the original packages and created independent packages for activities within the research project. We also got inspired and used the skeleton code from these open source project and courses: https://github.com/ros-controls/gazebo_ros2_control https://github.com/renan028/forklift_robot https://github.com/ROBOTIS-GIT/ http://turtlebot3.robotis.com Articulated Robotics \"ROS2 for Beginners Level 2 - TF | URDF | RViz | Gazebo\" Udemy course \"ROS2 Nav2 [Navigation 2 Stack] - with SLAM and Navigation\" Udemy course Visual Servoing in Gazebo grobot The modification of Human-Gazebo in /src/humanoid_robot/model/human-gazebo is licensed under LGPL-2.1. Moveit2 is licensed under BSD-3-Clause license. Resources Gazebo official video playlist Getting Ready to Build Robots with ROS Udemy course on ROS2 and Gazebo classcentral course theconstructsim course Articulated Robotics ROS URDF FoxGlove studio (rviz alternative) ROS2 Tutorials aws-robomaker model for factory Tugbot in Warehouse logistics in warehouse aws-robomaker gazebo_worlds warehouse forklift Docker install guide Hazard stripes taken from Tugbot warehouse Deadlock image Other solutions: Kollmorgen: How does an AGV navigate? SwissLog CarryPick Toyota forklifts ILIAD Project GoPal navigation_oru navigation stack by \u00d6rebro University https://alignproductionsystems.com/equipment-category/navigation/# Specification of items: SLAM Navigation Compact Pallet Mover Nature Navigation Mini Forklift with Payload 1000KG Driverless Lifting System: Single & Double Scissor Lift | AGILOX Wholesale Pallet Agv Trucks Jack Automated Autonomous Forklift Volvo modular containers Volvo Emballage Specifications Volvo Group Packaging System Freecad Blender example worlds SDF format SDF tutorial FreeCAD RobotCreator Workbench Coordinates: odom Camera projection: https://github.com/polygon-software/python-visual-odometry/blob/master/Chapter%203%20-%20Camera%20Projection.ipynb https://classic.gazebosim.org/tutorials?tut=camera_distortion https://learnopencv.com/rotation-matrix-to-euler-angles/ https://www.geeksforgeeks.org/calibratecamera-opencv-in-python/ https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html http://sdformat.org/tutorials?tut=specify_pose Jackal Robot: The Clearpath Jackal Robot code is forked by Dyno Robotics from Clearpaths jackal Github. Branch foxy-devel is ported to dyno_humble, where changes for namespacing and a bringup package is added. To improve collaboration in development environment we use vscode and docker as explained in this instruction using these docker files . For production environment follow installation procedure used in .devcontainer/Dockerfile to install dependencies. The Docker setup is added by Christoffer Johanesson (Dyno Robotics), based on Dyno experience working with Docker. UR10 & Robotiq gripper: UR Description: https://github.com/UniversalRobots/Universal_Robots_ROS2_Description UR Moveit Config:https://github.com/UniversalRobots/Universal_Robots_ROS2_Driver/tree/main/ur_moveit_config Robotiq Description:https://github.com/PickNikRobotics/ros2_robotiq_gripper Panda Arm: links https://github.com/cpezzato/panda_simulation https://www.researchgate.net/figure/Panda-robot-with-labeled-joints_fig1_361659008 https://www.kuka.com/it-it/settori/banca-dati-di-soluzioni/2025/05/5-kuka-robots-optimize-pet-food-handling-in-74-sqm https://arxiv.org/pdf/2506.12089 AI/ML: links https://www.calibraint.com/blog/beginners-guide-to-diffusion-models Humanoid: links https://gazebosim.org/docs/latest/actors/ https://research.google/blog/on-device-real-time-body-pose-tracking-with-mediapipe-blazepose/ Licenses Imu_tools is the one with several licenses, using BSD-3, GPLv3, GNU v3, but as we don't change the code so we believe there is no license conflict with the current project license. The dependency wireless (https://github.com/clearpathrobotics/wireless.git) from Clearpath robotics is the without a separate license file. There are license names within files, referencing BSD so we believe there is no license conflict with the current project license. Docker and VS Code setup The Docker setup added by Christoffer Johanesson (Dyno Robotics) , based on Dyno experience working with Docker. Project maintainer This project is currently maintained by Hamid Ebadi .","title":"Credits"},{"location":"credits/#licenses-and-credits","text":"The majority of code for the pallet truck comes from these turtlebot3 and turtlebot3_simulations repositories (in humble-devel branch) with Apache-2.0 license and we continue using the same license: The Turtlebot3 robot project belongs to robotis.com and accessible in git repository . The authors and maintainers of the original packages that all credits go to are: Darby Lim Pyo Will Son Ryan Shim In October 2023, Hamid Ebadi renamed the package owner information and name for turtlebot3 projects to avoid dependency issues any naming confusion with the original packages and created independent packages for activities within the research project. We also got inspired and used the skeleton code from these open source project and courses: https://github.com/ros-controls/gazebo_ros2_control https://github.com/renan028/forklift_robot https://github.com/ROBOTIS-GIT/ http://turtlebot3.robotis.com Articulated Robotics \"ROS2 for Beginners Level 2 - TF | URDF | RViz | Gazebo\" Udemy course \"ROS2 Nav2 [Navigation 2 Stack] - with SLAM and Navigation\" Udemy course Visual Servoing in Gazebo grobot The modification of Human-Gazebo in /src/humanoid_robot/model/human-gazebo is licensed under LGPL-2.1. Moveit2 is licensed under BSD-3-Clause license.","title":"Licenses and Credits"},{"location":"credits/#resources","text":"Gazebo official video playlist Getting Ready to Build Robots with ROS Udemy course on ROS2 and Gazebo classcentral course theconstructsim course Articulated Robotics ROS URDF FoxGlove studio (rviz alternative) ROS2 Tutorials aws-robomaker model for factory Tugbot in Warehouse logistics in warehouse aws-robomaker gazebo_worlds warehouse forklift Docker install guide Hazard stripes taken from Tugbot warehouse Deadlock image Other solutions: Kollmorgen: How does an AGV navigate? SwissLog CarryPick Toyota forklifts ILIAD Project GoPal navigation_oru navigation stack by \u00d6rebro University https://alignproductionsystems.com/equipment-category/navigation/# Specification of items: SLAM Navigation Compact Pallet Mover Nature Navigation Mini Forklift with Payload 1000KG Driverless Lifting System: Single & Double Scissor Lift | AGILOX Wholesale Pallet Agv Trucks Jack Automated Autonomous Forklift Volvo modular containers Volvo Emballage Specifications Volvo Group Packaging System Freecad Blender example worlds SDF format SDF tutorial FreeCAD RobotCreator Workbench Coordinates: odom Camera projection: https://github.com/polygon-software/python-visual-odometry/blob/master/Chapter%203%20-%20Camera%20Projection.ipynb https://classic.gazebosim.org/tutorials?tut=camera_distortion https://learnopencv.com/rotation-matrix-to-euler-angles/ https://www.geeksforgeeks.org/calibratecamera-opencv-in-python/ https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html http://sdformat.org/tutorials?tut=specify_pose Jackal Robot: The Clearpath Jackal Robot code is forked by Dyno Robotics from Clearpaths jackal Github. Branch foxy-devel is ported to dyno_humble, where changes for namespacing and a bringup package is added. To improve collaboration in development environment we use vscode and docker as explained in this instruction using these docker files . For production environment follow installation procedure used in .devcontainer/Dockerfile to install dependencies. The Docker setup is added by Christoffer Johanesson (Dyno Robotics), based on Dyno experience working with Docker. UR10 & Robotiq gripper: UR Description: https://github.com/UniversalRobots/Universal_Robots_ROS2_Description UR Moveit Config:https://github.com/UniversalRobots/Universal_Robots_ROS2_Driver/tree/main/ur_moveit_config Robotiq Description:https://github.com/PickNikRobotics/ros2_robotiq_gripper Panda Arm: links https://github.com/cpezzato/panda_simulation https://www.researchgate.net/figure/Panda-robot-with-labeled-joints_fig1_361659008 https://www.kuka.com/it-it/settori/banca-dati-di-soluzioni/2025/05/5-kuka-robots-optimize-pet-food-handling-in-74-sqm https://arxiv.org/pdf/2506.12089 AI/ML: links https://www.calibraint.com/blog/beginners-guide-to-diffusion-models Humanoid: links https://gazebosim.org/docs/latest/actors/ https://research.google/blog/on-device-real-time-body-pose-tracking-with-mediapipe-blazepose/","title":"Resources"},{"location":"credits/#licenses","text":"Imu_tools is the one with several licenses, using BSD-3, GPLv3, GNU v3, but as we don't change the code so we believe there is no license conflict with the current project license. The dependency wireless (https://github.com/clearpathrobotics/wireless.git) from Clearpath robotics is the without a separate license file. There are license names within files, referencing BSD so we believe there is no license conflict with the current project license.","title":"Licenses"},{"location":"credits/#docker-and-vs-code-setup","text":"The Docker setup added by Christoffer Johanesson (Dyno Robotics) , based on Dyno experience working with Docker.","title":"Docker and VS Code setup"},{"location":"credits/#project-maintainer","text":"This project is currently maintained by Hamid Ebadi .","title":"Project maintainer"},{"location":"dependencies/","text":"Dependencies Docker sudo apt install curl git curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh sudo groupadd docker sudo usermod -aG docker $USER newgrp docker Nvidia GPU see resources/ISSUES.md if you faced any problem after running the commands below: NVIDIA Driver Use 'nvidia-smi' to ensure that the right NVIDIA driver is installed. If you have not installed Additional Drivers when installing Ubuntu, you need to manually install NVIDIA drivers. nvidia-container-toolkit To install Docker and nvidia-container-toolkit , use the following commands: curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list sudo sed -i -e '/experimental/ s/^#//g' /etc/apt/sources.list.d/nvidia-container-toolkit.list sudo apt-get update sudo apt-get install -y nvidia-container-toolkit sudo nvidia-ctk runtime configure --runtime=docker INFO[0000] Config file does not exist; using empty config INFO[0000] Wrote updated config to /etc/docker/daemon.json INFO[0000] It is recommended that docker daemon be restarted. sudo systemctl restart docker sudo nvidia-ctk runtime configure --runtime=containerd INFO[0000] Using config version 1 INFO[0000] Using CRI runtime plugin name \"cri\" WARN[0000] could not infer options from runtimes [runc crun]; using defaults INFO[0000] Wrote updated config to /etc/containerd/config.toml INFO[0000] It is recommended that containerd daemon be restarted. Restart the computer to apply the group and user changes. To check for correct installation of Docker's NVIDIA runtime: docker info|grep -i runtime Runtimes: nvidia runc Default Runtime: runc Otherwise you get the following error message in VS Code: Error response from daemon: unknown or invalid runtime name: nvidia On a host machine's terminal ( not inside Visual Studio Code terminal): xhost +local:docker . No Nvidia GPU To start the project without NVIDIA GPU , please comment out these lines in docker-compose.yaml as shown below: # runtime: nvidia # # factory_simulation_nvidia: # <<: *research-base # container_name: factory_simulation_nvidia # runtime: nvidia # deploy: # resources: # reservations: # devices: # - driver: nvidia # count: \"all\" # capabilities: [compute,utility,graphics,display] Windows Dependencies These instructions are tested on Windows 11, Docker Desktop for Windows ARM64 and VS Code. To get the Docker container up and running, download and install Docker Desktop for your system: https://www.docker.com/products/docker-desktop/ When this is done, clone the repo and open the folder in VS Code. Then you should automatically be prompted to download and install VS Code extensions that are needed and recommended for the project. Once that is completed, make sure Docker Desktop is running and then you should be able to start the container environment in VS Code by pressing Ctrl+Shift+P and searching for Dev Containers: Rebuild and Reopen in Container. To make Gazebo's and RViz's GUIs visible from the Docker container on your screen, download and install the following program: https://sourceforge.net/projects/vcxsrv/ When this is done, start the XLaunch program and configure it with these settings: Select display settings \u25c9 Multiple windows \u25ef Fullscreen \u25ef One large window \u25ef One window without titlebar Display number: -1 Select how to start clients \u25c9 Start no client \u25ef Start a program \u25ef Open session via XDMCP Extra settings \u2611 Clipboard (optional) \u2611 Primary Selection (optional) \u2610 Native OpenGL \u2611 Disable access control Before continuing, make sure Docker Desktop is running.","title":"Dependencies"},{"location":"dependencies/#dependencies","text":"","title":"Dependencies"},{"location":"dependencies/#docker","text":"sudo apt install curl git curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh sudo groupadd docker sudo usermod -aG docker $USER newgrp docker","title":"Docker"},{"location":"dependencies/#nvidia-gpu","text":"see resources/ISSUES.md if you faced any problem after running the commands below:","title":"Nvidia GPU"},{"location":"dependencies/#nvidia-driver","text":"Use 'nvidia-smi' to ensure that the right NVIDIA driver is installed. If you have not installed Additional Drivers when installing Ubuntu, you need to manually install NVIDIA drivers.","title":"NVIDIA Driver"},{"location":"dependencies/#nvidia-container-toolkit","text":"To install Docker and nvidia-container-toolkit , use the following commands: curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list sudo sed -i -e '/experimental/ s/^#//g' /etc/apt/sources.list.d/nvidia-container-toolkit.list sudo apt-get update sudo apt-get install -y nvidia-container-toolkit sudo nvidia-ctk runtime configure --runtime=docker INFO[0000] Config file does not exist; using empty config INFO[0000] Wrote updated config to /etc/docker/daemon.json INFO[0000] It is recommended that docker daemon be restarted. sudo systemctl restart docker sudo nvidia-ctk runtime configure --runtime=containerd INFO[0000] Using config version 1 INFO[0000] Using CRI runtime plugin name \"cri\" WARN[0000] could not infer options from runtimes [runc crun]; using defaults INFO[0000] Wrote updated config to /etc/containerd/config.toml INFO[0000] It is recommended that containerd daemon be restarted. Restart the computer to apply the group and user changes. To check for correct installation of Docker's NVIDIA runtime: docker info|grep -i runtime Runtimes: nvidia runc Default Runtime: runc Otherwise you get the following error message in VS Code: Error response from daemon: unknown or invalid runtime name: nvidia On a host machine's terminal ( not inside Visual Studio Code terminal): xhost +local:docker .","title":"nvidia-container-toolkit"},{"location":"dependencies/#no-nvidia-gpu","text":"To start the project without NVIDIA GPU , please comment out these lines in docker-compose.yaml as shown below: # runtime: nvidia # # factory_simulation_nvidia: # <<: *research-base # container_name: factory_simulation_nvidia # runtime: nvidia # deploy: # resources: # reservations: # devices: # - driver: nvidia # count: \"all\" # capabilities: [compute,utility,graphics,display]","title":"No Nvidia GPU"},{"location":"dependencies/#windows-dependencies","text":"These instructions are tested on Windows 11, Docker Desktop for Windows ARM64 and VS Code. To get the Docker container up and running, download and install Docker Desktop for your system: https://www.docker.com/products/docker-desktop/ When this is done, clone the repo and open the folder in VS Code. Then you should automatically be prompted to download and install VS Code extensions that are needed and recommended for the project. Once that is completed, make sure Docker Desktop is running and then you should be able to start the container environment in VS Code by pressing Ctrl+Shift+P and searching for Dev Containers: Rebuild and Reopen in Container. To make Gazebo's and RViz's GUIs visible from the Docker container on your screen, download and install the following program: https://sourceforge.net/projects/vcxsrv/ When this is done, start the XLaunch program and configure it with these settings:","title":"Windows Dependencies"},{"location":"dependencies/#select-display-settings","text":"\u25c9 Multiple windows \u25ef Fullscreen \u25ef One large window \u25ef One window without titlebar Display number: -1","title":"Select display settings"},{"location":"dependencies/#select-how-to-start-clients","text":"\u25c9 Start no client \u25ef Start a program \u25ef Open session via XDMCP","title":"Select how to start clients"},{"location":"dependencies/#extra-settings","text":"\u2611 Clipboard (optional) \u2611 Primary Selection (optional) \u2610 Native OpenGL \u2611 Disable access control Before continuing, make sure Docker Desktop is running.","title":"Extra settings"},{"location":"presentation/","text":"false marp: true theme: default size: 4K paginate: false footer: 'Hamid Ebadi' header: ' \u25a3 SIMLAN Project' SIMLAN open-source project Open-Source Simulation for Multi-Camera Robotics The SIMLAN Framework Hamid Ebadi , senior researcher at Infotiv AB Research Projects SMILE-IV : safety assurance framework for transport services ARTWORK : The smart and connected worker Volvo Projects Volvo GTO in Tuve, G\u00f6teborg: RITA (Robot In The Air): collaborative robot designed to assist with kitting GPSS (Generic Photogrammetry-based Sensor System): ceiling-mounted cameras act as the shared \"eyes\" of the robot fleet. (more later) Autonomous Robotics (intro) Autonomous Robotics SLAM Vacuum cleaner Simultaneous localization and mapping Reliance on onboard sensors Distributed decision making Communication and synchronization Autonomous Robotics Limited field of view Sensor interference (LiDAR) No global view resolving right-of-way avoiding gridlock Handling challenging environments no landmarks repetitive landmarks dynamic landmarks Watch the Video Centralised Robotics (pros) GPSS (camera-based) Simpler onboard computation Focus on control Energy consumption Simpler hardware Easier to maintain and upgrade No robot-to-robot communication Centralised Robotics (pros) Improved explainability and accountability Camera is used for safety monitoring and repudiation. Improving the safety by using both onboard and offboard sensors More flexible to add ML based models Centralised Robotics (cons) Real-time needs and latency Centralised processing and a single point of failure No mapping but only localization using fixed cameras Continuously testing these ML systems is challenging. Open-Source Simulation for Multi-Camera Robotics The SIMLAN Framework Using simulation for complex human-robot collaboration. Inspired by Volvo Group\u2019s GPSS/RITA Models ceiling-mounted cameras + factory layouts SIMLAN: Asset & Environment Modeling (1) Realistic warehouse models Free/Open-source 3D software: FreeCAD, Blender Relevant Assets: shelves pallets boxes, ... Configurable physical properties: collision, inertia, mass, dimensions, visuals SIMLAN: Asset & Environment Modeling (2) Sensors : camera semantic segmentation depth sensors collision sensors Static Elements : layouts camera coordination and orientation ArUco markers on agents DEMO: SIMLAN physics failures SIMLAN: Asset & Environment Modeling (3) Dynamic Elements : Pallet truck Forklift Human worker Robotic Arm Multi-Agent & Namespace Support & DOMAIN ID Unique namespace + ArUco ID Spawning static & dynamic agents Localisation and Navigation Camera Configuration/Calibration Intrinsics : focal lengths, principal point, distortion coeffs Extrinsics : rotation matrix + translation vector enables precise world-to-pixel projection crucial for image stitching & ArUco localization Bird\u2019s-Eye View & Image stitching transform world \u2192 camera \u2192 pixel coordinates enables stitching of multiple camera feeds camera_bird_eye_view package ArUco Localization proof-of-concept GPSS system in SIMLAN uses OpenCV ArUco markers for localization aruco_localization package ArUco Navigation Input: tf2 (positions) Nav2 navigates (with a lot of wiring) multi-camera robustness Safety \"Behavior Tree\" for Geo-fencing loss of observability restricted area collision SIMLAN GPSS video demo RITA (Robot In The Air): collaborative robot designed to assist with kitting Panda arm demo Panda arm and humanoid demo Gazebo Actors Gazebo actors: skeleton animation from COLLADA or BVH files and scripted trajectories Gazebo actors are static (scripted trajectories only) and cannot interact physically. This limits their behavior to what they are strictly scripted for Humanoid Motion Capture Humanoid robots replicate a real worker's movements. Google MediaPipe pose estimation (landmarks) Neural Network translates landmarks to joint controls. MoveIt2 handles motion planning and execution of the humanoid in Gazebo. Humanoid training Summary of SIMLAN Features Dockerized dev environment Lower barriers for research in robotics/ML Features: Bird\u2019s-eye stitching ArUco-based localization ROS 2 / Nav2 integration Panda arm and humanoid SIMLAN Use Cases Rapid prototyping of ML-based localization/navigation Reproducible experiments : consistent testing Synthetic data generation for ML models Safety testing without risking physical assets High level of interaction reinforcement learning & genetic algorithm experimentation CI/CD : continuous development V&V to support verification and validation of complex, machine learning-based systems SIMLAN Use Cases Testing and Development Cost-efficient Fast Scalable Safe Privacy-friendly Reproducible (CI/CD) Open source Apache 2.0 license SIMLAN: https://github.com/infotiv-research/SIMLAN Infotiv portfolio of projects (academic papers): https://infotiv-research.github.io/ Technical Highlights Middleware : ROS2 (Robot Operating System) - Jazzy Jalisco standard interfaces Simulation Engine : Ignition Gazebo, simulating Physics Sensor Developer environment : Docker + VS Code devcontainers (consistency and reproducibility) Documentation : extensive & reproducible Testing or Development Simulation is for testing ONLY? Pushing simulation toward the entire Software Development Life Cycle (SDLC) Demo 1 , Demo 2 Future Work Generative AI Style transfer w/ GANs for higher visual fidelity Forward diffusion process Reverse diffusion process Hallucination Integrate World Foundation Models (e.g., NVIDIA Cosmos) Conclusion SIMLAN : powerful platform for indoor multi-camera robotics Reproducible, scalable, open-source Academia & Industry Roadmap: ML integration, human-robot collaboration, sim-to-real transfer Need your support Acknowledgements INFOTIV AB SMILE IV (Vinnova grant 2023-00789) EUREKA ITEA4 ArtWork (Vinnova grant 2023-00970) INFOTIV Colleagues : P\u00e4r Aronsson, Anton Hill, David Espedalen, Siyu Yi, Anders B\u00e4ckelie, Jacob Rohdin, Vasiliki Kostara, Nazeeh Alhosary, Marwa Naili Other contributors : Tove Casparsson, Filip Melberg (Chalmers), Christoffer Johannesson, Sebastian Olsson, Hjalmar Ruscck from Dyno-robotics, Erik Brorsson (Chalmers/Volvo), Other Partners : Infotiv AB, RISE, Volvo Group, Dyno-Robotics, Chalmers INFOTIV AB Dyno-robotics RISE Research Institutes of Sweden CHALMERS Volvo Group https://infotiv-research.github.io https://github.com/infotiv-research/SIMLAN","title":"Presentation"},{"location":"presentation/#footer-hamid-ebadi","text":"","title":"footer: 'Hamid Ebadi'"},{"location":"presentation/#header-simlan-project","text":"","title":"header: '\u25a3  \u00a0 SIMLAN Project'"},{"location":"presentation/#simlan-open-source-project","text":"","title":"SIMLAN open-source project"},{"location":"presentation/#open-source-simulation-for-multi-camera-robotics","text":"","title":"Open-Source Simulation for Multi-Camera Robotics"},{"location":"presentation/#the-simlan-framework","text":"Hamid Ebadi , senior researcher at Infotiv AB","title":"The SIMLAN Framework"},{"location":"presentation/#research-projects","text":"SMILE-IV : safety assurance framework for transport services ARTWORK : The smart and connected worker","title":"Research Projects"},{"location":"presentation/#volvo-projects","text":"Volvo GTO in Tuve, G\u00f6teborg: RITA (Robot In The Air): collaborative robot designed to assist with kitting GPSS (Generic Photogrammetry-based Sensor System): ceiling-mounted cameras act as the shared \"eyes\" of the robot fleet. (more later)","title":"Volvo Projects"},{"location":"presentation/#autonomous-robotics-intro","text":"","title":"Autonomous Robotics (intro)"},{"location":"presentation/#autonomous-robotics","text":"","title":"Autonomous Robotics"},{"location":"presentation/#slam","text":"Vacuum cleaner Simultaneous localization and mapping Reliance on onboard sensors Distributed decision making Communication and synchronization","title":"SLAM"},{"location":"presentation/#autonomous-robotics_1","text":"Limited field of view Sensor interference (LiDAR) No global view resolving right-of-way avoiding gridlock Handling challenging environments no landmarks repetitive landmarks dynamic landmarks Watch the Video","title":"Autonomous Robotics"},{"location":"presentation/#centralised-robotics-pros","text":"GPSS (camera-based) Simpler onboard computation Focus on control Energy consumption Simpler hardware Easier to maintain and upgrade No robot-to-robot communication","title":"Centralised Robotics (pros)"},{"location":"presentation/#centralised-robotics-pros_1","text":"Improved explainability and accountability Camera is used for safety monitoring and repudiation. Improving the safety by using both onboard and offboard sensors More flexible to add ML based models","title":"Centralised Robotics (pros)"},{"location":"presentation/#centralised-robotics-cons","text":"Real-time needs and latency Centralised processing and a single point of failure No mapping but only localization using fixed cameras","title":"Centralised Robotics (cons)"},{"location":"presentation/#continuously-testing-these-ml-systems-is-challenging","text":"","title":"Continuously testing these ML systems is challenging."},{"location":"presentation/#open-source-simulation-for-multi-camera-robotics_1","text":"","title":"Open-Source Simulation for Multi-Camera Robotics"},{"location":"presentation/#the-simlan-framework_1","text":"Using simulation for complex human-robot collaboration. Inspired by Volvo Group\u2019s GPSS/RITA Models ceiling-mounted cameras + factory layouts","title":"The SIMLAN Framework"},{"location":"presentation/#simlan-asset-environment-modeling-1","text":"Realistic warehouse models Free/Open-source 3D software: FreeCAD, Blender Relevant Assets: shelves pallets boxes, ... Configurable physical properties: collision, inertia, mass, dimensions, visuals","title":"SIMLAN: Asset &amp; Environment Modeling (1)"},{"location":"presentation/#simlan-asset-environment-modeling-2","text":"Sensors : camera semantic segmentation depth sensors collision sensors Static Elements : layouts camera coordination and orientation ArUco markers on agents DEMO: SIMLAN physics failures","title":"SIMLAN: Asset &amp; Environment Modeling (2)"},{"location":"presentation/#simlan-asset-environment-modeling-3","text":"Dynamic Elements : Pallet truck Forklift Human worker Robotic Arm","title":"SIMLAN: Asset &amp; Environment Modeling (3)"},{"location":"presentation/#multi-agent-namespace-support-domain-id","text":"Unique namespace + ArUco ID Spawning static & dynamic agents Localisation and Navigation","title":"Multi-Agent &amp; Namespace Support &amp; DOMAIN ID"},{"location":"presentation/#camera-configurationcalibration","text":"Intrinsics : focal lengths, principal point, distortion coeffs Extrinsics : rotation matrix + translation vector enables precise world-to-pixel projection crucial for image stitching & ArUco localization","title":"Camera Configuration/Calibration"},{"location":"presentation/#birds-eye-view-image-stitching","text":"transform world \u2192 camera \u2192 pixel coordinates enables stitching of multiple camera feeds camera_bird_eye_view package","title":"Bird\u2019s-Eye View &amp; Image stitching"},{"location":"presentation/#aruco-localization","text":"proof-of-concept GPSS system in SIMLAN uses OpenCV ArUco markers for localization aruco_localization package","title":"ArUco Localization"},{"location":"presentation/#aruco-navigation","text":"Input: tf2 (positions) Nav2 navigates (with a lot of wiring) multi-camera robustness","title":"ArUco Navigation"},{"location":"presentation/#safety","text":"\"Behavior Tree\" for Geo-fencing loss of observability restricted area collision SIMLAN GPSS video demo","title":"Safety"},{"location":"presentation/#rita-robot-in-the-air-collaborative-robot-designed-to-assist-with-kitting","text":"Panda arm demo Panda arm and humanoid demo","title":"RITA (Robot In The Air): collaborative robot designed to assist with kitting"},{"location":"presentation/#gazebo-actors","text":"Gazebo actors: skeleton animation from COLLADA or BVH files and scripted trajectories Gazebo actors are static (scripted trajectories only) and cannot interact physically. This limits their behavior to what they are strictly scripted for","title":"Gazebo Actors"},{"location":"presentation/#humanoid-motion-capture","text":"Humanoid robots replicate a real worker's movements. Google MediaPipe pose estimation (landmarks) Neural Network translates landmarks to joint controls. MoveIt2 handles motion planning and execution of the humanoid in Gazebo. Humanoid training","title":"Humanoid Motion Capture"},{"location":"presentation/#summary-of-simlan-features","text":"Dockerized dev environment Lower barriers for research in robotics/ML Features: Bird\u2019s-eye stitching ArUco-based localization ROS 2 / Nav2 integration Panda arm and humanoid","title":"Summary of SIMLAN Features"},{"location":"presentation/#simlan-use-cases","text":"Rapid prototyping of ML-based localization/navigation Reproducible experiments : consistent testing Synthetic data generation for ML models Safety testing without risking physical assets High level of interaction reinforcement learning & genetic algorithm experimentation CI/CD : continuous development V&V to support verification and validation of complex, machine learning-based systems","title":"SIMLAN Use Cases"},{"location":"presentation/#simlan-use-cases_1","text":"Testing and Development Cost-efficient Fast Scalable Safe Privacy-friendly Reproducible (CI/CD)","title":"SIMLAN Use Cases"},{"location":"presentation/#open-source","text":"Apache 2.0 license SIMLAN: https://github.com/infotiv-research/SIMLAN Infotiv portfolio of projects (academic papers): https://infotiv-research.github.io/","title":"Open source"},{"location":"presentation/#technical-highlights","text":"Middleware : ROS2 (Robot Operating System) - Jazzy Jalisco standard interfaces Simulation Engine : Ignition Gazebo, simulating Physics Sensor Developer environment : Docker + VS Code devcontainers (consistency and reproducibility) Documentation : extensive & reproducible","title":"Technical Highlights"},{"location":"presentation/#testing-or-development","text":"Simulation is for testing ONLY? Pushing simulation toward the entire Software Development Life Cycle (SDLC) Demo 1 , Demo 2","title":"Testing or Development"},{"location":"presentation/#future-work","text":"","title":"Future Work"},{"location":"presentation/#generative-ai","text":"Style transfer w/ GANs for higher visual fidelity Forward diffusion process Reverse diffusion process Hallucination Integrate World Foundation Models (e.g., NVIDIA Cosmos)","title":"Generative AI"},{"location":"presentation/#conclusion","text":"SIMLAN : powerful platform for indoor multi-camera robotics Reproducible, scalable, open-source Academia & Industry Roadmap: ML integration, human-robot collaboration, sim-to-real transfer Need your support","title":"Conclusion"},{"location":"presentation/#acknowledgements","text":"INFOTIV AB SMILE IV (Vinnova grant 2023-00789) EUREKA ITEA4 ArtWork (Vinnova grant 2023-00970) INFOTIV Colleagues : P\u00e4r Aronsson, Anton Hill, David Espedalen, Siyu Yi, Anders B\u00e4ckelie, Jacob Rohdin, Vasiliki Kostara, Nazeeh Alhosary, Marwa Naili Other contributors : Tove Casparsson, Filip Melberg (Chalmers), Christoffer Johannesson, Sebastian Olsson, Hjalmar Ruscck from Dyno-robotics, Erik Brorsson (Chalmers/Volvo), Other Partners : Infotiv AB, RISE, Volvo Group, Dyno-Robotics, Chalmers INFOTIV AB Dyno-robotics RISE Research Institutes of Sweden CHALMERS Volvo Group https://infotiv-research.github.io https://github.com/infotiv-research/SIMLAN","title":"Acknowledgements"},{"location":"config_generation/","text":"Config Generation When building the workspaces, the generation scripts found in the scripts/ directory are run. These automatically generate the bt.xml , control.yaml , gz_bridge.yaml , nav2_params.yaml , as well as validate the setup of the ROBOTS list in the config.sh script. This is done to automatically set up the ROS2 controllers, navigation stack parameters, and other /topics depending on what robots and namespaces are spawned. Every time the workspace is built, the generate_XXX files print that they were generated, and at the top of each generated file, a comment specifies from where the auto-generation occurred. As of now, the parameter files are saved in the package in which they're used instead of directly in the share/ directory to increase code readability for new users. Explanation of the different generate_XXX files: generate_bt_xml.py : The bt_navigator needs the namespace of the robot it is checking conditions for. generate_control_yaml.py : The ros2_controller maps namespaced /cmd_vel topics to wheel_joint movements. generate_gz_bridge_yaml.py : Some gz_topics need to be mapped to ROS2 topics. generate_humanoid_control_yaml.py : The ros2_controller maps namespaced /cmd_vel topics to wheel_joint movements for humanoids. generate_humanoid_nav2_params_yaml.py : The individual frame IDs and some ROS2 topics are needed to define humanoids and their corresponding maps. generate_nav2_params_yaml.py : The individual frame IDs and some ROS2 topics are needed to define robots and their corresponding maps. validate_humanoids.py : A converter from string to list of dicts including a sanity check to throw errors if the HUMANOIDS variable is wrongly configured. validate_robots.py : A converter from string to list of dicts including a sanity check to throw errors if the ROBOTS variable is wrongly configured.","title":"Config Generation"},{"location":"config_generation/#config-generation","text":"When building the workspaces, the generation scripts found in the scripts/ directory are run. These automatically generate the bt.xml , control.yaml , gz_bridge.yaml , nav2_params.yaml , as well as validate the setup of the ROBOTS list in the config.sh script. This is done to automatically set up the ROS2 controllers, navigation stack parameters, and other /topics depending on what robots and namespaces are spawned. Every time the workspace is built, the generate_XXX files print that they were generated, and at the top of each generated file, a comment specifies from where the auto-generation occurred. As of now, the parameter files are saved in the package in which they're used instead of directly in the share/ directory to increase code readability for new users. Explanation of the different generate_XXX files: generate_bt_xml.py : The bt_navigator needs the namespace of the robot it is checking conditions for. generate_control_yaml.py : The ros2_controller maps namespaced /cmd_vel topics to wheel_joint movements. generate_gz_bridge_yaml.py : Some gz_topics need to be mapped to ROS2 topics. generate_humanoid_control_yaml.py : The ros2_controller maps namespaced /cmd_vel topics to wheel_joint movements for humanoids. generate_humanoid_nav2_params_yaml.py : The individual frame IDs and some ROS2 topics are needed to define humanoids and their corresponding maps. generate_nav2_params_yaml.py : The individual frame IDs and some ROS2 topics are needed to define robots and their corresponding maps. validate_humanoids.py : A converter from string to list of dicts including a sanity check to throw errors if the HUMANOIDS variable is wrongly configured. validate_robots.py : A converter from string to list of dicts including a sanity check to throw errors if the ROBOTS variable is wrongly configured.","title":"Config Generation"},{"location":"humanoid_utility/","text":"Humanoid Motion Capture This project develops a system for translating human pose detection to humanoid robot motion in simulation environments. Using Google MediaPipe for pose landmark detection from camera input, the system maps detected human poses to corresponding joint movements executed by a humanoid robot in the Gazebo simulator. The implementation leverages MoveIt2 for motion planning and control, with a data generation pipeline that creates training pairs of pose landmarks and robot joint configurations. This project employs a deep neural network to learn the mapping between human pose and humanoid robot motion. The model takes 33 MediaPipe pose landmarks as input and predicts corresponding robot joint positions. The work is based on a master's thesis \"Human Motion Replay on a Simulated Humanoid Robot Using Pose Estimation\" by Tove Casparsson and Siyu Yi, supervised by Hamid Ebadi, June 2025. Terminology: Forward Kinematics (FK) : The process of calculating the position and orientation of a robot's links given the values of its joint parameters (e.g., angles or displacements). In other words, FK answers the question: \"Where is the robot's hand if I know all the joint values? (usually have one answer)\" Inverse Kinematics (IK) : The process of determining the joint parameters (e.g., angles or displacements) required to achieve a desired position and orientation of the robot's links. In other words, IK answers the question: \"What joint values will place the robot\u2019s hand here? (usually have many answers)\" Pose : 3D pose landmarks (MediaPipe) extracted by MediaPipe from a 2D image of human posture Motion : A kinematic instruction (joint parameters) sent to the robot (via MoveIt) for execution. While \"kinematic instruction\" would be a more accurate term, we continue to use \"motion\" for historical reasons (used in the word motion-capture), even though \"motion\" often refers to the difference/movement between two postures (e.g., posture2 - posture1). Motion Capture : Here it means using 2D images to find the motion (kinematic instruction/joint parameters) to instruct a humanoid robot to mimic the human posture. Relevant Documents Debug : for debugging purposes and development Preprocessing : details about pre processing of datasets humanoid_motion_planner - To execute motions in the sim from a motion.json file. Dataset generation To build the synthetic dataset, first ensure that the cameras pointing to the humanoids are active in the ./config.sh . CAMERA_ENABLED_IDS='500 501 502 503' dataset_cameras='500 501 502 503' and rebuild the project with these commands: ./control.sh build The image below describes how the dataset generation system works. To create a humanoid dataset (paired pose data, motion data and reference images) in the DATASET/TRAIN/ directory: As of now the dataset generation uses humanoid_1 hardcoded. TODO: Fix so we can use \"replay_motion_namespace\". ./control.sh dataset TRAIN/ ./control.sh dataset EVAL/ To find the implementation of how the dataset is created i.e. mediapipe, go to pre_processing/media_to_pose_landmark.py . Available datasets We use 20251028-DATASET-TRAINONLY-MULTI.zip (place it in DATASET for training) and 20251028-DATASET-EVAL1000-MULTI.zip (place it in input/ directory for prediction) that are available at SharePoint as our common datasets. Raw dataset shape DATASET: \u2500\u2500 TRAIN \u251c\u2500\u2500 camera_500 \u2502 \u251c\u2500\u2500 pose_data : 1.json , 2.json \u2502 \u2514\u2500\u2500 pose_images : 1.png , 2.png \u251c\u2500\u2500 camera_501 \u2502 \u251c\u2500\u2500 pose_data : 1.json , 1.json \u2502 \u2514\u2500\u2500 pose_images : 1.png, 2.png \u2514\u2500\u2500 motion_data: 1.json, 2.json Cameras Its important to know how the cameras are located. This is the camera setup for the cameras used for dataset creation, make sure to enable them in config.sh : 500: Back side 501: Right side 502: Front side 503: Left side When selecting what cameras to use in \"dataset_cameras\" in config.sh it is important that the order remains the same. To avoid confusion always use the cameras in order: \"500 501 502 503\" meaning \"Back Right Front Left\". Models PyTorch We have more control over the PyTorch model. It has its own implementation of dataset and model definition, located inside utils.py . What is specific about using PyTorch has the possibility to use Optuna is a hyperparameter optimization tool. This tool helps find the best suitable set of hyperparameters given its training data. Input Layer: 33 MediaPipe pose landmarks for each camera (x, y, z coordinates). Output Layer: Robot joint position sequences for humanoid motion control, this results in a total of 47 joints to be outputted. Currently the model hyperparameters ( HyperparameterDict ) in humanoid_utility/pose_to_motion/pytorch/utils.py are tuned using Optuna. Future model can use Optuna for adjusting the values. AutoGluon We use Autogluon tabular predictor . This model is trained as an ensemble (collection) of models where each separate model aims to predict a single joint given the complete input poses. The result from each model is then merged together and becomes a predicted list for each target joint. Model Training To train a model run the command below. The input size depends on the number of cameras you plan on using, meaning that if you only select one camera the input shape will take 1 camera into account. If you select 4 the model will have a 4 times larger input size. To set it up in the way you want, go to config.sh . The variables are: model_instance : If you want to save a model, specify its name here. Keep blank if you don't want to save. dataset_cameras : The cameras you want to train on. This can be a list of cameras i.e. \"500 501 502\". For single training set this to one \"\" model_type : Possible selections: \"pytorch\", \"autogluon\" Keep in mind that we use a separate virtual environment to install machine learning related pip packages called mlenv with a separate requirements.txt . Build the environment first using ./control.sh mlenv . This has to be done once and ./control.sh enables the environment automatically Finally you specify what dataset directory (JSON) you want to use as training data which becomes the second argument, below is an example of running train using the TRAIN/ dataset. ./control.sh mlenv ./control.sh train TRAIN/ After a training run is complete, the model is saved inside of pose_to_motion/{model_type}/output/saved_model_states . A summary report of the session is also generated and will be saved inside pose_to_motion/{model_type}/output/train_report.csv . Model Evaluation ./control.sh eval EVAL/ Running the command above in the terminal runs the evaluation pipeline. The evaluation will run metrics defined in metrics.py ; currently MSE and MAE are calculated. The results are then saved in a summary report located in pose_to_motion/{model_type}/outputs/eval_report.csv . As the result MSE and MAE values are printed. Prediction on synthetic data The prediction pipeline uses these variables in the config.sh : dataset_cameras : What cameras to take into concern for prediction. Should match the number of cameras the model was trained on. model_instance : What trained model instance to use. replay_motion_namespace : What humanoid you want to replay motions on. Default \"humanoid_2\". It is possible to predict on three different types of data: images, videos, and raw JSON data. Set \"save_prediction_output='true'\" in config.sh . If false then no data will be saved and only prediction is run. You can choose if you want to store any motion, pose, image data when predicting, which is stored in the output/ folder. Run the command below to predict on different types of datasets: ./control.sh predict JSON/ # This has the same \"EVAL/\", \"TRAIN/\" directories. ./control.sh predict VIDEOS/ # As in input/VIDEOS/ ./control.sh predict IMAGES/ # As in input/IMAGES/ The input for prediction needs to be structured as the template below. The camera names are connected with the variable dataset_cameras inside config.sh . input/JSON or VIDEOS or IMAGES camera_500/ file_1.(JSON, PNG, JPG, MP4) file_2.(JSON, PNG, JPG, MP4) camera_501/ file_1.(JSON, PNG, JPG, MP4) file_2.(JSON, PNG, JPG, MP4) camera_502/ file_1.(JSON, PNG, JPG, MP4) file_2.(JSON, PNG, JPG, MP4) camera_503/ file_1.(JSON, PNG, JPG, MP4) file_2.(JSON, PNG, JPG, MP4) Predicting on real data/ your own data The predict pipeline works on real data as well. It is for example possible to predict this dataset fit3d*.tar.gz (or alternatively https://fit3d.imar.ro/fit3d.) If you want to predict on your own videos and images you can follow these steps below. This example will use the gym exercise from 5 angles dataset from fit3d*.tar.gz or alternatively https://fit3d.imar.ro/fit3d. This example assume you have a trained ML model on 4 cameras. Please follow humanoid_utility/README.md for how to train a model. download and extract the fit3d*.tar.gz dataset. Find a suitable video feed from multiple angles. You can find good examples in the folder \"fit3d_train/train/s03/videos/\". In this folder you will find subfolders named \"50591643\", \"58860488\", \"60457274\", \"65906101\". These represent 4 different camera angles. It is the same when we name our \"dataset_cameras\" in config.sh as \"500\",\"501\",\"502\",\"503\". Going into any of the subfolders there are many different videos, this example uses \"band_pull_apart.mp4\" add a new folder inside input/ with whatever name you want. We will use the name input/band_pull_apart/ For this guide we will rename the folders \"50591643\", \"58860488\", \"60457274\", \"65906101\" to \"500\",\"501\",\"502\",\"503\". IMPORTANT : The angles for this example is not accurate since angles: \"50591643\" & \"58860488\" both show back side and \"60457274\", \"65906101\" show the front. Thus there become a mismatch from our camera setup where we use back, right, front, left. The ML model will become confused by the angles if not taken into account. Add subfolders inside input/band_pull_apart/ named after the cameras and add respective video inside of it. This example has 4 videos: band_pull_apart/camera_500/band_pull_apart.mp4 band_pull_apart/camera_50X/band_pull_apart.mp4 IMPORTANT: Make sure variable \"dataset_cameras\" match the names of the folders. Now we can run the following command: ./control.sh predict band_pull_apart # based on input/band_pull_apart/ The videos will now be processed and the result is found in output/band_pull_apart/ . Gazebo will also open and execute the motions from the videos. Project Structure: humanoid_config.py : it contains the humanoid configuration like the number of joints and the number of pose landmarks. humanoid_ml_requirements.txt : Contains the humanoid machine learning requirements /DATASET/TRAIN/ : Contains motion, pose and image files generated by ./control.sh dataset TRAIN motion_data : Corresponding random motion (request) camera/pose_data : Mediapipe pose (result) for each camera camera/pose_images : Mediapipe annotated images and original images for each camera /DATASET/EVAL/ : Same as TRAIN/ input/ : This folder has the pose data to be predicted output/ : This folder saves the predicted motion data and intermediate results pre_processing/ : contains all pre-processing files, used to convert the JSON data into tabular data. media_to_pose_landmark.py : Handles pose detection from actual images or videos using MediaPipe dataframe_json_bridge.py : Contains helper functions to convert json files into dataframe/numpy format and is used by the models. Running its main will generate csv files that contains both pose and motion data from the json files. Inputs the cameras you want to use, the json dir, the csv filename to generate. pose_to_motion/ : Contains the ML training pipeline for pose-to-motion prediction model.py : The main file containing implementations for training, evaluating, and predicting motions. This file acts as an orchestrator and inputs the data, what model to use, and what operation should be done. pytorch/ : directory containing model implementation for pytorch model. Handles training, evaluation, and predicting motions. framework.py : Framework file, contains the implementation for train, evaluation, and prediction. utils.py : Utils file, contains helper methods, and model definition. The model.py use this. output/ : Contain reports of ran sessions and saved model states. Saved models will be found here saved_model_states/ : Saved autogluon models are stored here reports/ : Manually generated reports file containing training/evaluation run information. Good for reproducibility. The reports are generated by generate_report.py and the new report row is saved in train_report.csv or eval_report.csv optuna_studies/ : Specific for pytorch. Optuna studies are stored at this folder. See optuna section for more information about Optuna. autogluon/ : directory containing model implementation for autogluon model. Handles training, evaluation, and predicting motions. Has similar files as pytorch. framework.py utils.py output/ saved_model_states/ reports/ generate_report.py : Contains methods to generate report files, used after a training or evaluation session has been done for any selected model. These generated reports contain information for reproducing runs and how a model performed against metrics. A new report is generated every time we run train or eval on a model and every report is saved in a single CSV file where every row is a report. The files are named: train_report.csv or eval_report.csv . metrics.py : Contains all functions for calculating metrics for models for any selected model.","title":"Humanoid Motion Capture"},{"location":"humanoid_utility/#humanoid-motion-capture","text":"This project develops a system for translating human pose detection to humanoid robot motion in simulation environments. Using Google MediaPipe for pose landmark detection from camera input, the system maps detected human poses to corresponding joint movements executed by a humanoid robot in the Gazebo simulator. The implementation leverages MoveIt2 for motion planning and control, with a data generation pipeline that creates training pairs of pose landmarks and robot joint configurations. This project employs a deep neural network to learn the mapping between human pose and humanoid robot motion. The model takes 33 MediaPipe pose landmarks as input and predicts corresponding robot joint positions. The work is based on a master's thesis \"Human Motion Replay on a Simulated Humanoid Robot Using Pose Estimation\" by Tove Casparsson and Siyu Yi, supervised by Hamid Ebadi, June 2025.","title":"Humanoid Motion Capture"},{"location":"humanoid_utility/#terminology","text":"Forward Kinematics (FK) : The process of calculating the position and orientation of a robot's links given the values of its joint parameters (e.g., angles or displacements). In other words, FK answers the question: \"Where is the robot's hand if I know all the joint values? (usually have one answer)\" Inverse Kinematics (IK) : The process of determining the joint parameters (e.g., angles or displacements) required to achieve a desired position and orientation of the robot's links. In other words, IK answers the question: \"What joint values will place the robot\u2019s hand here? (usually have many answers)\" Pose : 3D pose landmarks (MediaPipe) extracted by MediaPipe from a 2D image of human posture Motion : A kinematic instruction (joint parameters) sent to the robot (via MoveIt) for execution. While \"kinematic instruction\" would be a more accurate term, we continue to use \"motion\" for historical reasons (used in the word motion-capture), even though \"motion\" often refers to the difference/movement between two postures (e.g., posture2 - posture1). Motion Capture : Here it means using 2D images to find the motion (kinematic instruction/joint parameters) to instruct a humanoid robot to mimic the human posture.","title":"Terminology:"},{"location":"humanoid_utility/#relevant-documents","text":"Debug : for debugging purposes and development Preprocessing : details about pre processing of datasets humanoid_motion_planner - To execute motions in the sim from a motion.json file.","title":"Relevant Documents"},{"location":"humanoid_utility/#dataset-generation","text":"To build the synthetic dataset, first ensure that the cameras pointing to the humanoids are active in the ./config.sh . CAMERA_ENABLED_IDS='500 501 502 503' dataset_cameras='500 501 502 503' and rebuild the project with these commands: ./control.sh build The image below describes how the dataset generation system works. To create a humanoid dataset (paired pose data, motion data and reference images) in the DATASET/TRAIN/ directory: As of now the dataset generation uses humanoid_1 hardcoded. TODO: Fix so we can use \"replay_motion_namespace\". ./control.sh dataset TRAIN/ ./control.sh dataset EVAL/ To find the implementation of how the dataset is created i.e. mediapipe, go to pre_processing/media_to_pose_landmark.py .","title":"Dataset generation"},{"location":"humanoid_utility/#available-datasets","text":"We use 20251028-DATASET-TRAINONLY-MULTI.zip (place it in DATASET for training) and 20251028-DATASET-EVAL1000-MULTI.zip (place it in input/ directory for prediction) that are available at SharePoint as our common datasets.","title":"Available datasets"},{"location":"humanoid_utility/#raw-dataset-shape","text":"DATASET: \u2500\u2500 TRAIN \u251c\u2500\u2500 camera_500 \u2502 \u251c\u2500\u2500 pose_data : 1.json , 2.json \u2502 \u2514\u2500\u2500 pose_images : 1.png , 2.png \u251c\u2500\u2500 camera_501 \u2502 \u251c\u2500\u2500 pose_data : 1.json , 1.json \u2502 \u2514\u2500\u2500 pose_images : 1.png, 2.png \u2514\u2500\u2500 motion_data: 1.json, 2.json","title":"Raw dataset shape"},{"location":"humanoid_utility/#cameras","text":"Its important to know how the cameras are located. This is the camera setup for the cameras used for dataset creation, make sure to enable them in config.sh : 500: Back side 501: Right side 502: Front side 503: Left side When selecting what cameras to use in \"dataset_cameras\" in config.sh it is important that the order remains the same. To avoid confusion always use the cameras in order: \"500 501 502 503\" meaning \"Back Right Front Left\".","title":"Cameras"},{"location":"humanoid_utility/#models","text":"","title":"Models"},{"location":"humanoid_utility/#pytorch","text":"We have more control over the PyTorch model. It has its own implementation of dataset and model definition, located inside utils.py . What is specific about using PyTorch has the possibility to use Optuna is a hyperparameter optimization tool. This tool helps find the best suitable set of hyperparameters given its training data. Input Layer: 33 MediaPipe pose landmarks for each camera (x, y, z coordinates). Output Layer: Robot joint position sequences for humanoid motion control, this results in a total of 47 joints to be outputted. Currently the model hyperparameters ( HyperparameterDict ) in humanoid_utility/pose_to_motion/pytorch/utils.py are tuned using Optuna. Future model can use Optuna for adjusting the values.","title":"PyTorch"},{"location":"humanoid_utility/#autogluon","text":"We use Autogluon tabular predictor . This model is trained as an ensemble (collection) of models where each separate model aims to predict a single joint given the complete input poses. The result from each model is then merged together and becomes a predicted list for each target joint.","title":"AutoGluon"},{"location":"humanoid_utility/#model-training","text":"To train a model run the command below. The input size depends on the number of cameras you plan on using, meaning that if you only select one camera the input shape will take 1 camera into account. If you select 4 the model will have a 4 times larger input size. To set it up in the way you want, go to config.sh . The variables are: model_instance : If you want to save a model, specify its name here. Keep blank if you don't want to save. dataset_cameras : The cameras you want to train on. This can be a list of cameras i.e. \"500 501 502\". For single training set this to one \"\" model_type : Possible selections: \"pytorch\", \"autogluon\" Keep in mind that we use a separate virtual environment to install machine learning related pip packages called mlenv with a separate requirements.txt . Build the environment first using ./control.sh mlenv . This has to be done once and ./control.sh enables the environment automatically Finally you specify what dataset directory (JSON) you want to use as training data which becomes the second argument, below is an example of running train using the TRAIN/ dataset. ./control.sh mlenv ./control.sh train TRAIN/ After a training run is complete, the model is saved inside of pose_to_motion/{model_type}/output/saved_model_states . A summary report of the session is also generated and will be saved inside pose_to_motion/{model_type}/output/train_report.csv .","title":"Model Training"},{"location":"humanoid_utility/#model-evaluation","text":"./control.sh eval EVAL/ Running the command above in the terminal runs the evaluation pipeline. The evaluation will run metrics defined in metrics.py ; currently MSE and MAE are calculated. The results are then saved in a summary report located in pose_to_motion/{model_type}/outputs/eval_report.csv . As the result MSE and MAE values are printed.","title":"Model Evaluation"},{"location":"humanoid_utility/#prediction-on-synthetic-data","text":"The prediction pipeline uses these variables in the config.sh : dataset_cameras : What cameras to take into concern for prediction. Should match the number of cameras the model was trained on. model_instance : What trained model instance to use. replay_motion_namespace : What humanoid you want to replay motions on. Default \"humanoid_2\". It is possible to predict on three different types of data: images, videos, and raw JSON data. Set \"save_prediction_output='true'\" in config.sh . If false then no data will be saved and only prediction is run. You can choose if you want to store any motion, pose, image data when predicting, which is stored in the output/ folder. Run the command below to predict on different types of datasets: ./control.sh predict JSON/ # This has the same \"EVAL/\", \"TRAIN/\" directories. ./control.sh predict VIDEOS/ # As in input/VIDEOS/ ./control.sh predict IMAGES/ # As in input/IMAGES/ The input for prediction needs to be structured as the template below. The camera names are connected with the variable dataset_cameras inside config.sh . input/JSON or VIDEOS or IMAGES camera_500/ file_1.(JSON, PNG, JPG, MP4) file_2.(JSON, PNG, JPG, MP4) camera_501/ file_1.(JSON, PNG, JPG, MP4) file_2.(JSON, PNG, JPG, MP4) camera_502/ file_1.(JSON, PNG, JPG, MP4) file_2.(JSON, PNG, JPG, MP4) camera_503/ file_1.(JSON, PNG, JPG, MP4) file_2.(JSON, PNG, JPG, MP4)","title":"Prediction on synthetic data"},{"location":"humanoid_utility/#predicting-on-real-data-your-own-data","text":"The predict pipeline works on real data as well. It is for example possible to predict this dataset fit3d*.tar.gz (or alternatively https://fit3d.imar.ro/fit3d.) If you want to predict on your own videos and images you can follow these steps below. This example will use the gym exercise from 5 angles dataset from fit3d*.tar.gz or alternatively https://fit3d.imar.ro/fit3d. This example assume you have a trained ML model on 4 cameras. Please follow humanoid_utility/README.md for how to train a model. download and extract the fit3d*.tar.gz dataset. Find a suitable video feed from multiple angles. You can find good examples in the folder \"fit3d_train/train/s03/videos/\". In this folder you will find subfolders named \"50591643\", \"58860488\", \"60457274\", \"65906101\". These represent 4 different camera angles. It is the same when we name our \"dataset_cameras\" in config.sh as \"500\",\"501\",\"502\",\"503\". Going into any of the subfolders there are many different videos, this example uses \"band_pull_apart.mp4\" add a new folder inside input/ with whatever name you want. We will use the name input/band_pull_apart/ For this guide we will rename the folders \"50591643\", \"58860488\", \"60457274\", \"65906101\" to \"500\",\"501\",\"502\",\"503\". IMPORTANT : The angles for this example is not accurate since angles: \"50591643\" & \"58860488\" both show back side and \"60457274\", \"65906101\" show the front. Thus there become a mismatch from our camera setup where we use back, right, front, left. The ML model will become confused by the angles if not taken into account. Add subfolders inside input/band_pull_apart/ named after the cameras and add respective video inside of it. This example has 4 videos: band_pull_apart/camera_500/band_pull_apart.mp4 band_pull_apart/camera_50X/band_pull_apart.mp4 IMPORTANT: Make sure variable \"dataset_cameras\" match the names of the folders. Now we can run the following command: ./control.sh predict band_pull_apart # based on input/band_pull_apart/ The videos will now be processed and the result is found in output/band_pull_apart/ . Gazebo will also open and execute the motions from the videos.","title":"Predicting on real data/ your own data"},{"location":"humanoid_utility/#project-structure","text":"humanoid_config.py : it contains the humanoid configuration like the number of joints and the number of pose landmarks. humanoid_ml_requirements.txt : Contains the humanoid machine learning requirements /DATASET/TRAIN/ : Contains motion, pose and image files generated by ./control.sh dataset TRAIN motion_data : Corresponding random motion (request) camera/pose_data : Mediapipe pose (result) for each camera camera/pose_images : Mediapipe annotated images and original images for each camera /DATASET/EVAL/ : Same as TRAIN/ input/ : This folder has the pose data to be predicted output/ : This folder saves the predicted motion data and intermediate results pre_processing/ : contains all pre-processing files, used to convert the JSON data into tabular data. media_to_pose_landmark.py : Handles pose detection from actual images or videos using MediaPipe dataframe_json_bridge.py : Contains helper functions to convert json files into dataframe/numpy format and is used by the models. Running its main will generate csv files that contains both pose and motion data from the json files. Inputs the cameras you want to use, the json dir, the csv filename to generate. pose_to_motion/ : Contains the ML training pipeline for pose-to-motion prediction model.py : The main file containing implementations for training, evaluating, and predicting motions. This file acts as an orchestrator and inputs the data, what model to use, and what operation should be done. pytorch/ : directory containing model implementation for pytorch model. Handles training, evaluation, and predicting motions. framework.py : Framework file, contains the implementation for train, evaluation, and prediction. utils.py : Utils file, contains helper methods, and model definition. The model.py use this. output/ : Contain reports of ran sessions and saved model states. Saved models will be found here saved_model_states/ : Saved autogluon models are stored here reports/ : Manually generated reports file containing training/evaluation run information. Good for reproducibility. The reports are generated by generate_report.py and the new report row is saved in train_report.csv or eval_report.csv optuna_studies/ : Specific for pytorch. Optuna studies are stored at this folder. See optuna section for more information about Optuna. autogluon/ : directory containing model implementation for autogluon model. Handles training, evaluation, and predicting motions. Has similar files as pytorch. framework.py utils.py output/ saved_model_states/ reports/ generate_report.py : Contains methods to generate report files, used after a training or evaluation session has been done for any selected model. These generated reports contain information for reproducing runs and how a model performed against metrics. A new report is generated every time we run train or eval on a model and every report is saved in a single CSV file where every row is a report. The files are named: train_report.csv or eval_report.csv . metrics.py : Contains all functions for calculating metrics for models for any selected model.","title":"Project Structure:"},{"location":"humanoid_utility/DEBUG/","text":"Model structure model.py is responsible for routing the training and prediction to the right model. When running model.py train modelX=pytorch_model cams=[500,501] from ./control.sh , the pseudocode below in model.py loads the right model and passes the needed information for training and prediction to the model. import modelX modelX.train(pose_NP, motion_NP, cams) motion = modelX.train(pose_NP, cams) pred= modelX.train(pose_NP, cams) modelX can now have all information that it needs for training or prediction. Each model has to define at least the following interfaces: modelX.py: def modelX.train(): FL_POSE = FLATTEN_POSE_BY_SCENARIO(pose_NP, cams) // 1, [LDATA_a], [LDATA_b] x = join(FL_POSE, motion_NP) // [LDATA_a], [LDATA_b], [JDATA_a] train with the data above SAVE_MODEL_AS(str(cams)) def modelX.predict(pose_NP, cams): FL_POSE = FLATTEN_POSE_BY_SCENARIO(pose_NP, cams) LOAD_MODEL(str(cams), FL_POSE) WHATEVER return motion_np Preprocessing Pose landmark detected by Mediapipe : These landmarks can be normalised in many ways. Three potential steps of normalization (feature engineering) of MediaPipe landmarks: Position normalization Scale normalization Rotation normalization","title":"Humanoid internal"},{"location":"humanoid_utility/DEBUG/#model-structure","text":"model.py is responsible for routing the training and prediction to the right model. When running model.py train modelX=pytorch_model cams=[500,501] from ./control.sh , the pseudocode below in model.py loads the right model and passes the needed information for training and prediction to the model. import modelX modelX.train(pose_NP, motion_NP, cams) motion = modelX.train(pose_NP, cams) pred= modelX.train(pose_NP, cams) modelX can now have all information that it needs for training or prediction. Each model has to define at least the following interfaces: modelX.py: def modelX.train(): FL_POSE = FLATTEN_POSE_BY_SCENARIO(pose_NP, cams) // 1, [LDATA_a], [LDATA_b] x = join(FL_POSE, motion_NP) // [LDATA_a], [LDATA_b], [JDATA_a] train with the data above SAVE_MODEL_AS(str(cams)) def modelX.predict(pose_NP, cams): FL_POSE = FLATTEN_POSE_BY_SCENARIO(pose_NP, cams) LOAD_MODEL(str(cams), FL_POSE) WHATEVER return motion_np","title":"Model structure"},{"location":"humanoid_utility/DEBUG/#preprocessing","text":"Pose landmark detected by Mediapipe : These landmarks can be normalised in many ways. Three potential steps of normalization (feature engineering) of MediaPipe landmarks: Position normalization Scale normalization Rotation normalization","title":"Preprocessing"},{"location":"humanoid_utility/pose_to_motion/autogluon/","text":"Autogluon Model configuration This section describes how AutoGluon is configured for a multi-label regression problem in our current setup. To use AutoGluon for a multi-label regression problem, first we need to create a MultilabelPredictor by setting labels : the labels that we want to predict. problem_types : the problem type for each TabularPredictor. path : path to the directory where models and intermediate outputs should be saved. consider_labels_correlation : Whether the predictions of multiple labels should account for label correlations or predict each label independently of the others. For consider_labels_correlation , we set it to FALSE in order to disable using one label as a feature for another. The reasons for this are: Each joint has its own degree of freedom and control, and it is independent from the rest of the joints. Training stability without error propagation. When set to False, the training will be faster. More general and simple model. For training the model, we should take into consideration the following hyperparameters related to: Stacking: which is an ensemble technique where multiple models are trained and then a \"meta-model\" learns how to combine their predictions. If we use dynamic Stacking, it will automatically determine the optimal number of stacking layers and which models to include. Bagging: multiple training versions of the same model on different subsets of data and combining their predictions. preset: which condenses the complex hyperparameter setups. For example, we can use a small model size by using medium_quality, which will lead to faster training but with less prediction quality. Or we can use a large model by setting preset to best_quality, which will lead to better performance but much longer training time. The current AutoGluon uses the following hyperparameters: dynamic_stacking=False : Disables the automatic stacking optimization. num_stack_levels=0 : no stacking level. auto_stack=False : Disables automatic ensemble stacking. num_bag_folds=0 and num_bag_sets=1 , which means no bagging. So here is the current training flow for each joint: Train NN_TORCH, GBM, and XGB models. No bagging: Each model trains on the full dataset once, no multiple training versions of the same model on different subsets of data and combining their predictions. No stacking: No meta-models combining predictions. Best model selection: Choose the best performing model per joint. Eliminating dynamic stacking and multi-level stacking will lead to 50% to 70% time savings. Overall, this configuration prioritizes training speed and stability over ensemble complexity, making it suitable for fast iteration and independent joint predictions. Hyperparameter Tuning In the current AutoGluon model, we are training NN_TORCH, GBM, and XGB models by using their default built-in hyperparameter settings. The next step for performance improvement is to add the hyperparameter_tune_kwargs argument to enable AutoGluon\u2019s internal hyperparameter optimization. This will allow AutoGluon to automatically search for the best hyperparameters for each model type, balancing training time and prediction quality.","title":"Pose to Motion AutoGluon model"},{"location":"humanoid_utility/pose_to_motion/autogluon/#autogluon-model-configuration","text":"This section describes how AutoGluon is configured for a multi-label regression problem in our current setup. To use AutoGluon for a multi-label regression problem, first we need to create a MultilabelPredictor by setting labels : the labels that we want to predict. problem_types : the problem type for each TabularPredictor. path : path to the directory where models and intermediate outputs should be saved. consider_labels_correlation : Whether the predictions of multiple labels should account for label correlations or predict each label independently of the others. For consider_labels_correlation , we set it to FALSE in order to disable using one label as a feature for another. The reasons for this are: Each joint has its own degree of freedom and control, and it is independent from the rest of the joints. Training stability without error propagation. When set to False, the training will be faster. More general and simple model. For training the model, we should take into consideration the following hyperparameters related to: Stacking: which is an ensemble technique where multiple models are trained and then a \"meta-model\" learns how to combine their predictions. If we use dynamic Stacking, it will automatically determine the optimal number of stacking layers and which models to include. Bagging: multiple training versions of the same model on different subsets of data and combining their predictions. preset: which condenses the complex hyperparameter setups. For example, we can use a small model size by using medium_quality, which will lead to faster training but with less prediction quality. Or we can use a large model by setting preset to best_quality, which will lead to better performance but much longer training time. The current AutoGluon uses the following hyperparameters: dynamic_stacking=False : Disables the automatic stacking optimization. num_stack_levels=0 : no stacking level. auto_stack=False : Disables automatic ensemble stacking. num_bag_folds=0 and num_bag_sets=1 , which means no bagging. So here is the current training flow for each joint: Train NN_TORCH, GBM, and XGB models. No bagging: Each model trains on the full dataset once, no multiple training versions of the same model on different subsets of data and combining their predictions. No stacking: No meta-models combining predictions. Best model selection: Choose the best performing model per joint. Eliminating dynamic stacking and multi-level stacking will lead to 50% to 70% time savings. Overall, this configuration prioritizes training speed and stability over ensemble complexity, making it suitable for fast iteration and independent joint predictions.","title":"Autogluon Model configuration"},{"location":"humanoid_utility/pose_to_motion/autogluon/#hyperparameter-tuning","text":"In the current AutoGluon model, we are training NN_TORCH, GBM, and XGB models by using their default built-in hyperparameter settings. The next step for performance improvement is to add the hyperparameter_tune_kwargs argument to enable AutoGluon\u2019s internal hyperparameter optimization. This will allow AutoGluon to automatically search for the best hyperparameters for each model type, balancing training time and prediction quality.","title":"Hyperparameter Tuning"},{"location":"humanoid_utility/pre_processing/","text":"Dataset preprocessing media_to_pose_landmark.py : To convert media file to pose landmarks using mediapipe. dataframe_json_bridge.py : to load poses and motions into a tabulated dataframe. Models MUST use dataframe_json_bridge.py standard interface to load their data. This tools also has a cli interface to see the intermediate results that can be accessed by ./control.sh convert2csv ... . Pose and motion data are first collected from JSON files and combined into a single CSV file. These processed CSV files are ready to be fed into a model of your choice, AutoGluon or PyTorch. See below to find more information about the dataset or more about the models . The dataset, inputs, and outputs are in humanoid_utility/DATASET/ directory. The resulting files are stored as parallel .json files in multi-camera folders camera_*/pose_data , camera_*/motion_data , camera_*/pose_images . To avoid creation of pose_images that are only used as the ground truth and debugging purposes (especially if you are building your training data), comment out the call to self.save_pose_image() in camera_viewer.py . Finally, to merge them all into a tabular .csv file, run the following command. The argument dataset_dir is the directory containing the JSON files and must be located as a subdirectory of DATASET/ . Additionally, in config.sh you need to set the variable dataset_cameras to the cameras you want to take into consideration. i.e., if you use all cameras, the value is \"500 501 502 503\". The resulting CSV file will then be saved as dataset_dir/flattened_data.csv . By running the command below results in the following csv files being created: ./control.sh convert2csv TRAIN ./control.sh convert2csv EVAL Results in merging: DATASET/TRAIN/** to DATASET/TRAIN/flattened_data.csv DATASET/EVAL/** to DATASET/EVAL/flattened_data.csv Note: The TRAIN/** refers to the contents of folders motion_data/ and camera_5XX/pose_data . The resulting CSV file will contain these columns: cam500_0_x,cam501_0_x,cam502_0_x,cam503_0_x,...,cam503_32_z, JOINTS... When the data is transformed into numpy/tabular format, each row in the CSV file represents multiple pose samples from each camera and its corresponding robot motion. The number of columns depends on the number of cameras, and it's possible to select what camera inputs you want to use; i.e., just one camera input or up to four camera inputs. The first columns are the 3D coordinates (x, y, z) for each of the 33 MediaPipe pose landmarks and for each camera , named like cam500_0_x, cam501_0_x, cam502_0_x, cam503_0_x, cam500_0_y, cam501_0_y, cam502_0_y, cam503_0_y, cam500_0_z, cam501_0_z, cam502_0_z, cam503_0_z... . The remaining columns are the robot joint positions (motion targets) for that sample, with names like jRightShoulder_rotx, jLeftElbow_roty, etc. Loading Dataset For training and prediction, we have a helper library, dataframe_json_bridge.py , that loads the data into a pandas dataframe with the column size of num_pose_landmarks * num_cameras * 3 + num_humanoid_joints Media to pose python3 $humanoid_utility_dir/pre_processing/media_to_pose_landmark.py \\ --mode $2 \\ # image or video \\ --data_dir $3 \\ # input dataset directory name \\ --camera_ids \"$dataset_cameras\" \\ --input_dir $humanoid_input_dir \\ --output_dir $humanoid_output_dir","title":"Humanoid Preprocessing"},{"location":"humanoid_utility/pre_processing/#dataset-preprocessing","text":"media_to_pose_landmark.py : To convert media file to pose landmarks using mediapipe. dataframe_json_bridge.py : to load poses and motions into a tabulated dataframe. Models MUST use dataframe_json_bridge.py standard interface to load their data. This tools also has a cli interface to see the intermediate results that can be accessed by ./control.sh convert2csv ... . Pose and motion data are first collected from JSON files and combined into a single CSV file. These processed CSV files are ready to be fed into a model of your choice, AutoGluon or PyTorch. See below to find more information about the dataset or more about the models . The dataset, inputs, and outputs are in humanoid_utility/DATASET/ directory. The resulting files are stored as parallel .json files in multi-camera folders camera_*/pose_data , camera_*/motion_data , camera_*/pose_images . To avoid creation of pose_images that are only used as the ground truth and debugging purposes (especially if you are building your training data), comment out the call to self.save_pose_image() in camera_viewer.py . Finally, to merge them all into a tabular .csv file, run the following command. The argument dataset_dir is the directory containing the JSON files and must be located as a subdirectory of DATASET/ . Additionally, in config.sh you need to set the variable dataset_cameras to the cameras you want to take into consideration. i.e., if you use all cameras, the value is \"500 501 502 503\". The resulting CSV file will then be saved as dataset_dir/flattened_data.csv . By running the command below results in the following csv files being created: ./control.sh convert2csv TRAIN ./control.sh convert2csv EVAL Results in merging: DATASET/TRAIN/** to DATASET/TRAIN/flattened_data.csv DATASET/EVAL/** to DATASET/EVAL/flattened_data.csv Note: The TRAIN/** refers to the contents of folders motion_data/ and camera_5XX/pose_data . The resulting CSV file will contain these columns: cam500_0_x,cam501_0_x,cam502_0_x,cam503_0_x,...,cam503_32_z, JOINTS... When the data is transformed into numpy/tabular format, each row in the CSV file represents multiple pose samples from each camera and its corresponding robot motion. The number of columns depends on the number of cameras, and it's possible to select what camera inputs you want to use; i.e., just one camera input or up to four camera inputs. The first columns are the 3D coordinates (x, y, z) for each of the 33 MediaPipe pose landmarks and for each camera , named like cam500_0_x, cam501_0_x, cam502_0_x, cam503_0_x, cam500_0_y, cam501_0_y, cam502_0_y, cam503_0_y, cam500_0_z, cam501_0_z, cam502_0_z, cam503_0_z... . The remaining columns are the robot joint positions (motion targets) for that sample, with names like jRightShoulder_rotx, jLeftElbow_roty, etc.","title":"Dataset preprocessing"},{"location":"humanoid_utility/pre_processing/#loading-dataset","text":"For training and prediction, we have a helper library, dataframe_json_bridge.py , that loads the data into a pandas dataframe with the column size of num_pose_landmarks * num_cameras * 3 + num_humanoid_joints","title":"Loading Dataset"},{"location":"humanoid_utility/pre_processing/#media-to-pose","text":"python3 $humanoid_utility_dir/pre_processing/media_to_pose_landmark.py \\ --mode $2 \\ # image or video \\ --data_dir $3 \\ # input dataset directory name \\ --camera_ids \"$dataset_cameras\" \\ --input_dir $humanoid_input_dir \\ --output_dir $humanoid_output_dir","title":"Media to pose"},{"location":"integration_tests/","text":"Integration tests To run the tests in any package you need to run: colcon test or for specific packages: colcon test --packages-select integration_tests Here is an example of a more complex command you can run: colcon test --packages-select integration_tests --event-handlers console_direct+ --merge-install --pytest-args \"-s\" . --event-handlers console_direct+ : prints output to console --merge-install : Use a merged install space instead of one per package. --pytest-args \"-s\" : Pass -s to pytest and do not capture stdout/stderr, so print() shows up. Adding tests. If you want to add your own tests, the most important part is to prefix the filename with \"test_\" in order for colcon to find it. A template for a test file can look like this below containing a fixture/launch_description and a test case: ### Fixture @launch_pytest.fixture(autouse=True) def launch_description(): ######## Arguments ######## world_setup = \"default\" log_level = \"error\" humanoid_str = '[{\"namespace\": \"humanoid_1\",\"initial_pose_x\":10,\"initial_pose_y\":0.0}]' ######## Launch-files / Processes ######## os.environ[\"ROS_DOMAIN_ID\"] = str(os.getpid() % 232) # sim_proc = ExecuteProcess( cmd=[ \"ros2\", \"launch\", \"simlan_bringup\", \"sim.launch.py\", f\"log_level:={log_level}\", f\"world_setup:={world_setup}\", f\"headless_gazebo:=true\", ], output=\"screen\", ) humanoid_proc = ExecuteProcess( cmd=[ \"ros2\", \"launch\", \"humanoid_robot\", \"multiple_humanoid_spawn.launch.py\", f\"log_level:={log_level}\", f\"humanoids:={humanoid_str}\", ], output=\"screen\", ) ######## Test execution ######## yield launch.LaunchDescription( [ sim_proc, humanoid_proc, launch_pytest.actions.ReadyToTest(), ] ) ### TEST @pytest.mark.launch(fixture=launch_description) async def test_sim_and_multiple_robots_bringup_Startup_Nodes_and_topics_should_be_visible(): print( \"STARTING TEST: sim_and_multiple_robots_bringup_Startup_Nodes_and_topics_should_be_visible \" ) Information This package aims to test all the features in the project with the use of Launch_pytest as the testing framework. To view the implemented tests, go to the test/ directory. All tests need to be named with the prefix test_ for colcon to run them. Each test file should only cover one area and also, each test should only cover one feature within the test. Meaning a test file might cover humanoid, and one of the tests, checks if it can start normally. A helper node is used that acts as the communicator to the ros2 interface. Its primarily used to fetch the topics and nodes that exist. This helper node is placed here . The structure within each file is the following: Fixture The test fixture acts as the setup for each test, where repeated logic exists. Before every test is ran, the fixture runs, and when the test is finished, the fixture shuts down. Test method The tests have been written to follow the guideline from osherove.com where each test covers one thing only and its naming convention is \"UnitOfWork_StateUnderTest_ExpectedBehavior\".","title":"Index"},{"location":"integration_tests/#integration-tests","text":"To run the tests in any package you need to run: colcon test or for specific packages: colcon test --packages-select integration_tests Here is an example of a more complex command you can run: colcon test --packages-select integration_tests --event-handlers console_direct+ --merge-install --pytest-args \"-s\" . --event-handlers console_direct+ : prints output to console --merge-install : Use a merged install space instead of one per package. --pytest-args \"-s\" : Pass -s to pytest and do not capture stdout/stderr, so print() shows up.","title":"Integration tests"},{"location":"integration_tests/#adding-tests","text":"If you want to add your own tests, the most important part is to prefix the filename with \"test_\" in order for colcon to find it. A template for a test file can look like this below containing a fixture/launch_description and a test case: ### Fixture @launch_pytest.fixture(autouse=True) def launch_description(): ######## Arguments ######## world_setup = \"default\" log_level = \"error\" humanoid_str = '[{\"namespace\": \"humanoid_1\",\"initial_pose_x\":10,\"initial_pose_y\":0.0}]' ######## Launch-files / Processes ######## os.environ[\"ROS_DOMAIN_ID\"] = str(os.getpid() % 232) # sim_proc = ExecuteProcess( cmd=[ \"ros2\", \"launch\", \"simlan_bringup\", \"sim.launch.py\", f\"log_level:={log_level}\", f\"world_setup:={world_setup}\", f\"headless_gazebo:=true\", ], output=\"screen\", ) humanoid_proc = ExecuteProcess( cmd=[ \"ros2\", \"launch\", \"humanoid_robot\", \"multiple_humanoid_spawn.launch.py\", f\"log_level:={log_level}\", f\"humanoids:={humanoid_str}\", ], output=\"screen\", ) ######## Test execution ######## yield launch.LaunchDescription( [ sim_proc, humanoid_proc, launch_pytest.actions.ReadyToTest(), ] ) ### TEST @pytest.mark.launch(fixture=launch_description) async def test_sim_and_multiple_robots_bringup_Startup_Nodes_and_topics_should_be_visible(): print( \"STARTING TEST: sim_and_multiple_robots_bringup_Startup_Nodes_and_topics_should_be_visible \" )","title":"Adding tests."},{"location":"integration_tests/#information","text":"This package aims to test all the features in the project with the use of Launch_pytest as the testing framework. To view the implemented tests, go to the test/ directory. All tests need to be named with the prefix test_ for colcon to run them. Each test file should only cover one area and also, each test should only cover one feature within the test. Meaning a test file might cover humanoid, and one of the tests, checks if it can start normally. A helper node is used that acts as the communicator to the ros2 interface. Its primarily used to fetch the topics and nodes that exist. This helper node is placed here . The structure within each file is the following:","title":"Information"},{"location":"integration_tests/#fixture","text":"The test fixture acts as the setup for each test, where repeated logic exists. Before every test is ran, the fixture runs, and when the test is finished, the fixture shuts down.","title":"Fixture"},{"location":"integration_tests/#test-method","text":"The tests have been written to follow the guideline from osherove.com where each test covers one thing only and its naming convention is \"UnitOfWork_StateUnderTest_ExpectedBehavior\".","title":"Test method"},{"location":"resources/build-documentation/","text":"This script is responsible to build two type of documentations: a PDF file using Pandoc a webpage using MkDocs Run the bash script OUTSIDE the visual studio dev-container within this directory.","title":"Index"},{"location":"resources/diagrams/","text":"high-level_diagram_SIMLAN.drawio Humanoid_mocap_flow.drawio humanoid_mocap_pipeline.drawio launch_bringup.drawio SIMLAN_DIAGRAM.drawio","title":"Diagrams"},{"location":"resources/diagrams/#high-level_diagram_simlandrawio","text":"","title":"high-level_diagram_SIMLAN.drawio"},{"location":"resources/diagrams/#humanoid_mocap_flowdrawio","text":"","title":"Humanoid_mocap_flow.drawio"},{"location":"resources/diagrams/#humanoid_mocap_pipelinedrawio","text":"","title":"humanoid_mocap_pipeline.drawio"},{"location":"resources/diagrams/#launch_bringupdrawio","text":"","title":"launch_bringup.drawio"},{"location":"resources/diagrams/#simlan_diagramdrawio","text":"","title":"SIMLAN_DIAGRAM.drawio"},{"location":"resources/plots/Experiments/","text":"RTF tests for paper The following tests were performed on the PC Research2 with the following specs: Intel(R) Core(TM) i7-10700K CPU @ 3.80GHz NVIDIA GeForce RTX 2070 SUPER Corsair Vengeance LPX 32GB kit (2 \u00d7 16GB), 3000 MHz, CL16. running at 2133 MT/s MSI MEG Z490 UNIFY (MS-7C71) KINGSTON SA2000M8 1TB NVMe SSD Ubuntu 24.04.3 LTS every instance in the tables below are referenced in % with camera feed update rates as image 20 Hz depth 6 Hz semantic 10 Hz number_of_agents=0 world=empty: number of cameras image image depth semantic n=1 99 44 n=2 56 22 n=3 46 14 n=4 34 9 n=5 32 9 n=6 23 7 n=7 21 5 n=8 19 4 n=9 17 5 n=10 15 3 n=11 8 3 n=12 7 2 camera_feed=image, worth noting the default world took ~30s to render in gazebo GUI no robots: world 0 cameras 6 cameras 12 cameras empty 99 24 7 light 99 22 7 medium 85 17 7 default 84 17 7 world=empty camera_feed=image: robots 0 cameras 6 cameras 12 cameras robot_agentx1 99 11 5 robot_agentx2 99 11 4 robot_agentx3 99 9 4 robot_agentx4 99 9 4 humanoidx1 99 - - humanoidx2 99 - - panda_armx1 99 - - robot_agentx4+humanoidx1 99 9 4 robot_agentx4+humanoidx2 99 8 4 robot_agentx4+humanoidx2+panda_armx1 99 9 3 navigation world=light camera_feed=image: robots 0 cams 9 cams humanoidsx2 99 13 humanoidsx4 90 10 robot_agentsx4 - 5","title":"RTF tests for paper"},{"location":"resources/plots/Experiments/#rtf-tests-for-paper","text":"The following tests were performed on the PC Research2 with the following specs: Intel(R) Core(TM) i7-10700K CPU @ 3.80GHz NVIDIA GeForce RTX 2070 SUPER Corsair Vengeance LPX 32GB kit (2 \u00d7 16GB), 3000 MHz, CL16. running at 2133 MT/s MSI MEG Z490 UNIFY (MS-7C71) KINGSTON SA2000M8 1TB NVMe SSD Ubuntu 24.04.3 LTS every instance in the tables below are referenced in % with camera feed update rates as image 20 Hz depth 6 Hz semantic 10 Hz number_of_agents=0 world=empty: number of cameras image image depth semantic n=1 99 44 n=2 56 22 n=3 46 14 n=4 34 9 n=5 32 9 n=6 23 7 n=7 21 5 n=8 19 4 n=9 17 5 n=10 15 3 n=11 8 3 n=12 7 2 camera_feed=image, worth noting the default world took ~30s to render in gazebo GUI no robots: world 0 cameras 6 cameras 12 cameras empty 99 24 7 light 99 22 7 medium 85 17 7 default 84 17 7 world=empty camera_feed=image: robots 0 cameras 6 cameras 12 cameras robot_agentx1 99 11 5 robot_agentx2 99 11 4 robot_agentx3 99 9 4 robot_agentx4 99 9 4 humanoidx1 99 - - humanoidx2 99 - - panda_armx1 99 - - robot_agentx4+humanoidx1 99 9 4 robot_agentx4+humanoidx2 99 8 4 robot_agentx4+humanoidx2+panda_armx1 99 9 3 navigation world=light camera_feed=image: robots 0 cams 9 cams humanoidsx2 99 13 humanoidsx4 90 10 robot_agentsx4 - 5","title":"RTF tests for paper"},{"location":"simulation/","text":"Simulation World Fidelity It is possible to adjust the level of fidelity for a world in config.sh ; there the world_setup is sent to simulation/simlan_bringup/launch/sim.launch.py . simulation/simlan_gazebo_environment/launch/simlan_factory.launch.py (for world generation) simulation/simlan_gazebo_environment/launch/generate_world_file.py (both original_world and the world_setup ) Real time factor inside of config.sh you can can modify the scalar variable real_time_factor (RTF) to control the speed of the simulator. Reasonable ranges are 0 \\< RTF \\<= 1.0. inside the world config file \" simulation/simlan_gazebo_environment/worlds/ign_simlan_factory.world.xacro \" you can specify the real time factor that sets how slow time passes in the simulator. Per default the value is 1.0 meaning that 1:1 ratio. 1 second in sim correspond to 1 second in real-time. For more computational tasks it is preferred to have a lower RTF. Example, setting real_time_factor=0.1, means 10 seconds pass in real-time for ever simulated second. Adding Aruco and camera (static agents) Aruco codes and cameras are all attached to the same link in Gazebo. To create new static agents, go to simulation/static_agent_launcher/description/agents.urdf.xacro . There you only need to add a new line on the form <xacro:camera number=\"1\" x=\"3\" y=\"3\" z=\"6\" r=\"0\" p=\"0\" w=\"0\"/> for a new camera, or a new line on the form <xacro:aruco number=\"1\" x=\"3\" y=\"3\" z=\"0.1\" r=\"0\" p=\"0\" w=\"0\"/> for a new aruco. The commands are identical apart from the name. Number is added to the name to make the static agent unique. For cameras this means they will publish on, e.g., the topic static_agents/camera_[number]/image_raw . For aruco the number indicates which aruco png to use. x, y, z is the coordinate offset from the link the agents are all attached to. r, p, w are the rotation, pitch, and yaw of the camera, dictating in which direction and at what angle the cameras are looking. Gazebo supports simulation of camera based on Brown's distortion model. It expects 5 distortion coefficients k1, k2, k3, p1, p2 that you can get from the camera calibration tools. The k coefficients are the radial components of the distortion model, while the p coefficients are the tangential components. <aspect_ratio> : The ratio of the width and height of the camera. <horizontal_fov> : The horizontal field of view of the camera in radians. OpenCV uses five parameters, known as distortion coefficients given by like this: k1, k2, p1, p2 , k3 # pay attention to the order . Using actors in Gazebo Placed in sdf or world file Actors use the actor tag (as opposed to the model tag most other objects use). actor tags contain links like usual, but also a <script> tag. The script tag contains: loop: Whether the script should loop on completion delay_start: Time to wait before running the script, also waits between loops auto_start: Whether the script should start automatically when the sim starts A trajectory tag, which has an (unique) id and a type (used to couple it with an animation) Inside the trajectory tags we define waypoint tags which consist of a pose and the time where we are supposed to reach the pose. Note: The trajectory is smoothed as a whole. This means that you'll get a fluid motion, but the exact poses contained in the waypoints might not be reached. Script structure: <script> <loop>1</loop> <auto_start>1</auto_start> <trajectory id='0' type='square'> <waypoint> <time>0</time> <pose>-1 -1 1 0 -0 0</pose> </waypoint> <waypoint> <time>1</time> <pose>-1 1 1 0 -0 0</pose> </waypoint> <waypoint> <time>2</time> <pose>1 1 1 0 -0 0</pose> </waypoint> <waypoint> <time>3</time> <pose>1 -1 1 0 -0 0</pose> </waypoint> <waypoint> <time>4</time> <pose>-1 -1 1 0 -0 0</pose> </waypoint> </trajectory> </script> Gazebo supports two different skeleton animation file formats: COLLADA (.dae) and Biovision Hierarchy (.bvh). .bvh: Text-based format with section 'HIERARCHY' and section 'MOTION'. .dae: XML-based format, structured into multiple sections. .bvh works for Ignition, while .dae works for both Gazebo classic and Ignition Skin is similar, simply import a collada file. \\ true\\ makes the animation match the movement. The animation is done briefly along the x-axis and then it can be interpolated into any direction by gazebo. In a top down positive x,y frame of reference: 0 = right, 1.57 = up, -1.57 = down, 3.14 = left Pos/negative matters! 0 to -1.57 does not behave the same as 0 to 4.71! In other words increasing the angle of rotation means rotating left, and decreasing it means rotating right. Tension = how strictly it follows the waypoints in a span 0-1 where 0 is not very strict, 1 very strict Simulation Specification Environment (world) An in-door factory warehouse An out door street/sidewalk (side walk or street) Objects human actor shelves standard euro pallets pallet racks traffic cone Boxes differential-drive AMR (Autonomous Mobile Robot) forklift Textures Aruco tag on actor and floor objects texture Floor texture Placement of sensors camera (depth) on ceiling 2D Lidar/camera on AMR Physics mass moment of inertia collision friction damping Lighting Agents movements: deterministically following waypoints trajectories (in curve or in form of A-B-C...Z) AMR differential-drive using nav2 with obstacle avoidance replaying scenario using rosbag Additionally, these are expected from the simulator: following realistic and standard measurements and sizes for warehouse, pallettes, shelves, cart, human body, etc agent/robot status (position) and control via ROS exposing and recording camera images and agent status (e.g. positions) in Python tele-operation with keyboard and programmatically from python AMR differential-drive (caster wheel)/Ackermann (car) type of robot /cmd_vel topic accepts Twist formatted messages; only non-zero value for linear.x (forward/backward) and non-zero value for angular.z (differential drive, rotating around the z axis). Warehouse The warehouse was created in FreeCAD's BIM Workbench. This workbench isn't available by default, but can easily be acquired by going to Tools -> Addon Manager -> BIM -> Install/update Selected . Using the measurements found in the blueprint in the documentation folder, I drew four lines and turned them into walls (line tool and wall tool respectively). By default the wall will be created around the line, so that the line is in the middle of the wall. This can be changed in the wall properties so that it ends up entirely on one side of the line. As the blueprint lacked walls I used the dimensions specified there as the inner measurements, meaning that the origin point is located at the inner bottom left corner of the wall. All the inner space is in positive x and y coordinates, while the two of the walls are in negative coordinate space. I used the Aligned Dimension tool from the Annotation tools to confirm that the inner measurements matched those of the blueprint. While the BIM Workbench has support for door objects, they tend to act as solids when imported into Gazebo, so the door is just a hole in the wall. This hole was created by adding a cube object and having it intersect the wall where the door should be located. Then you select the cube and the wall (in that order) in the tree view and press Remove Component . This should remove the cube object and create a hole where the cube and wall overlapped. The hole didn't appear in the right place, but it was possible to edit the position of the hole in its properties. I measured the location of the hole using Aligned Dimension to make sure it ended up in the right spot. Going to the top view, I created a new rectangle object covering the whole warehouse. Then I turned it into a slab with the slab tool to create a floor for the warehouse. Everything was added to a level object (note: by default this level object is named \"Floor\", but don't confuse it with the slab that constitutes the physical floor) so that it's grouped together. If we set the wall heights to 0, they will automatically inherit the height of the Level object, so we can change all the walls easily by changing the height of the level. I added two materials from presets, concrete and wood. I applied the concrete material to all walls and the wood material to the floor. These should be considered to be temporary placeholders. Finally, I exported the project as a Collada file (.dae) and added it to a simple .world file to see if it loaded properly in Gazebo, and the results seemed correct. Since the warehouse will be static, we shouldn't need to define any additional parameters like mass or inertia; visuals and collision should be enough. Textures might need some improvement, as currently they're just basic colors, but it should be possible to add those in FreeCAD and have them included in the .dae file so we can load them visually in Gazebo later. I also added windows for slightly better visibility. Add warehouse to world-file The floor of the warehouse goes below z=0, so the ground plane was lowered by 0.2 so that the warehouse still rests on top of it. As our simulations will mainly take place inside the warehouse, the warehouse floor replaces the ground plane at z=0. The warehouse itself was placed in the models directory and loaded into the world with an <include> tag. Additionally, the maze in the stage4 world was moved away from the origin so that it is fully contained inside the warehouse, though in the future it should be removed altogether. Textures Shelf, pallet, warehouse walls are using CC0 textures from https://polyhaven.com/textures Box and warehouse floor are using images from Volvo as a base. box: picture from Volvo-group-packaging-specifications_2015.pdf Conventions Keep your links/joints paired, and use the suffix _link and _joint (e.g. arm_link and arm_joint) and maybe follow REP 120 naming conventions . Define visual, collision, inertial and Gazebo material (and maybe friction) for all objects cartography, localization, navigation ros2 launch pallet_truck_navigation cartography.launch.py ros2 run nav2_map_server map_saver_cli -f simulation/mapname","title":"Simulation"},{"location":"simulation/#simulation","text":"","title":"Simulation"},{"location":"simulation/#world-fidelity","text":"It is possible to adjust the level of fidelity for a world in config.sh ; there the world_setup is sent to simulation/simlan_bringup/launch/sim.launch.py . simulation/simlan_gazebo_environment/launch/simlan_factory.launch.py (for world generation) simulation/simlan_gazebo_environment/launch/generate_world_file.py (both original_world and the world_setup )","title":"World Fidelity"},{"location":"simulation/#real-time-factor","text":"inside of config.sh you can can modify the scalar variable real_time_factor (RTF) to control the speed of the simulator. Reasonable ranges are 0 \\< RTF \\<= 1.0. inside the world config file \" simulation/simlan_gazebo_environment/worlds/ign_simlan_factory.world.xacro \" you can specify the real time factor that sets how slow time passes in the simulator. Per default the value is 1.0 meaning that 1:1 ratio. 1 second in sim correspond to 1 second in real-time. For more computational tasks it is preferred to have a lower RTF. Example, setting real_time_factor=0.1, means 10 seconds pass in real-time for ever simulated second.","title":"Real time factor"},{"location":"simulation/#adding-aruco-and-camera-static-agents","text":"Aruco codes and cameras are all attached to the same link in Gazebo. To create new static agents, go to simulation/static_agent_launcher/description/agents.urdf.xacro . There you only need to add a new line on the form <xacro:camera number=\"1\" x=\"3\" y=\"3\" z=\"6\" r=\"0\" p=\"0\" w=\"0\"/> for a new camera, or a new line on the form <xacro:aruco number=\"1\" x=\"3\" y=\"3\" z=\"0.1\" r=\"0\" p=\"0\" w=\"0\"/> for a new aruco. The commands are identical apart from the name. Number is added to the name to make the static agent unique. For cameras this means they will publish on, e.g., the topic static_agents/camera_[number]/image_raw . For aruco the number indicates which aruco png to use. x, y, z is the coordinate offset from the link the agents are all attached to. r, p, w are the rotation, pitch, and yaw of the camera, dictating in which direction and at what angle the cameras are looking. Gazebo supports simulation of camera based on Brown's distortion model. It expects 5 distortion coefficients k1, k2, k3, p1, p2 that you can get from the camera calibration tools. The k coefficients are the radial components of the distortion model, while the p coefficients are the tangential components. <aspect_ratio> : The ratio of the width and height of the camera. <horizontal_fov> : The horizontal field of view of the camera in radians. OpenCV uses five parameters, known as distortion coefficients given by like this: k1, k2, p1, p2 , k3 # pay attention to the order .","title":"Adding Aruco and camera (static agents)"},{"location":"simulation/#using-actors-in-gazebo","text":"Placed in sdf or world file Actors use the actor tag (as opposed to the model tag most other objects use). actor tags contain links like usual, but also a <script> tag. The script tag contains: loop: Whether the script should loop on completion delay_start: Time to wait before running the script, also waits between loops auto_start: Whether the script should start automatically when the sim starts A trajectory tag, which has an (unique) id and a type (used to couple it with an animation) Inside the trajectory tags we define waypoint tags which consist of a pose and the time where we are supposed to reach the pose. Note: The trajectory is smoothed as a whole. This means that you'll get a fluid motion, but the exact poses contained in the waypoints might not be reached. Script structure: <script> <loop>1</loop> <auto_start>1</auto_start> <trajectory id='0' type='square'> <waypoint> <time>0</time> <pose>-1 -1 1 0 -0 0</pose> </waypoint> <waypoint> <time>1</time> <pose>-1 1 1 0 -0 0</pose> </waypoint> <waypoint> <time>2</time> <pose>1 1 1 0 -0 0</pose> </waypoint> <waypoint> <time>3</time> <pose>1 -1 1 0 -0 0</pose> </waypoint> <waypoint> <time>4</time> <pose>-1 -1 1 0 -0 0</pose> </waypoint> </trajectory> </script> Gazebo supports two different skeleton animation file formats: COLLADA (.dae) and Biovision Hierarchy (.bvh). .bvh: Text-based format with section 'HIERARCHY' and section 'MOTION'. .dae: XML-based format, structured into multiple sections. .bvh works for Ignition, while .dae works for both Gazebo classic and Ignition Skin is similar, simply import a collada file. \\ true\\ makes the animation match the movement. The animation is done briefly along the x-axis and then it can be interpolated into any direction by gazebo. In a top down positive x,y frame of reference: 0 = right, 1.57 = up, -1.57 = down, 3.14 = left Pos/negative matters! 0 to -1.57 does not behave the same as 0 to 4.71! In other words increasing the angle of rotation means rotating left, and decreasing it means rotating right. Tension = how strictly it follows the waypoints in a span 0-1 where 0 is not very strict, 1 very strict","title":"Using actors in Gazebo"},{"location":"simulation/#simulation-specification","text":"Environment (world) An in-door factory warehouse An out door street/sidewalk (side walk or street) Objects human actor shelves standard euro pallets pallet racks traffic cone Boxes differential-drive AMR (Autonomous Mobile Robot) forklift Textures Aruco tag on actor and floor objects texture Floor texture Placement of sensors camera (depth) on ceiling 2D Lidar/camera on AMR Physics mass moment of inertia collision friction damping Lighting Agents movements: deterministically following waypoints trajectories (in curve or in form of A-B-C...Z) AMR differential-drive using nav2 with obstacle avoidance replaying scenario using rosbag Additionally, these are expected from the simulator: following realistic and standard measurements and sizes for warehouse, pallettes, shelves, cart, human body, etc agent/robot status (position) and control via ROS exposing and recording camera images and agent status (e.g. positions) in Python tele-operation with keyboard and programmatically from python","title":"Simulation Specification"},{"location":"simulation/#amr","text":"differential-drive (caster wheel)/Ackermann (car) type of robot /cmd_vel topic accepts Twist formatted messages; only non-zero value for linear.x (forward/backward) and non-zero value for angular.z (differential drive, rotating around the z axis).","title":"AMR"},{"location":"simulation/#warehouse","text":"The warehouse was created in FreeCAD's BIM Workbench. This workbench isn't available by default, but can easily be acquired by going to Tools -> Addon Manager -> BIM -> Install/update Selected . Using the measurements found in the blueprint in the documentation folder, I drew four lines and turned them into walls (line tool and wall tool respectively). By default the wall will be created around the line, so that the line is in the middle of the wall. This can be changed in the wall properties so that it ends up entirely on one side of the line. As the blueprint lacked walls I used the dimensions specified there as the inner measurements, meaning that the origin point is located at the inner bottom left corner of the wall. All the inner space is in positive x and y coordinates, while the two of the walls are in negative coordinate space. I used the Aligned Dimension tool from the Annotation tools to confirm that the inner measurements matched those of the blueprint. While the BIM Workbench has support for door objects, they tend to act as solids when imported into Gazebo, so the door is just a hole in the wall. This hole was created by adding a cube object and having it intersect the wall where the door should be located. Then you select the cube and the wall (in that order) in the tree view and press Remove Component . This should remove the cube object and create a hole where the cube and wall overlapped. The hole didn't appear in the right place, but it was possible to edit the position of the hole in its properties. I measured the location of the hole using Aligned Dimension to make sure it ended up in the right spot. Going to the top view, I created a new rectangle object covering the whole warehouse. Then I turned it into a slab with the slab tool to create a floor for the warehouse. Everything was added to a level object (note: by default this level object is named \"Floor\", but don't confuse it with the slab that constitutes the physical floor) so that it's grouped together. If we set the wall heights to 0, they will automatically inherit the height of the Level object, so we can change all the walls easily by changing the height of the level. I added two materials from presets, concrete and wood. I applied the concrete material to all walls and the wood material to the floor. These should be considered to be temporary placeholders. Finally, I exported the project as a Collada file (.dae) and added it to a simple .world file to see if it loaded properly in Gazebo, and the results seemed correct. Since the warehouse will be static, we shouldn't need to define any additional parameters like mass or inertia; visuals and collision should be enough. Textures might need some improvement, as currently they're just basic colors, but it should be possible to add those in FreeCAD and have them included in the .dae file so we can load them visually in Gazebo later. I also added windows for slightly better visibility.","title":"Warehouse"},{"location":"simulation/#add-warehouse-to-world-file","text":"The floor of the warehouse goes below z=0, so the ground plane was lowered by 0.2 so that the warehouse still rests on top of it. As our simulations will mainly take place inside the warehouse, the warehouse floor replaces the ground plane at z=0. The warehouse itself was placed in the models directory and loaded into the world with an <include> tag. Additionally, the maze in the stage4 world was moved away from the origin so that it is fully contained inside the warehouse, though in the future it should be removed altogether.","title":"Add warehouse to world-file"},{"location":"simulation/#textures","text":"Shelf, pallet, warehouse walls are using CC0 textures from https://polyhaven.com/textures Box and warehouse floor are using images from Volvo as a base. box: picture from Volvo-group-packaging-specifications_2015.pdf","title":"Textures"},{"location":"simulation/#conventions","text":"Keep your links/joints paired, and use the suffix _link and _joint (e.g. arm_link and arm_joint) and maybe follow REP 120 naming conventions . Define visual, collision, inertial and Gazebo material (and maybe friction) for all objects","title":"Conventions"},{"location":"simulation/#cartography-localization-navigation","text":"ros2 launch pallet_truck_navigation cartography.launch.py ros2 run nav2_map_server map_saver_cli -f simulation/mapname","title":"cartography, localization, navigation"},{"location":"simulation/aruco_localization/","text":"Aruco Localization Package This is the aruco_localization package, which runs a ROS2 node that takes images from camera topics CAMERA_X/camera_info and CAMERA_X/image_raw , processes the ArUco code, and publishes the result in the /TF of the detected ArUco marker, mimicking Volvo's logic for tracking robot positions and generating navigation routes. A launch file is provided to run multiple nodes, with each node dedicated and assigned to a single camera. The camera_enabled_ids variable found in config.sh is used to control which cameras are enabled. Note: The marker link is named based on the camera. For example, if camera 164 detects an ArUco marker with ID of 12 , the marker link in /tf is named camera_164_marker_12 . This is not the final link that is used. The result from different cameras are accumulated and merged as a single frame with parent as robot_agent_12/odom and child robot_agent_12/baselink . Note: The marker ID is tightly linked with the name for the robots, meaning an ArUco with marker_ID=1 will detect the robot with name: {namespace}/{marker_ID}. For example 'robot_agent_1'. Important points to pay attention to: If two cameras are pointed at the same ArUco code, the system does not alternate between them, but instead shows the midpoint between the two detections. Inside the aruco_node, a callback logs the relative position of the newly created aruco_link from the base_link . Multiple cameras can be linked to the same ArUco code simultaneously. Important launch files aruco_detection_node.py : Handles ArUco detection and publishes the TF between the camera and marker. (e.g. camera_164_marker_12 ) aruco_pose_pub.py : A new node that listens to all transforms of camera_164_marker_12 as input, and outputs the transform to either TF or ODOM as robot_agent_12/odom and child robot_agent_12/baselink . multi_detection.launch.py : Launch file for the new node. Odom These have to be valid for both humanoid and pallet trucks world: is the origin (0,0,0) odom: is in the initial pose of the robot base_link: tracks the movement of the robot relative to the odom","title":"Aruco Localization"},{"location":"simulation/aruco_localization/#aruco-localization-package","text":"This is the aruco_localization package, which runs a ROS2 node that takes images from camera topics CAMERA_X/camera_info and CAMERA_X/image_raw , processes the ArUco code, and publishes the result in the /TF of the detected ArUco marker, mimicking Volvo's logic for tracking robot positions and generating navigation routes. A launch file is provided to run multiple nodes, with each node dedicated and assigned to a single camera. The camera_enabled_ids variable found in config.sh is used to control which cameras are enabled. Note: The marker link is named based on the camera. For example, if camera 164 detects an ArUco marker with ID of 12 , the marker link in /tf is named camera_164_marker_12 . This is not the final link that is used. The result from different cameras are accumulated and merged as a single frame with parent as robot_agent_12/odom and child robot_agent_12/baselink . Note: The marker ID is tightly linked with the name for the robots, meaning an ArUco with marker_ID=1 will detect the robot with name: {namespace}/{marker_ID}. For example 'robot_agent_1'.","title":"Aruco Localization Package"},{"location":"simulation/aruco_localization/#important-points-to-pay-attention-to","text":"If two cameras are pointed at the same ArUco code, the system does not alternate between them, but instead shows the midpoint between the two detections. Inside the aruco_node, a callback logs the relative position of the newly created aruco_link from the base_link . Multiple cameras can be linked to the same ArUco code simultaneously.","title":"Important points to pay attention to:"},{"location":"simulation/aruco_localization/#important-launch-files","text":"aruco_detection_node.py : Handles ArUco detection and publishes the TF between the camera and marker. (e.g. camera_164_marker_12 ) aruco_pose_pub.py : A new node that listens to all transforms of camera_164_marker_12 as input, and outputs the transform to either TF or ODOM as robot_agent_12/odom and child robot_agent_12/baselink . multi_detection.launch.py : Launch file for the new node.","title":"Important launch files"},{"location":"simulation/aruco_localization/#odom","text":"These have to be valid for both humanoid and pallet trucks world: is the origin (0,0,0) odom: is in the initial pose of the robot base_link: tracks the movement of the robot relative to the odom","title":"Odom"},{"location":"simulation/bt_failsafe/","text":"Failsafe for the navigation The SIMLAN systems support a failsafe and geo-fencing mechanism for the pallet trucks' navigation. The main idea is that the pallet trucks should stop the navigation as soon as a dangerous situation below is detected: In this approach for a new Behavior Tree, the condition node named StopRobotCondition is defined and can be triggered when a safety issue occurs. We then added a Behavior Tree plugin that calls an action called CancelControl when the given condition is triggered. Currently, this safety controller triggers StopRobotCondition : Activation of collision sensor When a collision is detected by the simulator, the collision's physical properties (such as the force and the object that the pallet truck collided with) are published to the pallet trucks' /contact topic. Loss of Observability To implement geofencing and the safety situation in which a pallet truck is not observable in any camera, the aruco_localization pkg under aruco_localization/aruco_pose_pub.py continuously publishes the list of pallet trucks that are observable in the /aruco_marker_seen topic. Then, if one stops being observable, the BT condition passes. Behavior tree and direct implementation in aruco_localization pkg At first, we define a custom behavior_tree.xml in simulation/pallet_truck/pallet_truck_navigation/config/navigate_w_replanning_and_recovery_robot_agent_X.xml . This XML file defines which BT plugins we want to use during the navigation and which run continuously during the navigation. In this XML file, the StopRobotCondition and CancelControl plugins are defined in a fallback function. A fallback function works as it runs the first stated plugin, and when that plugin fails, it moves over to the second one and executes that plugin. So in this case, we first run the StopRobotCondition plugin until the robot is out of bounds. Once the plugin fails, the CancelControl plugin executes and stops the pallet truck. In order to know whether a pallet truck is out of bounds or not, it is determined by looking at the aruco marker on the pallet truck and deciding if it's seen by any of the cameras. This is implemented in the aruco_localization pkg under aruco_localization/aruco_pose_pub.py . As long as the aruco marker on the pallet truck is seen by any of the cameras, the node publishes the robot namespace in a list to the topic /aruco_marker_seen . The same goes for the collision sensor. It publishes to the topic /robot_agent_X/contact when the pallet trucks collide with something. Step two in the failsafe is to stop the pallet truck when it gets out of bounds or collides with something. This is done by a custom-made behavior tree plugin called StopRobotCondition, which is found in simulation/bt_failsafe/src/stop_robot.cpp . This plugin listens to the /aruco_marker_lost and /robot_agent_X/contact topics. As soon as a contact is detected or a robot's namespace is lost, it fails, and the CancelControl behavior tree starts. This, in turn, stops the pallet truck. The CancelControl plugin is an existing built-in plugin in Nav2. Why Nav2 behavior tree plugins were not suitable We tried to use only built-in plugins, which most likely is the most robust and secure way to do it as the plugins are updated accordingly to ROS2 and Nav2. We tried to use several plugins to detect if the pallet trucks are out of bounds, but none of them really suited this purpose of this project. The first one we tried to use is the TransformAvailable plugin . This plugin checks if a certain TF exists. If the TF is missing, the plugin returns FAILURE . So in this case, we thought we could look at the TFs between the cameras and the aruco marker, and if none of them exists, it should fail and stop the pallet truck. Unfortunately, we couldn't use this plugin because it only looks at the TF from the very beginning of the navigation. And if it exists from the beginning, it will always succeed and will never return FAILURE . Therefore, it cannot be used in our case because we have TF from the beginning, and our TF disappears after some time during the navigation. You could probably modify the plugin to work for our case as well, but if modifications are needed, it could be implemented in a better way instead. The second plugin we tried is the IsStuckCondition . This plugin checks if the robot is stuck by calculating the deceleration of the robot. If the deceleration is too big, it returns FAILURE . This wasn't anything we could use because the acceleration is set to zero as soon as the robot gets out of bounds. So this is probably not suitable for this case.","title":"Failsafe"},{"location":"simulation/bt_failsafe/#failsafe-for-the-navigation","text":"The SIMLAN systems support a failsafe and geo-fencing mechanism for the pallet trucks' navigation. The main idea is that the pallet trucks should stop the navigation as soon as a dangerous situation below is detected: In this approach for a new Behavior Tree, the condition node named StopRobotCondition is defined and can be triggered when a safety issue occurs. We then added a Behavior Tree plugin that calls an action called CancelControl when the given condition is triggered. Currently, this safety controller triggers StopRobotCondition :","title":"Failsafe for the navigation"},{"location":"simulation/bt_failsafe/#activation-of-collision-sensor","text":"When a collision is detected by the simulator, the collision's physical properties (such as the force and the object that the pallet truck collided with) are published to the pallet trucks' /contact topic.","title":"Activation of collision sensor"},{"location":"simulation/bt_failsafe/#loss-of-observability","text":"To implement geofencing and the safety situation in which a pallet truck is not observable in any camera, the aruco_localization pkg under aruco_localization/aruco_pose_pub.py continuously publishes the list of pallet trucks that are observable in the /aruco_marker_seen topic. Then, if one stops being observable, the BT condition passes.","title":"Loss of Observability"},{"location":"simulation/bt_failsafe/#behavior-tree-and-direct-implementation-in-aruco_localization-pkg","text":"At first, we define a custom behavior_tree.xml in simulation/pallet_truck/pallet_truck_navigation/config/navigate_w_replanning_and_recovery_robot_agent_X.xml . This XML file defines which BT plugins we want to use during the navigation and which run continuously during the navigation. In this XML file, the StopRobotCondition and CancelControl plugins are defined in a fallback function. A fallback function works as it runs the first stated plugin, and when that plugin fails, it moves over to the second one and executes that plugin. So in this case, we first run the StopRobotCondition plugin until the robot is out of bounds. Once the plugin fails, the CancelControl plugin executes and stops the pallet truck. In order to know whether a pallet truck is out of bounds or not, it is determined by looking at the aruco marker on the pallet truck and deciding if it's seen by any of the cameras. This is implemented in the aruco_localization pkg under aruco_localization/aruco_pose_pub.py . As long as the aruco marker on the pallet truck is seen by any of the cameras, the node publishes the robot namespace in a list to the topic /aruco_marker_seen . The same goes for the collision sensor. It publishes to the topic /robot_agent_X/contact when the pallet trucks collide with something. Step two in the failsafe is to stop the pallet truck when it gets out of bounds or collides with something. This is done by a custom-made behavior tree plugin called StopRobotCondition, which is found in simulation/bt_failsafe/src/stop_robot.cpp . This plugin listens to the /aruco_marker_lost and /robot_agent_X/contact topics. As soon as a contact is detected or a robot's namespace is lost, it fails, and the CancelControl behavior tree starts. This, in turn, stops the pallet truck. The CancelControl plugin is an existing built-in plugin in Nav2.","title":"Behavior tree and direct implementation in aruco_localization pkg"},{"location":"simulation/bt_failsafe/#why-nav2-behavior-tree-plugins-were-not-suitable","text":"We tried to use only built-in plugins, which most likely is the most robust and secure way to do it as the plugins are updated accordingly to ROS2 and Nav2. We tried to use several plugins to detect if the pallet trucks are out of bounds, but none of them really suited this purpose of this project. The first one we tried to use is the TransformAvailable plugin . This plugin checks if a certain TF exists. If the TF is missing, the plugin returns FAILURE . So in this case, we thought we could look at the TFs between the cameras and the aruco marker, and if none of them exists, it should fail and stop the pallet truck. Unfortunately, we couldn't use this plugin because it only looks at the TF from the very beginning of the navigation. And if it exists from the beginning, it will always succeed and will never return FAILURE . Therefore, it cannot be used in our case because we have TF from the beginning, and our TF disappears after some time during the navigation. You could probably modify the plugin to work for our case as well, but if modifications are needed, it could be implemented in a better way instead. The second plugin we tried is the IsStuckCondition . This plugin checks if the robot is stuck by calculating the deceleration of the robot. If the deceleration is too big, it returns FAILURE . This wasn't anything we could use because the acceleration is set to zero as soon as the robot gets out of bounds. So this is probably not suitable for this case.","title":"Why Nav2 behavior tree plugins were not suitable"},{"location":"simulation/humanoid_camera_system/","text":"Humanoid Camera System This is a package that views the output of the cameras for the humanoid project. The pkg is a legacy component from when the humanoid was its separate project, and it is probably wise to merge its content with another pkg. Setup and usage of the camera viewer. The commands below is examples of how you can use the camera_viewer.py . It is used to save images of humanoids when generating humanoid datasets. The arguments needed to run: output_dir : Folder where images and output is stored. camera_ids : What cameras to take into account. It uses \"dataset_cameras\" found in config.sh . Modify this variable to set what cameras camera_viewer.py should sample from. ros2 run humanoid_camera_system camera_viewer.py --ros-args -p output_dir:=$1 -p camera_ids:=\"${dataset_cameras}\"","title":"Humanoid Camera System"},{"location":"simulation/humanoid_camera_system/#humanoid-camera-system","text":"This is a package that views the output of the cameras for the humanoid project. The pkg is a legacy component from when the humanoid was its separate project, and it is probably wise to merge its content with another pkg.","title":"Humanoid Camera System"},{"location":"simulation/humanoid_camera_system/#setup-and-usage-of-the-camera-viewer","text":"The commands below is examples of how you can use the camera_viewer.py . It is used to save images of humanoids when generating humanoid datasets. The arguments needed to run: output_dir : Folder where images and output is stored. camera_ids : What cameras to take into account. It uses \"dataset_cameras\" found in config.sh . Modify this variable to set what cameras camera_viewer.py should sample from. ros2 run humanoid_camera_system camera_viewer.py --ros-args -p output_dir:=$1 -p camera_ids:=\"${dataset_cameras}\"","title":"Setup and usage of the camera viewer."},{"location":"simulation/humanoid_motion_planner/","text":"Motion planner The motion planner package has three features. it can execute a infinite series of random motions which is useful when you want to create synthetic data. you can send motions to execute directly in memory. you can pass a motion file to be read and executed When the package runs it subscribes to a topic: humanoid_X/execute_motion which inputs a stringified message of a dict (joint_name, value) or a filename. Requirement : Running this command require you to run these nodes first: replay_motion_namespace=\"humanoid_1\" ./control.sh sim # run simulation ./control.sh humanoid # spawn humanoid ./control.sh humanoid_moveit # allow humanoid to run motions file mode You can replay each motion data file separately: ./control.sh execute_motion simulation/humanoid_motion_planner/motions/default_motion.json If you don't have any motions available, there are ready ones inside the motions/ folder. string mode The easiest way to use it is in python using a dictionary, stringifying it with json, and then publishing it to the topic: motion_dict = {JOINT_DATA} json_str = json.dumps(motion_dict) cmd = [ \"ros2\", \"topic\", \"pub\", \"--once\", f\"{humanoid_namespace}/execute_motion\", \"std_msgs/msg/String\", f\"{{data: '{json_str}'}}\", ] subprocess.run(cmd, check=True) random mode To send random motion to the humanoid use the command below ros2 launch humanoid_motion_planner random_motion.launch.py run_random_generate:=true output_dir:=$1 \"log_level:=${log_level}\" or alternatively: ./control.sh random_motion","title":"Humanoid motion planner"},{"location":"simulation/humanoid_motion_planner/#motion-planner","text":"The motion planner package has three features. it can execute a infinite series of random motions which is useful when you want to create synthetic data. you can send motions to execute directly in memory. you can pass a motion file to be read and executed When the package runs it subscribes to a topic: humanoid_X/execute_motion which inputs a stringified message of a dict (joint_name, value) or a filename. Requirement : Running this command require you to run these nodes first: replay_motion_namespace=\"humanoid_1\" ./control.sh sim # run simulation ./control.sh humanoid # spawn humanoid ./control.sh humanoid_moveit # allow humanoid to run motions","title":"Motion planner"},{"location":"simulation/humanoid_motion_planner/#file-mode","text":"You can replay each motion data file separately: ./control.sh execute_motion simulation/humanoid_motion_planner/motions/default_motion.json If you don't have any motions available, there are ready ones inside the motions/ folder.","title":"file mode"},{"location":"simulation/humanoid_motion_planner/#string-mode","text":"The easiest way to use it is in python using a dictionary, stringifying it with json, and then publishing it to the topic: motion_dict = {JOINT_DATA} json_str = json.dumps(motion_dict) cmd = [ \"ros2\", \"topic\", \"pub\", \"--once\", f\"{humanoid_namespace}/execute_motion\", \"std_msgs/msg/String\", f\"{{data: '{json_str}'}}\", ] subprocess.run(cmd, check=True)","title":"string mode"},{"location":"simulation/humanoid_motion_planner/#random-mode","text":"To send random motion to the humanoid use the command below ros2 launch humanoid_motion_planner random_motion.launch.py run_random_generate:=true output_dir:=$1 \"log_level:=${log_level}\" or alternatively: ./control.sh random_motion","title":"random mode"},{"location":"simulation/humanoid_odom_pub/","text":"Humanoid_odom_pub When driving around with the humanoid, it was noticed that the TFs the ROS2 controller published did not cohere with the humanoid's position in Gazebo. Since the humanoid's purpose is only to simulate human behavior, it was decided to not use the odom frame from the ROS2 controllers but to get the odom frame from the Gazebo pose instead. Therefore, the humanoid_odom_pub was created to get that pose and use it as the odom frame. This is done by bridging the poses of the robot_agents and humanoids to the namespace/pose topic. Then a subscriber is created which listens to the given topic and publishes a transform between namespace/odom -> namespace/base_link depending on the pose of the robots. This works as the ground truth of the position. How to remove If it is decided to remove this pkg in the future, follow these steps: Go to simulation/humanoid_support_moveit_config/launch/launch_controllers.launch.py and delete the following: python odom_publisher = Node( package=\"humanoid_odom_pub\", executable=\"humanoid_odom_pub\", name=\"humanoid_odom_pub\", output=\"screen\", namespace=namespace, parameters=[ {\"namespace\": namespace}, {\"initial_pose_x\": initial_pose_x}, {\"initial_pose_y\": initial_pose_y}, {'use_sim_time': True} ], ) ... actions.append(odom_publisher) The humanoids still need a dynamic transform from humanoid_x/odom -> humanoid_x/base_link. Either implement the newly decided solution or revert the changes in the ros2_controller. To revert changes in the ros2_controller, go to config_generation/scripts/generate_humanoid_control_yaml.py and change: python \"enable_odom_tf\": False, to: python \"enable_odom_tf\": True, Please note if the pose and orientation in both Gazebo and RViz are the same after teleoping or doing navigation, to not break the navigation stack for the humanoids. Delete the simulation/humanoid_odom_pub pkg. Build the workspace.","title":"Humanoid_odom_pub"},{"location":"simulation/humanoid_odom_pub/#humanoid_odom_pub","text":"When driving around with the humanoid, it was noticed that the TFs the ROS2 controller published did not cohere with the humanoid's position in Gazebo. Since the humanoid's purpose is only to simulate human behavior, it was decided to not use the odom frame from the ROS2 controllers but to get the odom frame from the Gazebo pose instead. Therefore, the humanoid_odom_pub was created to get that pose and use it as the odom frame. This is done by bridging the poses of the robot_agents and humanoids to the namespace/pose topic. Then a subscriber is created which listens to the given topic and publishes a transform between namespace/odom -> namespace/base_link depending on the pose of the robots. This works as the ground truth of the position.","title":"Humanoid_odom_pub"},{"location":"simulation/humanoid_odom_pub/#how-to-remove","text":"If it is decided to remove this pkg in the future, follow these steps: Go to simulation/humanoid_support_moveit_config/launch/launch_controllers.launch.py and delete the following: python odom_publisher = Node( package=\"humanoid_odom_pub\", executable=\"humanoid_odom_pub\", name=\"humanoid_odom_pub\", output=\"screen\", namespace=namespace, parameters=[ {\"namespace\": namespace}, {\"initial_pose_x\": initial_pose_x}, {\"initial_pose_y\": initial_pose_y}, {'use_sim_time': True} ], ) ... actions.append(odom_publisher) The humanoids still need a dynamic transform from humanoid_x/odom -> humanoid_x/base_link. Either implement the newly decided solution or revert the changes in the ros2_controller. To revert changes in the ros2_controller, go to config_generation/scripts/generate_humanoid_control_yaml.py and change: python \"enable_odom_tf\": False, to: python \"enable_odom_tf\": True, Please note if the pose and orientation in both Gazebo and RViz are the same after teleoping or doing navigation, to not break the navigation stack for the humanoids. Delete the simulation/humanoid_odom_pub pkg. Build the workspace.","title":"How to remove"},{"location":"simulation/humanoid_robot/","text":"Humanoid Robot There needs to be better documentation of the difference between this pkg and the humanoid_support_moveit_config pkg. Is this supposed to be the bring-up of that pkg? Why is the humanoid model in this pkg and the URDF that spawns the robot in humanoid_support_moveit_config? Are the humanSubject01-08 used, or is only humanSubjectWithMeshes used? Clarification is needed. All in all, a restructure of the entire humanoid_robot and humanoid_support_moveit_config should be done. Configuring and spawning humanoids inside the sim Humanoids are configured in config.sh as a string in the HUMANOIDS variable. It is possible to add more humanoids and they are defined as: { \"namespace\": \"humanoid_1\", \"initial_pose_x\":5.5, \"initial_pose_y\":-10.0 }, model and URDF Package created based on this tutorial \\ Note that only the human with mesh model is installed in the package; see /model/CMakeLists.txt for how to install the non-mesh models. In the URDF, the fixed links and joints are (mostly) named [muscle name]_[location], for example, BicBrac_RUA = biceps brachii_right upper arm. In humanSubjectWithMesh_simplified.urdf, all fixed joints have been removed; only the skeleton joints remain. To open Rviz with the human model: ros2 launch urdf_tutorial display.launch.py model:=/home/ros/src/simulation/humanoid_robot/model/human-gazebo/humanSubjectWithMeshes/humanSubjectWithMesh_simplified.urdf TODO: Explain what are the parameters ( config.sh ). Add this to resources/build-documentation","title":"Humanoid Robot"},{"location":"simulation/humanoid_robot/#humanoid-robot","text":"There needs to be better documentation of the difference between this pkg and the humanoid_support_moveit_config pkg. Is this supposed to be the bring-up of that pkg? Why is the humanoid model in this pkg and the URDF that spawns the robot in humanoid_support_moveit_config? Are the humanSubject01-08 used, or is only humanSubjectWithMeshes used? Clarification is needed. All in all, a restructure of the entire humanoid_robot and humanoid_support_moveit_config should be done.","title":"Humanoid Robot"},{"location":"simulation/humanoid_robot/#configuring-and-spawning-humanoids-inside-the-sim","text":"Humanoids are configured in config.sh as a string in the HUMANOIDS variable. It is possible to add more humanoids and they are defined as: { \"namespace\": \"humanoid_1\", \"initial_pose_x\":5.5, \"initial_pose_y\":-10.0 },","title":"Configuring and spawning humanoids inside the sim"},{"location":"simulation/humanoid_robot/#model-and-urdf","text":"Package created based on this tutorial \\ Note that only the human with mesh model is installed in the package; see /model/CMakeLists.txt for how to install the non-mesh models. In the URDF, the fixed links and joints are (mostly) named [muscle name]_[location], for example, BicBrac_RUA = biceps brachii_right upper arm. In humanSubjectWithMesh_simplified.urdf, all fixed joints have been removed; only the skeleton joints remain. To open Rviz with the human model: ros2 launch urdf_tutorial display.launch.py model:=/home/ros/src/simulation/humanoid_robot/model/human-gazebo/humanSubjectWithMeshes/humanSubjectWithMesh_simplified.urdf TODO: Explain what are the parameters ( config.sh ). Add this to resources/build-documentation","title":"model and URDF"},{"location":"simulation/humanoid_robot/model/human-gazebo/","text":"human-gazebo This repository contains the human gazebo models that are used with Human Dynamics Estimation software suite. The files are generated using xsens motion capture data and mvnx-to-urdf . The human model links are made of several simple rigid bodies as shown in the figure below: The measurements of each of the human subject are available from the table. Please refer to the human subject data pdf file to know how these measurements are taken. The urdf models are generated from xsens mvnx file generated through xsens mvn studio software suite. The code to generate the model is available here Subject Mass [kg] Height [cm] Foot size [cm] Arm span [cm] Ankle height [cm] Hip height [cm] Hip width [cm] Knee height [cm] Shoulder width [cm] Shoulder height [cm] Sole height [cm] 1 62.2 168 24 163 8 91 25 48.5 35.4 140 - 2 79.4 176 26 169 8 94 33 48 40 140 - 3 75.4 180 27 190 8 102 28 58 43 148 - 4 72.7 182 26 197 8 102 29 56 42 150 - 5 55 168 24 168 8 98 25 52 38 139 - 6 71.2 179 29 180 8 100 31 49 43 147 - 7 78.9 178 28 192 8 102 30 52 44 148 - 8 55.2 166 25 170 8 90 28 45 37 139 - Currently, the legacy directory contains files related to joint motor control boards based on gazebo-yarp-plugins and other configuration files needed to control the human joints. Human subject with meshes In the folder humanSubjectWithMeshes there is a urdf model of a human subject generated using the code in human-model-generator with meshes under CC-BY_SA license (https://creativecommons.org/licenses/by-sa/2.0/deed.en); all the meshes were trimmed, morphed and totally or partially reconstructed to reach the desired shape and topology. The model is shown in the following figure: Mantainers Davide Gorbani ( @davidegorbani ) Carlotta Sartore ( @CarlottaSartore )","title":"human-gazebo"},{"location":"simulation/humanoid_robot/model/human-gazebo/#human-gazebo","text":"This repository contains the human gazebo models that are used with Human Dynamics Estimation software suite. The files are generated using xsens motion capture data and mvnx-to-urdf . The human model links are made of several simple rigid bodies as shown in the figure below: The measurements of each of the human subject are available from the table. Please refer to the human subject data pdf file to know how these measurements are taken. The urdf models are generated from xsens mvnx file generated through xsens mvn studio software suite. The code to generate the model is available here Subject Mass [kg] Height [cm] Foot size [cm] Arm span [cm] Ankle height [cm] Hip height [cm] Hip width [cm] Knee height [cm] Shoulder width [cm] Shoulder height [cm] Sole height [cm] 1 62.2 168 24 163 8 91 25 48.5 35.4 140 - 2 79.4 176 26 169 8 94 33 48 40 140 - 3 75.4 180 27 190 8 102 28 58 43 148 - 4 72.7 182 26 197 8 102 29 56 42 150 - 5 55 168 24 168 8 98 25 52 38 139 - 6 71.2 179 29 180 8 100 31 49 43 147 - 7 78.9 178 28 192 8 102 30 52 44 148 - 8 55.2 166 25 170 8 90 28 45 37 139 - Currently, the legacy directory contains files related to joint motor control boards based on gazebo-yarp-plugins and other configuration files needed to control the human joints.","title":"human-gazebo"},{"location":"simulation/humanoid_robot/model/human-gazebo/#human-subject-with-meshes","text":"In the folder humanSubjectWithMeshes there is a urdf model of a human subject generated using the code in human-model-generator with meshes under CC-BY_SA license (https://creativecommons.org/licenses/by-sa/2.0/deed.en); all the meshes were trimmed, morphed and totally or partially reconstructed to reach the desired shape and topology. The model is shown in the following figure:","title":"Human subject with meshes"},{"location":"simulation/humanoid_robot/model/human-gazebo/#mantainers","text":"Davide Gorbani ( @davidegorbani ) Carlotta Sartore ( @CarlottaSartore )","title":"Mantainers"},{"location":"simulation/humanoid_robot/model/human-gazebo/humanSubjectWithMeshes/","text":"The meshes in this folder have been derived from https://blendswap.com/blend/11604.","title":"Index"},{"location":"simulation/humanoid_robot/model/human-gazebo/legacy/control/","text":"This module is for the control of the human model in gazebo.","title":"Index"},{"location":"simulation/humanoid_support_moveit_config/","text":"Humanoid In this file, the basics of the humanoid structure will be described. In the future, all nodes will be namespaced to add the functionality of spawning and controlling multiple humanoids. Moveit For MoveIt to work, 3 components must have correct information: URDF link names SRDF joint definitions TF frames published by robot_state_publisher There was an issue when trying to add namespaces to the humanoid project. Since the humanoid is made of 40+ links, it was decided to add the \"frame_prefix\": f\"{namespace}\" in the robot_state_publisher node. This, however, made a conflict since the URDF, SRDF, and TF frames no longer matched. This was solved by adding base_link in between the world and namespace/base_link frames and creating a static transform between them. This means that all further humanoids will be spawned under the base_link frame. (otherwise, the warning below again appears) Dynamically updated URDFs To make the multiple_humanoid_spawn work, we need to be able to launch different namespaces in the robot_description. This is done by running: moveit_config = ( MoveItConfigsBuilder(\"human_support\", package_name=\"humanoid_support_moveit_config\") .robot_description( file_path=\"config/human_support.urdf.xacro\", mappings={\"namespace\": namespace} ) .to_moveit_configs() ) This updates the namespace argument inside the .xacro files. Planning scene monitor When working with the MoveIt package, some bugs or undesired features were found. When MoveIt is launched, the PlanningSceneMonitor and PlanningFrame subscribe to all TF frames that exist and assume they can transform any known object or sensor data into its planning frame, which is base_link in this case. If this cannot be done, it will throw a warning saying the following: [WARN] [humanoid.moveit.moveit.ros.planning_scene_monitor] [id]: Unable to transform object from frame 'unconnected_frame' to planning frame 'base_link' (Could not find a connection between 'base_link' and 'unconnected_frame' because they are not part of the same tree. TF has two or more unconnected trees) To limit this, a wait function was added: ld.add_action(TimerAction(period=5.0, actions=[rsp_node])) to make sure the transform is published before the rsp_node starts. Figures Figure 1: Humanoid frames visualization Rviz2 visualization If you want to visualize the movement in Rviz, you need to configure the config/moveit.rviz file and change all /humanoid_X instances to the namespace you want to visualize. Bugs: TF visualization and Gazebo simulation not matching When visualizing and comparing the actual TF data and simulated locations of the humanoids when doing navigation and teleop, it can be seen that these do not match. This is a major issue since that means the location of where the humanoids think they are in the map and the location of where the humanoids actually are in Gazebo will be different. This can be visualized by turning the humanoid 360 degrees in Gazebo with the ./control.sh teleop humanoid_1 command and comparing it to Rviz. There, the humanoid has turned closer to 300 degrees. This problem probably comes from some URDF descriptions not matching the actual geometry or specifications the humanoid has in /home/ros/src/simulation/humanoid_support_moveit_config/config/human_support_wheels.urdf.xacro . By tuning the mass , mu , and inertial values in the <xacro:macro name=\"wheel\" params=\"wheel_prefix *joint_pose\"> xacro tag, it was possible to tune the degree mismatch. For proper navigation and control, this needs to be fixed!","title":"Humanoid Moveit"},{"location":"simulation/humanoid_support_moveit_config/#humanoid","text":"In this file, the basics of the humanoid structure will be described. In the future, all nodes will be namespaced to add the functionality of spawning and controlling multiple humanoids.","title":"Humanoid"},{"location":"simulation/humanoid_support_moveit_config/#moveit","text":"For MoveIt to work, 3 components must have correct information: URDF link names SRDF joint definitions TF frames published by robot_state_publisher There was an issue when trying to add namespaces to the humanoid project. Since the humanoid is made of 40+ links, it was decided to add the \"frame_prefix\": f\"{namespace}\" in the robot_state_publisher node. This, however, made a conflict since the URDF, SRDF, and TF frames no longer matched. This was solved by adding base_link in between the world and namespace/base_link frames and creating a static transform between them. This means that all further humanoids will be spawned under the base_link frame. (otherwise, the warning below again appears)","title":"Moveit"},{"location":"simulation/humanoid_support_moveit_config/#dynamically-updated-urdfs","text":"To make the multiple_humanoid_spawn work, we need to be able to launch different namespaces in the robot_description. This is done by running: moveit_config = ( MoveItConfigsBuilder(\"human_support\", package_name=\"humanoid_support_moveit_config\") .robot_description( file_path=\"config/human_support.urdf.xacro\", mappings={\"namespace\": namespace} ) .to_moveit_configs() ) This updates the namespace argument inside the .xacro files.","title":"Dynamically updated URDFs"},{"location":"simulation/humanoid_support_moveit_config/#planning-scene-monitor","text":"When working with the MoveIt package, some bugs or undesired features were found. When MoveIt is launched, the PlanningSceneMonitor and PlanningFrame subscribe to all TF frames that exist and assume they can transform any known object or sensor data into its planning frame, which is base_link in this case. If this cannot be done, it will throw a warning saying the following: [WARN] [humanoid.moveit.moveit.ros.planning_scene_monitor] [id]: Unable to transform object from frame 'unconnected_frame' to planning frame 'base_link' (Could not find a connection between 'base_link' and 'unconnected_frame' because they are not part of the same tree. TF has two or more unconnected trees) To limit this, a wait function was added: ld.add_action(TimerAction(period=5.0, actions=[rsp_node])) to make sure the transform is published before the rsp_node starts.","title":"Planning scene monitor"},{"location":"simulation/humanoid_support_moveit_config/#figures","text":"Figure 1: Humanoid frames visualization","title":"Figures"},{"location":"simulation/humanoid_support_moveit_config/#rviz2-visualization","text":"If you want to visualize the movement in Rviz, you need to configure the config/moveit.rviz file and change all /humanoid_X instances to the namespace you want to visualize.","title":"Rviz2 visualization"},{"location":"simulation/humanoid_support_moveit_config/#bugs-tf-visualization-and-gazebo-simulation-not-matching","text":"When visualizing and comparing the actual TF data and simulated locations of the humanoids when doing navigation and teleop, it can be seen that these do not match. This is a major issue since that means the location of where the humanoids think they are in the map and the location of where the humanoids actually are in Gazebo will be different. This can be visualized by turning the humanoid 360 degrees in Gazebo with the ./control.sh teleop humanoid_1 command and comparing it to Rviz. There, the humanoid has turned closer to 300 degrees. This problem probably comes from some URDF descriptions not matching the actual geometry or specifications the humanoid has in /home/ros/src/simulation/humanoid_support_moveit_config/config/human_support_wheels.urdf.xacro . By tuning the mass , mu , and inertial values in the <xacro:macro name=\"wheel\" params=\"wheel_prefix *joint_pose\"> xacro tag, it was possible to tune the degree mismatch. For proper navigation and control, this needs to be fixed!","title":"Bugs: TF visualization and Gazebo simulation not matching"},{"location":"simulation/object_mover/","text":"Object Mover package Dyno-robotics/Infotiv delivery Setting self.MODE = \"abnormal/normal\" in the main script causes the object movement to deviate from the normal distribution specified above. To reproduce git checkout .... # In three separate terminals: ./control.sh clean ; ./control.sh build ; ./control.sh sim ./control.sh move_object Random, In Distribution Objects Movement (normal mode) Data collection from cameras according to requirements Random self.objects are placed within the position range self.grid_x, self.grid_y: REQ.ID.1 Random rotation along the z axis. You can use camera_config ID.xacro for placement of several cameras in the intersection. You can alternatively use camera_config ID_noise.xacro to slightly changes the camera settings (REQ.ID.2) as below: at most +-10 degree (+-0.1) rotation around one of the axis at most +-10cm (+-0.1) change in x,y,z coordinates Random, Out of Distribution Objects Movement (abnormal mode) Data collection from cameras according to requirements . Addition of other objects (spotlight, support pole, traffic cone): REQ.OD.1, REQ.OD.2. With some probability, the objects don't leave the scene and may coexist and collide with new objects: REQ.OD.3. Objects are at least rotated by \u03c0/4 (45 degrees, upside-down object): REQ.OD.3, REQ.OD.4. The spotlight is tilted by \u03c0/6 (30 degrees): REQ.OD.1. Objects are not placed on the ground and instead are dropped from a height of 2-3 meters: REQ.OD.4. Deterministic, Disentanglement One-parameter Object Movements (one_object_deterministic mode) Object: forklift Camera: 1 (#Parameter: orientation of forklift) Annotation: Position and rotation of the forklift. Annotation in filename. 256x256 RGB non compression ./control.sh sim ./control.sh move_object","title":"Object Mover"},{"location":"simulation/object_mover/#object-mover-package","text":"Dyno-robotics/Infotiv delivery Setting self.MODE = \"abnormal/normal\" in the main script causes the object movement to deviate from the normal distribution specified above. To reproduce git checkout .... # In three separate terminals: ./control.sh clean ; ./control.sh build ; ./control.sh sim ./control.sh move_object","title":"Object Mover package"},{"location":"simulation/object_mover/#random-in-distribution-objects-movement-normal-mode","text":"Data collection from cameras according to requirements Random self.objects are placed within the position range self.grid_x, self.grid_y: REQ.ID.1 Random rotation along the z axis. You can use camera_config ID.xacro for placement of several cameras in the intersection. You can alternatively use camera_config ID_noise.xacro to slightly changes the camera settings (REQ.ID.2) as below: at most +-10 degree (+-0.1) rotation around one of the axis at most +-10cm (+-0.1) change in x,y,z coordinates","title":"Random, In Distribution Objects Movement (normal mode)"},{"location":"simulation/object_mover/#random-out-of-distribution-objects-movement-abnormal-mode","text":"Data collection from cameras according to requirements . Addition of other objects (spotlight, support pole, traffic cone): REQ.OD.1, REQ.OD.2. With some probability, the objects don't leave the scene and may coexist and collide with new objects: REQ.OD.3. Objects are at least rotated by \u03c0/4 (45 degrees, upside-down object): REQ.OD.3, REQ.OD.4. The spotlight is tilted by \u03c0/6 (30 degrees): REQ.OD.1. Objects are not placed on the ground and instead are dropped from a height of 2-3 meters: REQ.OD.4.","title":"Random, Out of Distribution Objects Movement (abnormal mode)"},{"location":"simulation/object_mover/#deterministic-disentanglement-one-parameter-object-movements-one_object_deterministic-mode","text":"Object: forklift Camera: 1 (#Parameter: orientation of forklift) Annotation: Position and rotation of the forklift. Annotation in filename. 256x256 RGB non compression ./control.sh sim ./control.sh move_object","title":"Deterministic, Disentanglement One-parameter Object Movements (one_object_deterministic mode)"},{"location":"simulation/pallet_truck/","text":"Pallet Truck Package Common packages for pallet_truck, including messages and robot description. These are packages relevant to all pallet_truck workspaces, whether simulation, desktop, or on the robot's own headless PC. Links to read about each individual pkg - pallet_truck_bringup - pallet_truck_control - pallet_truck_description - pallet_truck_navigation prefix is used to publish unique base link names of each robot agent to /tf . This way they are all visible in rviz and probably it is used for odometry done by aruco localization. namespace is used to separate nodes and topics related to each robot agent. This way we can control each robot separately and run a separate nav2 stack. Keep in mind that the value of namespace is used for prefix but they are different concepts and use cases.","title":"Pallet Truck"},{"location":"simulation/pallet_truck/#pallet-truck-package","text":"Common packages for pallet_truck, including messages and robot description. These are packages relevant to all pallet_truck workspaces, whether simulation, desktop, or on the robot's own headless PC. Links to read about each individual pkg - pallet_truck_bringup - pallet_truck_control - pallet_truck_description - pallet_truck_navigation prefix is used to publish unique base link names of each robot agent to /tf . This way they are all visible in rviz and probably it is used for odometry done by aruco localization. namespace is used to separate nodes and topics related to each robot agent. This way we can control each robot separately and run a separate nav2 stack. Keep in mind that the value of namespace is used for prefix but they are different concepts and use cases.","title":"Pallet Truck Package"},{"location":"simulation/pallet_truck/pallet_truck_bringup/","text":"Pallet Truck bringup This package handles initialization and status of spawned robots. gazebo.launch.py - Spawns robot, handles robot_state_publisher, robot_description keyboard_steering.launch.py - keyboard steering. multiple_robot_spawn.launch.py - contains configurable list of robots you spawn in the sim. sim.launch.py - main launch file for single robot. Make sure Gazebo is running; control, twist_mux, and keyboard_steering are all launched. rviz.launch.py - runs RViz The package focuses on pallet_truck and forklift for now, but could be made modular to spawn other types of robots. Configuring and spawning robots inside the sim Below is the structure which we use to spawn robots inside of the sim. Please read these notes before setting up a new or editing a robot. Here is the information from the table presented as a markdown list: Robot Spawning Attributes namespace Description: Used to differentiate between multiple robots. Follows the format robot_agent_N , where N is the robot ID. Example: \"robot_agent_1\" initial_pose_x Description: The robot's initial x-coordinate position. Float value wrapped as a string. Example: \"10.0\" initial_pose_y Description: The robot's initial y-coordinate position. Float value wrapped as a string. Example: \"1.0\" robot_type Description: Selects the robot mesh or appearance. Options are \"pallet_truck\" or \"forklift\" . Example: \"pallet_truck\" aruco_id Description: Sets the ID shown on the robot's ArUco marker. Must match the ID in the namespace . Example: \"1\" if namespace ID is 1 Would you like me to use these attributes to create a sample robot configuration? Example robot setup: { \"namespace\": \"robot_agent_1\", \"initial_pose_x\":\"10.0\", \"initial_pose_y\":\"1.0\", \"robot_type\":\"pallet_truck\", \"aruco_id\":\"1\" } Automatically generated parameter files Some parameter files are automatically generated for the pallet_truck_bringup package. The generation scripts can be found in the config_generation/ directory. The automatically generated files include the nav2_params.yaml , gz_bridge.yaml and more. To look at all the automatically generated files, build the workspace and the generated files will be printed in the terminal or look manually in the config_generation/generate.py.","title":"Bringup"},{"location":"simulation/pallet_truck/pallet_truck_bringup/#pallet-truck-bringup","text":"This package handles initialization and status of spawned robots. gazebo.launch.py - Spawns robot, handles robot_state_publisher, robot_description keyboard_steering.launch.py - keyboard steering. multiple_robot_spawn.launch.py - contains configurable list of robots you spawn in the sim. sim.launch.py - main launch file for single robot. Make sure Gazebo is running; control, twist_mux, and keyboard_steering are all launched. rviz.launch.py - runs RViz The package focuses on pallet_truck and forklift for now, but could be made modular to spawn other types of robots.","title":"Pallet Truck bringup"},{"location":"simulation/pallet_truck/pallet_truck_bringup/#configuring-and-spawning-robots-inside-the-sim","text":"Below is the structure which we use to spawn robots inside of the sim. Please read these notes before setting up a new or editing a robot. Here is the information from the table presented as a markdown list:","title":"Configuring and spawning robots inside the sim"},{"location":"simulation/pallet_truck/pallet_truck_bringup/#robot-spawning-attributes","text":"namespace Description: Used to differentiate between multiple robots. Follows the format robot_agent_N , where N is the robot ID. Example: \"robot_agent_1\" initial_pose_x Description: The robot's initial x-coordinate position. Float value wrapped as a string. Example: \"10.0\" initial_pose_y Description: The robot's initial y-coordinate position. Float value wrapped as a string. Example: \"1.0\" robot_type Description: Selects the robot mesh or appearance. Options are \"pallet_truck\" or \"forklift\" . Example: \"pallet_truck\" aruco_id Description: Sets the ID shown on the robot's ArUco marker. Must match the ID in the namespace . Example: \"1\" if namespace ID is 1 Would you like me to use these attributes to create a sample robot configuration? Example robot setup: { \"namespace\": \"robot_agent_1\", \"initial_pose_x\":\"10.0\", \"initial_pose_y\":\"1.0\", \"robot_type\":\"pallet_truck\", \"aruco_id\":\"1\" }","title":"Robot Spawning Attributes"},{"location":"simulation/pallet_truck/pallet_truck_bringup/#automatically-generated-parameter-files","text":"Some parameter files are automatically generated for the pallet_truck_bringup package. The generation scripts can be found in the config_generation/ directory. The automatically generated files include the nav2_params.yaml , gz_bridge.yaml and more. To look at all the automatically generated files, build the workspace and the generated files will be printed in the terminal or look manually in the config_generation/generate.py.","title":"Automatically generated parameter files"},{"location":"simulation/pallet_truck/pallet_truck_control/","text":"Pallet Truck Control The pallet_pallet_truck_control package includes all nodes necessary for the control of the robots. These include the following: twist_mux : Takes in velocity topics and orders them in order of relevance. from highest to lowest: /key_vel, /safety_vel, / scenario_vel, /nav_vel, /cmd_vel twist_stamper : As of ros2 jazzy the ros2_controllers need the twist messages to be stamped, therefore the stamper was introduced ros2_control_node : Main node which uses the control.yaml file and maps \"hardware\" to controller actions. spawner-joint_state_broadcaster : Publishes the joint_state so the robot moves in gazebo and rviz. spawner-velocity_controller : Accepts and publishes velocity commands With this setup, the robot_agents can be controlled by running the teleop command in control.sh.","title":"Control"},{"location":"simulation/pallet_truck/pallet_truck_control/#pallet-truck-control","text":"The pallet_pallet_truck_control package includes all nodes necessary for the control of the robots. These include the following: twist_mux : Takes in velocity topics and orders them in order of relevance. from highest to lowest: /key_vel, /safety_vel, / scenario_vel, /nav_vel, /cmd_vel twist_stamper : As of ros2 jazzy the ros2_controllers need the twist messages to be stamped, therefore the stamper was introduced ros2_control_node : Main node which uses the control.yaml file and maps \"hardware\" to controller actions. spawner-joint_state_broadcaster : Publishes the joint_state so the robot moves in gazebo and rviz. spawner-velocity_controller : Accepts and publishes velocity commands With this setup, the robot_agents can be controlled by running the teleop command in control.sh.","title":"Pallet Truck Control"},{"location":"simulation/pallet_truck/pallet_truck_description/","text":"pallet_truck Description This packages contains the meshes and URDF of the pallet_truck robot, its supported sensors, and their supported mounts. At the moment there are 2 viable pallet_truck urdfs. pallet_truck.urdf.xacro pallet_truck_simpkle_collisions.urdf.xacro The puropse is to make them the same but with the simple collision using a box as a collision tag instead of the mesh itself. To switch between them update the following code in simulation/pallet_truck/pallet_truck_bringup/launch/gazebo.launch.py: PathJoinSubstitution( [ FindPackageShare(\"pallet_truck_description\"), \"urdf\", \"pallet_truck.urdf.xacro\", #<---- ] ), Sensors GPS: Novatel Smart6 and Smart7 2D LiDAR: SICK LMS1xx 2D LiDAR: Hokuyo UST-10 3D LiDAR: Velodyne VLP16 and HDL-32E Camera: Flir/Pointgrey Flea3 and Flea3 Stereo Camera: Flir/Pointgrey Bumbleebee2 Camera: Flir/Pointgrey BlackflyS","title":"pallet_truck Description"},{"location":"simulation/pallet_truck/pallet_truck_description/#pallet_truck-description","text":"This packages contains the meshes and URDF of the pallet_truck robot, its supported sensors, and their supported mounts. At the moment there are 2 viable pallet_truck urdfs. pallet_truck.urdf.xacro pallet_truck_simpkle_collisions.urdf.xacro The puropse is to make them the same but with the simple collision using a box as a collision tag instead of the mesh itself. To switch between them update the following code in simulation/pallet_truck/pallet_truck_bringup/launch/gazebo.launch.py: PathJoinSubstitution( [ FindPackageShare(\"pallet_truck_description\"), \"urdf\", \"pallet_truck.urdf.xacro\", #<---- ] ),","title":"pallet_truck Description"},{"location":"simulation/pallet_truck/pallet_truck_description/#sensors","text":"GPS: Novatel Smart6 and Smart7 2D LiDAR: SICK LMS1xx 2D LiDAR: Hokuyo UST-10 3D LiDAR: Velodyne VLP16 and HDL-32E Camera: Flir/Pointgrey Flea3 and Flea3 Stereo Camera: Flir/Pointgrey Bumbleebee2 Camera: Flir/Pointgrey BlackflyS","title":"Sensors"},{"location":"simulation/pallet_truck/pallet_truck_description/urdf/","text":"Pallet_truck_description In addition to the change_log, this description is added to give vital information that may help other developers in the future to debug or update features faster. Here additions to the urdf.xacro files are mentioned Collision sensor A collision_sensor was added and linked to the mesh tag of the pallet_trucks. This is integrated in the pallet_truck.urdf.xacro. To visualize the topic subscribe to: /namespace/collision e.g. /robot_agent_1/contact. The topic publishes information about the position, torque and which models are in contact with each other. The topic name is defined by the automatically generated gz_bridge() which can be found in /home/ros/src/simulation/pallet_truck/pallet_truck_bringup/launch/generate_gz_bridge.py xacro/urdf/sdf files Bugs Gazebo only reads sdf files and if .xacro or .urdf files are used, they are later parsed to .sdf's before being used by Gazebo. A \"bug\" for Gazebo harmonic using ros2 jazzy was that when adding the collision sensor for the pallet_trucks, Gazebo was unable to find the collision tag for the mesh that we were using. After investigation it was found that when the .xacro file was parsed to an .sdf file, the name of the collision tag was updated by the parse plugin. Therefore this lumped renaming of the collision tag was \"hard coded\" instead of using its original name which is in the pallet_truck.urdf.xacro. original: \"\\$(arg prefix)/chassis_link::collision\" hard coded: \"\\${prefix}_base_link_fixed_joint_lump__collision_collision\" When adding sensors in the xacro file, make sure the plugins are also loaded. There are different world and robot plugins so if system plugins are used they should most likely be added to the .world file","title":"Pallet_truck_description"},{"location":"simulation/pallet_truck/pallet_truck_description/urdf/#pallet_truck_description","text":"In addition to the change_log, this description is added to give vital information that may help other developers in the future to debug or update features faster. Here additions to the urdf.xacro files are mentioned","title":"Pallet_truck_description"},{"location":"simulation/pallet_truck/pallet_truck_description/urdf/#collision-sensor","text":"A collision_sensor was added and linked to the mesh tag of the pallet_trucks. This is integrated in the pallet_truck.urdf.xacro. To visualize the topic subscribe to: /namespace/collision e.g. /robot_agent_1/contact. The topic publishes information about the position, torque and which models are in contact with each other. The topic name is defined by the automatically generated gz_bridge() which can be found in /home/ros/src/simulation/pallet_truck/pallet_truck_bringup/launch/generate_gz_bridge.py","title":"Collision sensor"},{"location":"simulation/pallet_truck/pallet_truck_description/urdf/#xacrourdfsdf-files-bugs","text":"Gazebo only reads sdf files and if .xacro or .urdf files are used, they are later parsed to .sdf's before being used by Gazebo. A \"bug\" for Gazebo harmonic using ros2 jazzy was that when adding the collision sensor for the pallet_trucks, Gazebo was unable to find the collision tag for the mesh that we were using. After investigation it was found that when the .xacro file was parsed to an .sdf file, the name of the collision tag was updated by the parse plugin. Therefore this lumped renaming of the collision tag was \"hard coded\" instead of using its original name which is in the pallet_truck.urdf.xacro. original: \"\\$(arg prefix)/chassis_link::collision\" hard coded: \"\\${prefix}_base_link_fixed_joint_lump__collision_collision\" When adding sensors in the xacro file, make sure the plugins are also loaded. There are different world and robot plugins so if system plugins are used they should most likely be added to the .world file","title":"xacro/urdf/sdf files Bugs"},{"location":"simulation/pallet_truck/pallet_truck_navigation/","text":"Navigation (Robot agent and Humanoid) and Map server the pallet truck navigation consist of: - map_server.launch.py - starts the map server and creates necessary transforms for all of the robot agents. - nav2.launch.py - starts the navigation stack and the update_map.py for each robot. - cartography.launch.py - starts cartography. - update_map.py - adds the dynamic obstacles (other robot_agents) to each robot_agent's map. To run the navigation stack do the following either with the input=ROBOTS or HUMANOIDS ./control.sh nav ROBOTS Map server Instead of starting with an empty map, to get the general layout of the environment and detect the static object in the environment (walls, cooridors and hallways) you can optionally build a map. To start mapping (otherwise known as cartography), you can use cartography.launch.py . It requires a lidar to be mounted on the robot_agent and odometry to exist, which can be activated from the camera_utility/aruco_localization package. Launch the following package, run gazebo, rviz, aruco_localization, and start moving around the simulation with the robot_agent. When you are finished you need to save it using this command: ros2 run nav2_map_server map_saver_cli -f simulation/pallet_truck/pallet_truck_navigation/maps/YOUR_MAP_NAME . Keep in mind that this step needs to be done only once and the map assumed to be static. The navigation stack uses a base map which is updated dynamically with other robot_agents for each robot. This done to add the static environment and update the dynamic moving obstacles later. Node( # Manually setting the joint between map and odom to initial_pose_x initial_pose_y 0, i.e. the location of where the robot spawn. map -> odom package=\"tf2_ros\", executable=\"static_transform_publisher\", namespace=robot[\"namespace\"], name=\"static_map_to_odom\", arguments=[f\"{robot[\"initial_pose_x\"]}\", f\"{robot[\"initial_pose_y\"]}\", \"0\", \"0\", \"0\", \"0\", f\"{robot[\"namespace\"]}/map\", f\"{robot[\"namespace\"]}/odom\"], ) Robot_agent navigation This package covers the functionality of mapping, localizing, and navigation for the robot_agent. Each package is built using code from the nav2 packages, thus you are able to modify the launch files but to change the node's behaviour you need to modify the config files. The code is tailored for the robot_agent and using the aruco_detection package that supplies /TF chain for the truck. The aruco_localization package runs the aruco localisation. The map_server.py creates a map for each agent The nav2.launch.py uses the localisation (from the previous steps) for the navigation of the robot_agent. Keep in mind that the same namespace has to be used when launching the navigation stack which is done automatically: ros2 launch pallet_truck_navigation nav2.launch.py robots:=ROBOTS For navigation to be able to automatically find the robot agent, we have to set a namespace that specified when launching the robot_agent. (see above) To do navigation for both humanoids and pallet_trucks at the same time you need to create a merged variable of the 2 and sent that as a input to the map_server.launch.py and nav2.launch.py to make sure the obstacles are detected. Each robot has it own navigation configuration in nav2_params.yaml which is dynamically generated on build based on the ROBOTS and HUMANOIDS variables in config.sh. the pallet_trucks and forklifts also have an individual navigate_w_replaning_and_recovery_robot_agent_x.xml which is supposed to stop the navigation of the agents if their aruco_markers are not detected. To visualize which aruco markers are seen, first run the gpss and nav operations and then echo the /aruco_marker_seen topic. We use following /TF structure: Note that world and map are static at the same position. robot_agent_X/odom is static at the position from where the robots are spawned and the new position of the robots are then determined by robot_agent_X/base_link which is a dynamic transform from robot_agent_X/odom . Good video to watch for multi agent navigation: https://www.youtube.com/watch?v=cGUueuIAFgw Humanoid_navigation The humanoid navigation is implemented in the same way as for the robot_agents with the difference of calling the function ./control.sh nav HUMANOIDS . Then the same principles apply but with a different nav2_parameter file being used as an argument to nav2 nodes. There is one major difference between the navigation of the humanoids and the robot_agents. The robot _agents get their odom frame from the aruco_localization pkg which find aruco_markers and publishes their orientation and position. The humanoids on the other hand get their odom frame from the humanoid_odom_pub node which subscribes to the ground truth of the humanoids pose from gazebo and creates a dynamic transform between the namespace/odom -> namespace/base_link frames. This means the robot_agents get their odom frame from what the cameras can see and the humanoids get their odom frame from the actual pose in the simulation. Obstacles detection (update_map_node) The obstacle detection is based on a static map which is dynamically updated with all other robot agents except itself. Therefore every robot_agent has its own map to not map itself. To not depend on the orientation of the robots, the obstacle is depicted as a circle. With this setup the map_server is gets an action call that updates the moving obstacles The /map_updater/update_map_node.py node works as following: First it makes a copy of the warehouse.pgm map which is a long array of pixel values ranging from (0-255). Since the map nav2 needs has a different setup than the .pgm file, a conversion is needed. The .pgm file has these pixels ranges value meaning 0 black obstacle 255 White Free space 1-254 ranges of gray, not defined at the moment The map nav2 needs is set up with pixels ranging from (-1-100) where value meaning -1 Unknown 0 Free space 100 obstacle 1-99 probabilistic occupancy therefore this has to be parsed to match by doing the following: current_array = self.original_array.copy() occupancy_array = np.zeros(current_array.shape, dtype=np.uint8) occupancy_array[current_array.copy()<100] = 100 The map is a 2D array, typically stored row-major from bottom-left, but many implementations flip it vertically ([::-1]) to match how image viewers treat top-left as (0,0). and therefore the following is done occupancy_array = occupancy_array[::-1, :] The position of all other robot_agents are found by their transforms between robot_agent_x/base_link and world and an obstacle is set at that position which is updated with 1 Hz to continuously update their positions. This can be tweaked but to limit CPU usage it was set to 1 Hz. This concept is repeated for all namespaces sent through the ./control.sh nav function. Future improvements How often the new path the pallet_trucks can take be update Add a speed limiter instead of obstacle, in those cases the robot will slow down instead of replanning the paths. Could probably mix with the inflation radius's of the costmaps to make robot take wider turns Collision Two robots that are approaching each other (from left and right) don't know where each one is planning to go. Therefore they might both plan to the same path and both trie to out turn each other resulting in a race condition. Possible solution: block future path on every other robots map! DEBUG Getting this warning below is a sign that the nav2 does not work as intended. If this appears repeatedly, you can check the TF tree to see if there is a connection between the robot_agent link and world link. If there is no connection, it probably means that no camera is viewing the robot. If so then either move the robot into a camera's view or add more cameras to the simulation. [update_map_node-6] [WARN] [robot_agent_1.update_map] [1767949972.586691493]: Transform not available yet: Could not find a connection between 'world' and 'robot_agent_2/base_link' because they are not part of the same tree.Tf has two or more unconnected trees.","title":"Navigation"},{"location":"simulation/pallet_truck/pallet_truck_navigation/#navigation-robot-agent-and-humanoid-and-map-server","text":"the pallet truck navigation consist of: - map_server.launch.py - starts the map server and creates necessary transforms for all of the robot agents. - nav2.launch.py - starts the navigation stack and the update_map.py for each robot. - cartography.launch.py - starts cartography. - update_map.py - adds the dynamic obstacles (other robot_agents) to each robot_agent's map. To run the navigation stack do the following either with the input=ROBOTS or HUMANOIDS ./control.sh nav ROBOTS","title":"Navigation (Robot agent and Humanoid) and Map server"},{"location":"simulation/pallet_truck/pallet_truck_navigation/#map-server","text":"Instead of starting with an empty map, to get the general layout of the environment and detect the static object in the environment (walls, cooridors and hallways) you can optionally build a map. To start mapping (otherwise known as cartography), you can use cartography.launch.py . It requires a lidar to be mounted on the robot_agent and odometry to exist, which can be activated from the camera_utility/aruco_localization package. Launch the following package, run gazebo, rviz, aruco_localization, and start moving around the simulation with the robot_agent. When you are finished you need to save it using this command: ros2 run nav2_map_server map_saver_cli -f simulation/pallet_truck/pallet_truck_navigation/maps/YOUR_MAP_NAME . Keep in mind that this step needs to be done only once and the map assumed to be static. The navigation stack uses a base map which is updated dynamically with other robot_agents for each robot. This done to add the static environment and update the dynamic moving obstacles later. Node( # Manually setting the joint between map and odom to initial_pose_x initial_pose_y 0, i.e. the location of where the robot spawn. map -> odom package=\"tf2_ros\", executable=\"static_transform_publisher\", namespace=robot[\"namespace\"], name=\"static_map_to_odom\", arguments=[f\"{robot[\"initial_pose_x\"]}\", f\"{robot[\"initial_pose_y\"]}\", \"0\", \"0\", \"0\", \"0\", f\"{robot[\"namespace\"]}/map\", f\"{robot[\"namespace\"]}/odom\"], )","title":"Map server"},{"location":"simulation/pallet_truck/pallet_truck_navigation/#robot_agent-navigation","text":"This package covers the functionality of mapping, localizing, and navigation for the robot_agent. Each package is built using code from the nav2 packages, thus you are able to modify the launch files but to change the node's behaviour you need to modify the config files. The code is tailored for the robot_agent and using the aruco_detection package that supplies /TF chain for the truck. The aruco_localization package runs the aruco localisation. The map_server.py creates a map for each agent The nav2.launch.py uses the localisation (from the previous steps) for the navigation of the robot_agent. Keep in mind that the same namespace has to be used when launching the navigation stack which is done automatically: ros2 launch pallet_truck_navigation nav2.launch.py robots:=ROBOTS For navigation to be able to automatically find the robot agent, we have to set a namespace that specified when launching the robot_agent. (see above) To do navigation for both humanoids and pallet_trucks at the same time you need to create a merged variable of the 2 and sent that as a input to the map_server.launch.py and nav2.launch.py to make sure the obstacles are detected. Each robot has it own navigation configuration in nav2_params.yaml which is dynamically generated on build based on the ROBOTS and HUMANOIDS variables in config.sh. the pallet_trucks and forklifts also have an individual navigate_w_replaning_and_recovery_robot_agent_x.xml which is supposed to stop the navigation of the agents if their aruco_markers are not detected. To visualize which aruco markers are seen, first run the gpss and nav operations and then echo the /aruco_marker_seen topic. We use following /TF structure: Note that world and map are static at the same position. robot_agent_X/odom is static at the position from where the robots are spawned and the new position of the robots are then determined by robot_agent_X/base_link which is a dynamic transform from robot_agent_X/odom . Good video to watch for multi agent navigation: https://www.youtube.com/watch?v=cGUueuIAFgw","title":"Robot_agent navigation"},{"location":"simulation/pallet_truck/pallet_truck_navigation/#humanoid_navigation","text":"The humanoid navigation is implemented in the same way as for the robot_agents with the difference of calling the function ./control.sh nav HUMANOIDS . Then the same principles apply but with a different nav2_parameter file being used as an argument to nav2 nodes. There is one major difference between the navigation of the humanoids and the robot_agents. The robot _agents get their odom frame from the aruco_localization pkg which find aruco_markers and publishes their orientation and position. The humanoids on the other hand get their odom frame from the humanoid_odom_pub node which subscribes to the ground truth of the humanoids pose from gazebo and creates a dynamic transform between the namespace/odom -> namespace/base_link frames. This means the robot_agents get their odom frame from what the cameras can see and the humanoids get their odom frame from the actual pose in the simulation.","title":"Humanoid_navigation"},{"location":"simulation/pallet_truck/pallet_truck_navigation/#obstacles-detection-update_map_node","text":"The obstacle detection is based on a static map which is dynamically updated with all other robot agents except itself. Therefore every robot_agent has its own map to not map itself. To not depend on the orientation of the robots, the obstacle is depicted as a circle. With this setup the map_server is gets an action call that updates the moving obstacles The /map_updater/update_map_node.py node works as following: First it makes a copy of the warehouse.pgm map which is a long array of pixel values ranging from (0-255). Since the map nav2 needs has a different setup than the .pgm file, a conversion is needed. The .pgm file has these pixels ranges value meaning 0 black obstacle 255 White Free space 1-254 ranges of gray, not defined at the moment The map nav2 needs is set up with pixels ranging from (-1-100) where value meaning -1 Unknown 0 Free space 100 obstacle 1-99 probabilistic occupancy therefore this has to be parsed to match by doing the following: current_array = self.original_array.copy() occupancy_array = np.zeros(current_array.shape, dtype=np.uint8) occupancy_array[current_array.copy()<100] = 100 The map is a 2D array, typically stored row-major from bottom-left, but many implementations flip it vertically ([::-1]) to match how image viewers treat top-left as (0,0). and therefore the following is done occupancy_array = occupancy_array[::-1, :] The position of all other robot_agents are found by their transforms between robot_agent_x/base_link and world and an obstacle is set at that position which is updated with 1 Hz to continuously update their positions. This can be tweaked but to limit CPU usage it was set to 1 Hz. This concept is repeated for all namespaces sent through the ./control.sh nav function.","title":"Obstacles detection (update_map_node)"},{"location":"simulation/pallet_truck/pallet_truck_navigation/#future-improvements","text":"How often the new path the pallet_trucks can take be update Add a speed limiter instead of obstacle, in those cases the robot will slow down instead of replanning the paths. Could probably mix with the inflation radius's of the costmaps to make robot take wider turns Collision Two robots that are approaching each other (from left and right) don't know where each one is planning to go. Therefore they might both plan to the same path and both trie to out turn each other resulting in a race condition. Possible solution: block future path on every other robots map!","title":"Future improvements"},{"location":"simulation/pallet_truck/pallet_truck_navigation/#debug","text":"Getting this warning below is a sign that the nav2 does not work as intended. If this appears repeatedly, you can check the TF tree to see if there is a connection between the robot_agent link and world link. If there is no connection, it probably means that no camera is viewing the robot. If so then either move the robot into a camera's view or add more cameras to the simulation. [update_map_node-6] [WARN] [robot_agent_1.update_map] [1767949972.586691493]: Transform not available yet: Could not find a connection between 'world' and 'robot_agent_2/base_link' because they are not part of the same tree.Tf has two or more unconnected trees.","title":"DEBUG"},{"location":"simulation/raw_models/","text":"Converting meshes to SDF models Here are the steps to convert FreeCAD meshes to SDF models (for Gazebo): Script step-by-step Install FreeCAD sudo apt-get install freecad=0.19.2+dfsg1-3ubuntu1 (Tested with version 0.19) Install Blender v3.3 LTS Download phobos.zip (tested with phobos 2.0 \"Perilled Pangolin\". ) Install phobos by following the guide on their GitHub In your phobos settings (easiest accessed through the Blender GUI), change the models folder to where you want the output to be. You can add a relative path here so that the output folder depends on where you run the script, e.g., setting the model folder to ./phobos_out/ would create a ./phobos_out/ directory as a sub-directory to wherever you run the script and save the exported SDFs there. Enter FreeCAD and load your .FCstd , then select the body in the tree view and select File -> Export and choose to export it as a Collada ( .dae ) file. Run ./build.sh You should now be able to load the model/directory created by the reformat script directly through Gazebo. wget https://download.blender.org/release/Blender3.3/blender-3.3.0-linux-x64.tar.xz tar -xf blender-3.3.0-linux-x64.tar.xz mv blender-3.3.0-linux-x64 blender alias blender=\"`pwd`/blender/blender\" blender --version git clone git@github.com:dfki-ric/phobos.git cd phobos git checkout 2.0.0 blender -b --python install_requirements.py cd .. ./build.sh In GUI Import .dae file Set phobostype to visual Select the object and set the geometry type Guide says to Object->Apply->Rotation & Scale here, but I'm not sure what it actually accomplishes This will apply the scaling of objects to their mesh information (vertices) and applies the current orientation to the mesh as well. After this option, all objects have zero orientation. If this is not desired, one can also only apply the Scale . Create a \"visual\" collection and put the model into there; this lets us create a tree structure. (unsure of necessity when we only have one model) Select the object and press Create Link(s) , make sure selected objects mode is chosen. Select the visual and then Create Collision . Select the object and then Create Inertials . Here we need to manually enter our mass; we should check if that can be adjusted somehow. Save and export. Then we need to move around the resulting files so they're structured in the way Gazebo wants them, i.e.: model \u251c\u2500\u2500 meshes \u2502 \u2514\u2500\u2500 filename.dae \u251c\u2500\u2500 model.config \u2514\u2500\u2500 model.sdf In order to accomplish this, we'll have to create the model.config file; however, this is relatively simple as it doesn't contain any unique data between different models aside from the model name. Example of a successful build: simulation/raw_models$ ./build_models.sh ../simlan_gazebo_environment/models/aruco/materials/textures/0.png ../simlan_gazebo_environment/models/aruco/materials/textures/1.png ../simlan_gazebo_environment/models/aruco/materials/textures/2.png ../simlan_gazebo_environment/models/aruco/materials/textures/3.png Color management: using fallback mode for management Color management: Error could not find role data role. Blender 3.0.1 Read prefs: [HOME]/.config/blender/3.0/config/userpref.blend Color management: scene view \"Filmic\" not found, setting default \"Standard\". /usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2 warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\" Checking requirements: ensurepip is disabled in Debian/Ubuntu for the system python. Python modules for the system python are usually handled by dpkg and apt-get. apt install python3-<module name> Install the python3-pip package to use pip itself. Using pip together with the system python might have unexpected results for any system-installed module, so use it at your own risk, or make sure to only use it in virtual environments. WARNING: We couldn't do ensurepip; we try to continue anyway Checking yaml Checking numpy Checking scipy Checking collada Checking pydot Checking lxml Checking networkx Checking trimesh Checking PIL Importing phobos IMPORT: phobos.blender.defs Parsing definitions from: [HOME]/.config/blender/3.0/scripts/addons/phobos/data/blender/definitions defaultControllers.yml defaultSensors.yml defaultMaterials.yml defaultJoints.yml defaultSubmechanisms.yml defaultMotors.yml Creating new definition type: joints Creating new definition type: motors IMPORT: phobos.blender.display IMPORT: phobos.blender.io RELOAD: phobos.blender.model IMPORT: phobos.blender.operators RELOAD: phobos.blender.phobosgui RELOAD: phobos.blender.phoboslog RELOAD: phobos.blender.phobossystem RELOAD: phobos.blender.reserved_keys RELOAD: phobos.blender.utils Registering operators.selection... Registering operators.io... Registering operators.editing... TypeError: EnumProperty(..., default='mechanism'): not found in enum members ValueError: bpy_struct \"PHOBOS_OT_define_submodel\" registration error: 'submodeltype' EnumProperty could not register (see previous error) Registering operators.naming... Registering operators.misc... Registering phobosgui... ... successful. +-- Collada Import parameters------ | input file : ./objects/meshes/eur-pallet.dae | use units : no | autoconnect : yes +-- Armature Import parameters ---- | find bone chains: yes | min chain len : 0 | fix orientation : yes | keep bind info : no | IOR of negative value is not allowed for materials (using Blender default value instead)+-- Import Scene -------- | NODE id='node0', name='node0' +---------------------------------- | Collada Import : OK +---------------------------------- Error in sys.excepthook: Original exception was: File \"[HOME]/.config/blender/3.0/scripts/addons/phobos/blender/operators/editing.py\", line 1048, in toggleVisual Error in sys.excepthook: Original exception was: File \"[HOME]/.config/blender/3.0/scripts/addons/phobos/blender/operators/editing.py\", line 1051, in toggleCollision [20231211_08:52:53] WARNING No text file README.md found. (phobos/blender/utils/blender.py - readTextFile (l259)) Collada export to: [PATH]simulation/raw_models/phobos_out/unnamed/meshes/dae/Body.dae Info: Exported 1 Object Info: Exported 1 Object Collada export to: [PATH]simulation/raw_models/phobos_out/unnamed/meshes/dae/Body.dae Info: Exported 1 Object Info: Exported 1 Object Info: Phobos exported to: phobos_out/unnamed Info: Export successful. Info: Phobos exported to: phobos_out/unnamed Info: Export successful. ---------------------------------------------------------------------------------------------------- Unregistering Phobos... Unregistering phobosgui... Unregistering display... Unregistering icons... Unregistering classes... Unregistering manuals... ... successful. Blender quit Error: Not freed memory blocks: 14, total unfreed memory 0.002426 MB","title":"Models"},{"location":"simulation/raw_models/#converting-meshes-to-sdf-models","text":"Here are the steps to convert FreeCAD meshes to SDF models (for Gazebo):","title":"Converting meshes to SDF models"},{"location":"simulation/raw_models/#script-step-by-step","text":"Install FreeCAD sudo apt-get install freecad=0.19.2+dfsg1-3ubuntu1 (Tested with version 0.19) Install Blender v3.3 LTS Download phobos.zip (tested with phobos 2.0 \"Perilled Pangolin\". ) Install phobos by following the guide on their GitHub In your phobos settings (easiest accessed through the Blender GUI), change the models folder to where you want the output to be. You can add a relative path here so that the output folder depends on where you run the script, e.g., setting the model folder to ./phobos_out/ would create a ./phobos_out/ directory as a sub-directory to wherever you run the script and save the exported SDFs there. Enter FreeCAD and load your .FCstd , then select the body in the tree view and select File -> Export and choose to export it as a Collada ( .dae ) file. Run ./build.sh You should now be able to load the model/directory created by the reformat script directly through Gazebo. wget https://download.blender.org/release/Blender3.3/blender-3.3.0-linux-x64.tar.xz tar -xf blender-3.3.0-linux-x64.tar.xz mv blender-3.3.0-linux-x64 blender alias blender=\"`pwd`/blender/blender\" blender --version git clone git@github.com:dfki-ric/phobos.git cd phobos git checkout 2.0.0 blender -b --python install_requirements.py cd .. ./build.sh","title":"Script step-by-step"},{"location":"simulation/raw_models/#in-gui","text":"Import .dae file Set phobostype to visual Select the object and set the geometry type Guide says to Object->Apply->Rotation & Scale here, but I'm not sure what it actually accomplishes This will apply the scaling of objects to their mesh information (vertices) and applies the current orientation to the mesh as well. After this option, all objects have zero orientation. If this is not desired, one can also only apply the Scale . Create a \"visual\" collection and put the model into there; this lets us create a tree structure. (unsure of necessity when we only have one model) Select the object and press Create Link(s) , make sure selected objects mode is chosen. Select the visual and then Create Collision . Select the object and then Create Inertials . Here we need to manually enter our mass; we should check if that can be adjusted somehow. Save and export. Then we need to move around the resulting files so they're structured in the way Gazebo wants them, i.e.: model \u251c\u2500\u2500 meshes \u2502 \u2514\u2500\u2500 filename.dae \u251c\u2500\u2500 model.config \u2514\u2500\u2500 model.sdf In order to accomplish this, we'll have to create the model.config file; however, this is relatively simple as it doesn't contain any unique data between different models aside from the model name. Example of a successful build: simulation/raw_models$ ./build_models.sh ../simlan_gazebo_environment/models/aruco/materials/textures/0.png ../simlan_gazebo_environment/models/aruco/materials/textures/1.png ../simlan_gazebo_environment/models/aruco/materials/textures/2.png ../simlan_gazebo_environment/models/aruco/materials/textures/3.png Color management: using fallback mode for management Color management: Error could not find role data role. Blender 3.0.1 Read prefs: [HOME]/.config/blender/3.0/config/userpref.blend Color management: scene view \"Filmic\" not found, setting default \"Standard\". /usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2 warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\" Checking requirements: ensurepip is disabled in Debian/Ubuntu for the system python. Python modules for the system python are usually handled by dpkg and apt-get. apt install python3-<module name> Install the python3-pip package to use pip itself. Using pip together with the system python might have unexpected results for any system-installed module, so use it at your own risk, or make sure to only use it in virtual environments. WARNING: We couldn't do ensurepip; we try to continue anyway Checking yaml Checking numpy Checking scipy Checking collada Checking pydot Checking lxml Checking networkx Checking trimesh Checking PIL Importing phobos IMPORT: phobos.blender.defs Parsing definitions from: [HOME]/.config/blender/3.0/scripts/addons/phobos/data/blender/definitions defaultControllers.yml defaultSensors.yml defaultMaterials.yml defaultJoints.yml defaultSubmechanisms.yml defaultMotors.yml Creating new definition type: joints Creating new definition type: motors IMPORT: phobos.blender.display IMPORT: phobos.blender.io RELOAD: phobos.blender.model IMPORT: phobos.blender.operators RELOAD: phobos.blender.phobosgui RELOAD: phobos.blender.phoboslog RELOAD: phobos.blender.phobossystem RELOAD: phobos.blender.reserved_keys RELOAD: phobos.blender.utils Registering operators.selection... Registering operators.io... Registering operators.editing... TypeError: EnumProperty(..., default='mechanism'): not found in enum members ValueError: bpy_struct \"PHOBOS_OT_define_submodel\" registration error: 'submodeltype' EnumProperty could not register (see previous error) Registering operators.naming... Registering operators.misc... Registering phobosgui... ... successful. +-- Collada Import parameters------ | input file : ./objects/meshes/eur-pallet.dae | use units : no | autoconnect : yes +-- Armature Import parameters ---- | find bone chains: yes | min chain len : 0 | fix orientation : yes | keep bind info : no | IOR of negative value is not allowed for materials (using Blender default value instead)+-- Import Scene -------- | NODE id='node0', name='node0' +---------------------------------- | Collada Import : OK +---------------------------------- Error in sys.excepthook: Original exception was: File \"[HOME]/.config/blender/3.0/scripts/addons/phobos/blender/operators/editing.py\", line 1048, in toggleVisual Error in sys.excepthook: Original exception was: File \"[HOME]/.config/blender/3.0/scripts/addons/phobos/blender/operators/editing.py\", line 1051, in toggleCollision [20231211_08:52:53] WARNING No text file README.md found. (phobos/blender/utils/blender.py - readTextFile (l259)) Collada export to: [PATH]simulation/raw_models/phobos_out/unnamed/meshes/dae/Body.dae Info: Exported 1 Object Info: Exported 1 Object Collada export to: [PATH]simulation/raw_models/phobos_out/unnamed/meshes/dae/Body.dae Info: Exported 1 Object Info: Exported 1 Object Info: Phobos exported to: phobos_out/unnamed Info: Export successful. Info: Phobos exported to: phobos_out/unnamed Info: Export successful. ---------------------------------------------------------------------------------------------------- Unregistering Phobos... Unregistering phobosgui... Unregistering display... Unregistering icons... Unregistering classes... Unregistering manuals... ... successful. Blender quit Error: Not freed memory blocks: 14, total unfreed memory 0.002426 MB","title":"In GUI"},{"location":"simulation/raw_models/objects/","text":"Blueprint This document contains explanations and motivations for the measurements as well as names for variables that should be used when designing the objects. The categories are mostly self-explanatory; the one that might need more of an explanation is the color red. This refers to loose objects such as pallets, barrels, boxes, and things that will be placed directly on the ground. Measurements: Category Part / Object Variable Name Value [unit] Shelf Load Beam beam_length 2.7 [m] Shelf Load Beam beam_depth 0.05 [m] Shelf Load Beam beam_height 0.14 [m] Shelf Load Beam beam_weight 15.2 [kg] Shelf Footplate footplate_length 0.111 [m] Shelf Footplate footplate_width 0.1 [m] Shelf Footplate footplate_height 0.004 [m] Shelf Stand / Pole stand_length 0.07 [m] Shelf Stand / Pole stand_width 0.08 [m] Shelf Stand / Pole stand_height 3 [m] Shelf Stand / Pole stand_weight 8.5 [kg] Shelf Cross Brace brace_length 0.96 [m] Shelf Cross Brace brace_width 0.01 [m] Shelf Cross Brace brace_height 0.02 [m] Shelf Cross Brace brace_weight 1 [kg] Beam Robot Arm Beam robot_arm_beam_height 1.6 [m] Beam Robot Arm Beam robot_arm_beam_length 2.6 [m] Beam Robot Arm Beam robot_arm_beam_width 0.2 [m] Loose Object EUR-Pallet EUR_pallet_length 1.2 [m] Loose Object EUR-Pallet EUR_pallet_width 0.8 [m] Loose Object EUR-Pallet EUR_pallet_height 0.144 [m] Loose Object EUR-Pallet EUR_pallet_weight 25 [kg] Loose Object Steel Drum steel_drum_radius 0.3 [m] Loose Object Steel Drum steel_drum_height 0.9 [m] Loose Object Steel Drum full_steel_drum_weight 188 [kg] Loose Object Steel Drum empty_steel_drum_weight 15 [kg] Loose Object Traffic Cone traffic_cone_radius 0.22 [m] Loose Object Traffic Cone traffic_cone_angle -10 [degrees] Loose Object Traffic Cone traffic_cone_total_height 1 [m] Loose Object Traffic Cone traffic_cone_base_length 0.52 [m] Loose Object Traffic Cone traffic_cone_base_width 0.52 [m] Loose Object Traffic Cone traffic_cone_base_height 0.03 [m] Loose Object Traffic Cone traffic_cone_base_angle -20 [degrees] Loose Object Traffic Cone traffic_cone_weight 6.5 [kg] Loose Object Box box_length Varies [m] Loose Object Box box_width Varies [m] Loose Object Box box_height Varies [m] Loose Object Box box_weight Varies [m] Warehouse Support Pole pole_length 0.3 [m] Warehouse Support Pole pole_width 0.3 [m] Warehouse Support Pole pole_hole_side_length 0.27 [m] Warehouse Support Pole pole_hole_side_width 0.13 [m] Warehouse Support Pole pole_height 6 [m] Warehouse Support Pole pole_weight Undefined [kg] EUR-pallet Measurements AMR It is defined in the XY plane, and the center of the AMR is on x,y=0,0, and the bottom of the robot is on z=0. Define scale , pos , and collision in these files: We write our AMR description in simlan_gazebo_environment/urdf and use xacro to create a urdf file with gazebo tags (so not a pure urdf file) that can be used both by state_publisher and gazebo (unlike 2 separate files: model.sdf that is used for Gazebo and turtlebot.urf that is used for state_publisher in the original Turtlebot_simulation git project). There are two actual diffdrive wheels that are named left and right and four supporting wheels to balance the robot that are named front_left , front_right , back_left , and back_right (they have no friction and can be moved to different positions) with the radius of 0.98 of the main wheels. Note : The reason for the shake is the difference between the radius of the real wheels and caster wheels. Orientation: x: forward y: left z: up Objects Modeled in FreeCAD All the objects are located in objects . Below follows a list of the objects currently available, models created in FreeCAD. EUR-pallet Shelf Modular Shelf - a stackable shelf part that can create shelves of varying sizes Steel Drum Traffic Cone Support Pole Boxes with measurements in mm [Box-Length x Width x Height] Box-160x130x70 Box-185x185x75 Box-185x185x185 Box-210x180x130 Box-230x160x85 Box-250x195x160 Box-430x250x260 Box-440x320x175 Box-570x380x380 Box-1185x785x1010 Box-600x800x400 (2 per level on EUR-Pallet) Box-600x400x400 (4 per level on EUR-Pallet) Box-300x400x400 (8 per level on EUR-pallet) AMR camera Inspired by CCTV Camera 3G-SDI In the model: camera base height (70mm) + camera lens height (50mm) = 120mm Resources Pallet Rack Specification & Configuration Guide Pallet Rack Estimator Pallet rack configurator pallet Shelf typical measurements: Shelf Measurements Forklift height which influences Shelf height: Forklift Height EUR-pallet: EUR-pallet Shelf design: Shelf design Shelf exact measurements: Shelf exact measurement Examples of boxes: Boxes Examples Measurement for steel drum: Steel Drum Full Steel drum weight is based on the oil density of 825kg/m^3 and the drum fits 210 liters. Some values helping in constructing the traffic cone: Traffic Cone","title":"Object Modeling"},{"location":"simulation/raw_models/objects/#blueprint","text":"This document contains explanations and motivations for the measurements as well as names for variables that should be used when designing the objects. The categories are mostly self-explanatory; the one that might need more of an explanation is the color red. This refers to loose objects such as pallets, barrels, boxes, and things that will be placed directly on the ground. Measurements: Category Part / Object Variable Name Value [unit] Shelf Load Beam beam_length 2.7 [m] Shelf Load Beam beam_depth 0.05 [m] Shelf Load Beam beam_height 0.14 [m] Shelf Load Beam beam_weight 15.2 [kg] Shelf Footplate footplate_length 0.111 [m] Shelf Footplate footplate_width 0.1 [m] Shelf Footplate footplate_height 0.004 [m] Shelf Stand / Pole stand_length 0.07 [m] Shelf Stand / Pole stand_width 0.08 [m] Shelf Stand / Pole stand_height 3 [m] Shelf Stand / Pole stand_weight 8.5 [kg] Shelf Cross Brace brace_length 0.96 [m] Shelf Cross Brace brace_width 0.01 [m] Shelf Cross Brace brace_height 0.02 [m] Shelf Cross Brace brace_weight 1 [kg] Beam Robot Arm Beam robot_arm_beam_height 1.6 [m] Beam Robot Arm Beam robot_arm_beam_length 2.6 [m] Beam Robot Arm Beam robot_arm_beam_width 0.2 [m] Loose Object EUR-Pallet EUR_pallet_length 1.2 [m] Loose Object EUR-Pallet EUR_pallet_width 0.8 [m] Loose Object EUR-Pallet EUR_pallet_height 0.144 [m] Loose Object EUR-Pallet EUR_pallet_weight 25 [kg] Loose Object Steel Drum steel_drum_radius 0.3 [m] Loose Object Steel Drum steel_drum_height 0.9 [m] Loose Object Steel Drum full_steel_drum_weight 188 [kg] Loose Object Steel Drum empty_steel_drum_weight 15 [kg] Loose Object Traffic Cone traffic_cone_radius 0.22 [m] Loose Object Traffic Cone traffic_cone_angle -10 [degrees] Loose Object Traffic Cone traffic_cone_total_height 1 [m] Loose Object Traffic Cone traffic_cone_base_length 0.52 [m] Loose Object Traffic Cone traffic_cone_base_width 0.52 [m] Loose Object Traffic Cone traffic_cone_base_height 0.03 [m] Loose Object Traffic Cone traffic_cone_base_angle -20 [degrees] Loose Object Traffic Cone traffic_cone_weight 6.5 [kg] Loose Object Box box_length Varies [m] Loose Object Box box_width Varies [m] Loose Object Box box_height Varies [m] Loose Object Box box_weight Varies [m] Warehouse Support Pole pole_length 0.3 [m] Warehouse Support Pole pole_width 0.3 [m] Warehouse Support Pole pole_hole_side_length 0.27 [m] Warehouse Support Pole pole_hole_side_width 0.13 [m] Warehouse Support Pole pole_height 6 [m] Warehouse Support Pole pole_weight Undefined [kg] EUR-pallet Measurements","title":"Blueprint"},{"location":"simulation/raw_models/objects/#amr","text":"It is defined in the XY plane, and the center of the AMR is on x,y=0,0, and the bottom of the robot is on z=0. Define scale , pos , and collision in these files: We write our AMR description in simlan_gazebo_environment/urdf and use xacro to create a urdf file with gazebo tags (so not a pure urdf file) that can be used both by state_publisher and gazebo (unlike 2 separate files: model.sdf that is used for Gazebo and turtlebot.urf that is used for state_publisher in the original Turtlebot_simulation git project). There are two actual diffdrive wheels that are named left and right and four supporting wheels to balance the robot that are named front_left , front_right , back_left , and back_right (they have no friction and can be moved to different positions) with the radius of 0.98 of the main wheels. Note : The reason for the shake is the difference between the radius of the real wheels and caster wheels. Orientation: x: forward y: left z: up","title":"AMR"},{"location":"simulation/raw_models/objects/#objects-modeled-in-freecad","text":"All the objects are located in objects . Below follows a list of the objects currently available, models created in FreeCAD. EUR-pallet Shelf Modular Shelf - a stackable shelf part that can create shelves of varying sizes Steel Drum Traffic Cone Support Pole Boxes with measurements in mm [Box-Length x Width x Height] Box-160x130x70 Box-185x185x75 Box-185x185x185 Box-210x180x130 Box-230x160x85 Box-250x195x160 Box-430x250x260 Box-440x320x175 Box-570x380x380 Box-1185x785x1010 Box-600x800x400 (2 per level on EUR-Pallet) Box-600x400x400 (4 per level on EUR-Pallet) Box-300x400x400 (8 per level on EUR-pallet)","title":"Objects Modeled in FreeCAD"},{"location":"simulation/raw_models/objects/#amr-camera","text":"Inspired by CCTV Camera 3G-SDI In the model: camera base height (70mm) + camera lens height (50mm) = 120mm","title":"AMR camera"},{"location":"simulation/raw_models/objects/#resources","text":"Pallet Rack Specification & Configuration Guide Pallet Rack Estimator Pallet rack configurator pallet Shelf typical measurements: Shelf Measurements Forklift height which influences Shelf height: Forklift Height EUR-pallet: EUR-pallet Shelf design: Shelf design Shelf exact measurements: Shelf exact measurement Examples of boxes: Boxes Examples Measurement for steel drum: Steel Drum Full Steel drum weight is based on the oil density of 825kg/m^3 and the drum fits 210 liters. Some values helping in constructing the traffic cone: Traffic Cone","title":"Resources"},{"location":"simulation/raw_models/warehouse/","text":"Blueprint This document contains explanations and motivations for the measurements, as well as names for variables that should be used when designing the objects. The categories are mostly self-explanatory; the one that might need more of an explanation is the color red. This refers to loose objects such as pallets, barrels, boxes, and things that will be placed directly on the ground. Measurements: Category Part / Object Variable Name Value [unit] Warehouse Walls warehouse_length 23.2 [m] Warehouse Walls warehouse_width 18 [m] Warehouse Walls warehouse_height 6 [m] Warehouse Aisle aisle_width 3.7 [m] Warehouse Door door_height 4 [m] Warehouse Door door_width 3 [m] Warehouse Warehouse door_distance_to_wall 2 [m] Warehouse Shelf shelf_distance_side_side 0 [m] Warehouse Shelf single_shelf_wall_distance 0.1 [m] Warehouse Shelf double_shelf_depth_distance 0.2 [m] Warehouse Shelf shelf_total_length 2.88 [m] Warehouse Shelf shelf_total_depth 1.1 [m] Warehouse Shelf shelf_total_height 3 [m] The storage blueprint Install instruction for BIM and Arch workbench The warehouse was created in FreeCAD's BIM Workbench. This workbench isn't available by default but can easily be acquired by: Installing pip3 install gitpython Activating Tools -> Addon Manager -> BIM -> Install/update Selected Using the measurements found in the blueprint in the documentation folder, I drew four lines and turned them into walls (line tool and wall tool respectively). By default, the wall will be created around the line, so that the line is in the middle of the wall. This can be changed in the wall properties so that it ends up entirely on one side of the line. As the blueprint lacked walls, I used the dimensions specified there as the inner measurements, meaning that the origin point is located at the inner bottom left corner of the wall. All the inner space is in positive x and y coordinates, while two of the walls are in negative coordinate space. I used the Aligned Dimension tool from the Annotation tools to confirm that the inner measurements matched those of the blueprint. While the BIM Workbench has support for door objects, they tend to act as solids when imported into Gazebo, so the door is just a hole in the wall. This hole was created by adding a cube object ( 3D/BIM -> Cube ) and having it intersect the wall where the door should be located. Then you select the cube and the wall (in that order) in the tree view and press Modify -> Remove Component . This should remove the cube object and create a hole where the cube and wall overlapped. The hole didn't appear in the right place, but it was possible to edit the position of the hole in its properties. I measured the location of the hole using Aligned Dimension to make sure it ended up in the right spot. Going to the top view, I created a new rectangle object covering the whole warehouse. Then I turned it into a slab with the slab tool to create a floor for the warehouse. Everything was added to a level object (note: by default, this level object is named \"Floor,\" but don't confuse it with the slab that constitutes the physical floor) so that it's grouped together. If we set the wall heights to 0, they will automatically inherit the height of the Level object, so we can change all the walls easily by changing the height of the level. Textures for the floor and walls can later be added in Blender. Finally, I exported the project as a Collada file (.dae) and added it into a simple .world file to see if it loaded properly in Gazebo, and the results seemed correct. Since the warehouse will be static, we shouldn't need to define any additional parameters like mass or inertia; visuals and collision should be enough. Textures might need some improvement, as currently they're just basic colors, but it should be possible to add those in FreeCAD and have them included in the .dae file so we can load them visually in Gazebo later. I also added windows for slightly better visibility. Add warehouse to world-file The floor of the warehouse goes below z=0, so the ground plane was lowered by 0.2 so that the warehouse still rests on top of it. As our simulations will mainly take place inside the warehouse, the warehouse floor replaces the ground plane at z=0. The warehouse itself was placed in the models directory and loaded into the world with an <include> tag. Additionally, the maze in the stage4 world was moved away from the origin so that it is fully contained inside the warehouse, though in the future it should be removed altogether. Resources Warehouse Aisle width: Aisle Width","title":"Warehouse Specification"},{"location":"simulation/raw_models/warehouse/#blueprint","text":"This document contains explanations and motivations for the measurements, as well as names for variables that should be used when designing the objects. The categories are mostly self-explanatory; the one that might need more of an explanation is the color red. This refers to loose objects such as pallets, barrels, boxes, and things that will be placed directly on the ground. Measurements: Category Part / Object Variable Name Value [unit] Warehouse Walls warehouse_length 23.2 [m] Warehouse Walls warehouse_width 18 [m] Warehouse Walls warehouse_height 6 [m] Warehouse Aisle aisle_width 3.7 [m] Warehouse Door door_height 4 [m] Warehouse Door door_width 3 [m] Warehouse Warehouse door_distance_to_wall 2 [m] Warehouse Shelf shelf_distance_side_side 0 [m] Warehouse Shelf single_shelf_wall_distance 0.1 [m] Warehouse Shelf double_shelf_depth_distance 0.2 [m] Warehouse Shelf shelf_total_length 2.88 [m] Warehouse Shelf shelf_total_depth 1.1 [m] Warehouse Shelf shelf_total_height 3 [m] The storage blueprint","title":"Blueprint"},{"location":"simulation/raw_models/warehouse/#install-instruction-for-bim-and-arch-workbench","text":"The warehouse was created in FreeCAD's BIM Workbench. This workbench isn't available by default but can easily be acquired by: Installing pip3 install gitpython Activating Tools -> Addon Manager -> BIM -> Install/update Selected Using the measurements found in the blueprint in the documentation folder, I drew four lines and turned them into walls (line tool and wall tool respectively). By default, the wall will be created around the line, so that the line is in the middle of the wall. This can be changed in the wall properties so that it ends up entirely on one side of the line. As the blueprint lacked walls, I used the dimensions specified there as the inner measurements, meaning that the origin point is located at the inner bottom left corner of the wall. All the inner space is in positive x and y coordinates, while two of the walls are in negative coordinate space. I used the Aligned Dimension tool from the Annotation tools to confirm that the inner measurements matched those of the blueprint. While the BIM Workbench has support for door objects, they tend to act as solids when imported into Gazebo, so the door is just a hole in the wall. This hole was created by adding a cube object ( 3D/BIM -> Cube ) and having it intersect the wall where the door should be located. Then you select the cube and the wall (in that order) in the tree view and press Modify -> Remove Component . This should remove the cube object and create a hole where the cube and wall overlapped. The hole didn't appear in the right place, but it was possible to edit the position of the hole in its properties. I measured the location of the hole using Aligned Dimension to make sure it ended up in the right spot. Going to the top view, I created a new rectangle object covering the whole warehouse. Then I turned it into a slab with the slab tool to create a floor for the warehouse. Everything was added to a level object (note: by default, this level object is named \"Floor,\" but don't confuse it with the slab that constitutes the physical floor) so that it's grouped together. If we set the wall heights to 0, they will automatically inherit the height of the Level object, so we can change all the walls easily by changing the height of the level. Textures for the floor and walls can later be added in Blender. Finally, I exported the project as a Collada file (.dae) and added it into a simple .world file to see if it loaded properly in Gazebo, and the results seemed correct. Since the warehouse will be static, we shouldn't need to define any additional parameters like mass or inertia; visuals and collision should be enough. Textures might need some improvement, as currently they're just basic colors, but it should be possible to add those in FreeCAD and have them included in the .dae file so we can load them visually in Gazebo later. I also added windows for slightly better visibility.","title":"Install instruction for BIM and Arch workbench"},{"location":"simulation/raw_models/warehouse/#add-warehouse-to-world-file","text":"The floor of the warehouse goes below z=0, so the ground plane was lowered by 0.2 so that the warehouse still rests on top of it. As our simulations will mainly take place inside the warehouse, the warehouse floor replaces the ground plane at z=0. The warehouse itself was placed in the models directory and loaded into the world with an <include> tag. Additionally, the maze in the stage4 world was moved away from the origin so that it is fully contained inside the warehouse, though in the future it should be removed altogether.","title":"Add warehouse to world-file"},{"location":"simulation/raw_models/warehouse/#resources","text":"Warehouse Aisle width: Aisle Width","title":"Resources"},{"location":"simulation/replay_data/","text":"Replay Data Place trajectory .json in the same directory as this README and stitched images in the images directory. For more comprehensive instructions see the README.md in the visualize_real_data package.","title":"Replay Data"},{"location":"simulation/replay_data/#replay-data","text":"Place trajectory .json in the same directory as this README and stitched images in the images directory. For more comprehensive instructions see the README.md in the visualize_real_data package.","title":"Replay Data"},{"location":"simulation/robot_arm_moveit2/","text":"Moveit Panda robot This package contains code for a robot called \"Panda\" which includes a description/URDFs (see panda_description/ ). Inside the panda_moveit_config/ exist all code and config files that allow you to spawn a panda robot in Gazebo and be able to plan and execute motions. Inside config/ there are several *.yaml files which act as configurations, and many *.srdf, *.urdf, *.xacro files. These are all related to the same panda robot. The main description file is panda.urdf.xacro . To run our package, make sure that the Gazebo simulator is running. Then we can run the panda_moveit_config/launch/demo.launch.py , which will do the following: Spawn a panda robot in Gazebo and publish its robot_description . Start RViz with a preset MoveIt config. Start the move_group_node , which handles all planning and executing of motions. Load all controllers for panda so that it can be controlled in Gazebo. UR10 resources The UR10 setup lives under moveit_resources/ ur_description/ : UR family URDFs, meshes, and parameter YAMLs. ur_description/urdf/ur.urdf.xacro and ur_description/urdf/ur_macro.xacro are the main description entry points. ur_description/config/ur10/*.yaml holds UR10-specific kinematics, limits, and visuals. ur_moveit_config/ : MoveIt2 configuration, controllers, and launch files for UR10. ur_moveit_config/config/ur10.urdf.xacro is the MoveIt/Gazebo description (includes Robotiq). ur_moveit_config/config/ros2_controllers.yaml and ur_moveit_config/config/gazebo_moveit_controllers.yaml define controllers. ur_moveit_config/srdf/ur.srdf.xacro contains the SRDF groups and gripper setup. ur_moveit_config/launch/demo.launch.py is the main entry point for spawning + MoveIt. robotiq_description/ : Robotiq 2F-140 gripper description and meshes. robotiq_description/urdf/robotiq_2f_140_macro.urdf.xacro is the main gripper macro used by UR10. UR10 quickstart Start Gazebo in a separate terminal: ./control.sh sim Spawn the UR10 + MoveIt stack (after Gazebo is already running): ./control.sh ur10 Launch the UR10 pick-and-place script (after the UR10 stack is up): ros2 launch motion_planning_python_api ur10_pick_and_place.launch.py Moveit motion planner using Python API The folder custom_motion_planning_python_api has scripting files that plan and execute motions for a panda robot. It uses the official MoveIt2 Python API to achieve this. In this package's scripts/ folder exist these Python files. Look at the demo scripts for inspiration. To run any of the scripts, you run the launch file: motion_planning_python_api_planning_scene.py is an original demo from MoveIt2. motion_planning_python_api_tutorial.py is an original demo from MoveIt2. demo_pick_and_place.py is custom made. The moveit2_py package is not currently available for Humble, but Jazzy supports it. The prerequisites to run a motion planner script are to run these in parallel terminals: gazebo sim & panda_moveit_config/launch/demo.launch.py . When these two are finished initializing, run the motion script with: motion_planning_python_api_tutorial.launch.py","title":"Panda MoveIt2"},{"location":"simulation/robot_arm_moveit2/#moveit-panda-robot","text":"This package contains code for a robot called \"Panda\" which includes a description/URDFs (see panda_description/ ). Inside the panda_moveit_config/ exist all code and config files that allow you to spawn a panda robot in Gazebo and be able to plan and execute motions. Inside config/ there are several *.yaml files which act as configurations, and many *.srdf, *.urdf, *.xacro files. These are all related to the same panda robot. The main description file is panda.urdf.xacro . To run our package, make sure that the Gazebo simulator is running. Then we can run the panda_moveit_config/launch/demo.launch.py , which will do the following: Spawn a panda robot in Gazebo and publish its robot_description . Start RViz with a preset MoveIt config. Start the move_group_node , which handles all planning and executing of motions. Load all controllers for panda so that it can be controlled in Gazebo.","title":"Moveit Panda robot"},{"location":"simulation/robot_arm_moveit2/#ur10-resources","text":"The UR10 setup lives under moveit_resources/ ur_description/ : UR family URDFs, meshes, and parameter YAMLs. ur_description/urdf/ur.urdf.xacro and ur_description/urdf/ur_macro.xacro are the main description entry points. ur_description/config/ur10/*.yaml holds UR10-specific kinematics, limits, and visuals. ur_moveit_config/ : MoveIt2 configuration, controllers, and launch files for UR10. ur_moveit_config/config/ur10.urdf.xacro is the MoveIt/Gazebo description (includes Robotiq). ur_moveit_config/config/ros2_controllers.yaml and ur_moveit_config/config/gazebo_moveit_controllers.yaml define controllers. ur_moveit_config/srdf/ur.srdf.xacro contains the SRDF groups and gripper setup. ur_moveit_config/launch/demo.launch.py is the main entry point for spawning + MoveIt. robotiq_description/ : Robotiq 2F-140 gripper description and meshes. robotiq_description/urdf/robotiq_2f_140_macro.urdf.xacro is the main gripper macro used by UR10.","title":"UR10 resources"},{"location":"simulation/robot_arm_moveit2/#ur10-quickstart","text":"Start Gazebo in a separate terminal: ./control.sh sim Spawn the UR10 + MoveIt stack (after Gazebo is already running): ./control.sh ur10 Launch the UR10 pick-and-place script (after the UR10 stack is up): ros2 launch motion_planning_python_api ur10_pick_and_place.launch.py","title":"UR10 quickstart"},{"location":"simulation/robot_arm_moveit2/#moveit-motion-planner-using-python-api","text":"The folder custom_motion_planning_python_api has scripting files that plan and execute motions for a panda robot. It uses the official MoveIt2 Python API to achieve this. In this package's scripts/ folder exist these Python files. Look at the demo scripts for inspiration. To run any of the scripts, you run the launch file: motion_planning_python_api_planning_scene.py is an original demo from MoveIt2. motion_planning_python_api_tutorial.py is an original demo from MoveIt2. demo_pick_and_place.py is custom made. The moveit2_py package is not currently available for Humble, but Jazzy supports it. The prerequisites to run a motion planner script are to run these in parallel terminals: gazebo sim & panda_moveit_config/launch/demo.launch.py . When these two are finished initializing, run the motion script with: motion_planning_python_api_tutorial.launch.py","title":"Moveit motion planner using Python API"},{"location":"simulation/robot_arm_moveit2/moveit_resources/","text":"MoveIt Resources This repository includes various resources (URDFs, meshes, moveit_config packages) needed for MoveIt testing. GitHub Actions: Included Robots PR2 Fanuc M-10iA Franka Emika Panda UR10 Notes The benchmarking resources have been moved to https://github.com/ros-planning/moveit_benchmark_resources.","title":"MoveIt Resources"},{"location":"simulation/robot_arm_moveit2/moveit_resources/#moveit-resources","text":"This repository includes various resources (URDFs, meshes, moveit_config packages) needed for MoveIt testing. GitHub Actions:","title":"MoveIt Resources"},{"location":"simulation/robot_arm_moveit2/moveit_resources/#included-robots","text":"PR2 Fanuc M-10iA Franka Emika Panda UR10","title":"Included Robots"},{"location":"simulation/robot_arm_moveit2/moveit_resources/#notes","text":"The benchmarking resources have been moved to https://github.com/ros-planning/moveit_benchmark_resources.","title":"Notes"},{"location":"simulation/robot_arm_moveit2/moveit_resources/panda_description/","text":"panda_description Note: This package contains a panda.urdf and a newer panda.urdf.xacro. The XACRO has been created to support finding package resource files dynamically which is needed for Gazebo. The URDF is still needed by RobotModelTestUtils which doesn't support xacro yet. The URDF model and meshes contained in this package were copied from the frankaemika franka_ros package and adapted for use with moveit_resources . All imported files were released under the Apache-2.0 license.","title":"panda_description"},{"location":"simulation/robot_arm_moveit2/moveit_resources/panda_description/#panda_description","text":"Note: This package contains a panda.urdf and a newer panda.urdf.xacro. The XACRO has been created to support finding package resource files dynamically which is needed for Gazebo. The URDF is still needed by RobotModelTestUtils which doesn't support xacro yet. The URDF model and meshes contained in this package were copied from the frankaemika franka_ros package and adapted for use with moveit_resources . All imported files were released under the Apache-2.0 license.","title":"panda_description"},{"location":"simulation/robot_arm_moveit2/moveit_resources/panda_moveit_config/","text":"MoveIt Resources for testing: Franka Emika Panda A project-internal moveit configuration for testing in MoveIt. Use the official panda_moveit_config if you actually want to work with the robot!","title":"Index"},{"location":"simulation/robot_arm_moveit2/moveit_resources/panda_moveit_config/#moveit-resources-for-testing-franka-emika-panda","text":"A project-internal moveit configuration for testing in MoveIt. Use the official panda_moveit_config if you actually want to work with the robot!","title":"MoveIt Resources for testing: Franka Emika Panda"},{"location":"simulation/scenario_manager/","text":"Scenario Manager The scenario_manager package is a ROS2 package designed to manage and execute various robot scenarios involving teleportation, speed setting, and collision simulations. It provides action servers to control robot behavior and a tool for calculating Time to Collision (TTC). Launching the Scenario Manager To start the scenario_manager , launch it using the following command: ros2 launch scenario_manager scenario_manager.launch.py This initializes three action servers: - teleport_action_server : Handles teleportation of robots. - set_speed_action_server : Controls robot speed. - collision_action_server : Configures scenarios where robots collide (only applicable to jackal and pallet_truck ). Collision Action Server The collision_action_server configures collisions by setting speeds and initial positions for two robots to ensure a collision occurs. It requires: - Angle : The approach angle. - Speed of pallet_truck : The movement speed of the pallet truck. - Collision Type : An enum from the simlan_custom_msg package specifying the type of collision. Collision Types There are three predefined collision types: Collision Type Enum Value Description HEAD_ON 0 Robots collide head-on. PALLET_TRUCK_SIDE 1 Pallet truck collides into the side of Jackal. JACKAL_SIDE 2 Jackal collides into the side of the pallet truck. Below are images illustrating the different collision types: Head-On Collision Pallet Truck Side Collision Jackal Side Collision Running a Scenario To execute a scenario, use the following command: ros2 launch scenario_execution_ros scenario_launch.py scenario:=<scenario_file> Replace <scenario_file> with a specific scenario file, such as: ros2 launch scenario_execution_ros scenario_launch.py scenario:=simulation/scenario_manager/scenarios/case1.osc or scenario 6 can be run from the scenarios.sh file like so: ./scenarios.sh 6 This runs a collision action client, executing multiple collision simulations with varying angles and speeds. Note: If the robots are not moving or something feels like it is not working, there can be ghost processes from an earlier run. Stop the program and run ./control.sh kill and try running again. Time to Collision (TTC) Calculation The package includes a TTC node that logs the Time to Collision (TTC) and Closest Point of Arrival (CPA) for two robots using the Minkowski-Difference, assuming constant speed and direction. Running the TTC Node ros2 run scenario_manager ttc This node calculates and logs: - \u23f1\ufe0f TTC : The time at which the closest approach occurs. - \ud83d\udccd CPA : The closest distance between the two robots, assuming constant speed and trajectory.","title":"Scenario Manager"},{"location":"simulation/scenario_manager/#scenario-manager","text":"The scenario_manager package is a ROS2 package designed to manage and execute various robot scenarios involving teleportation, speed setting, and collision simulations. It provides action servers to control robot behavior and a tool for calculating Time to Collision (TTC).","title":"Scenario Manager"},{"location":"simulation/scenario_manager/#launching-the-scenario-manager","text":"To start the scenario_manager , launch it using the following command: ros2 launch scenario_manager scenario_manager.launch.py This initializes three action servers: - teleport_action_server : Handles teleportation of robots. - set_speed_action_server : Controls robot speed. - collision_action_server : Configures scenarios where robots collide (only applicable to jackal and pallet_truck ).","title":"Launching the Scenario Manager"},{"location":"simulation/scenario_manager/#collision-action-server","text":"The collision_action_server configures collisions by setting speeds and initial positions for two robots to ensure a collision occurs. It requires: - Angle : The approach angle. - Speed of pallet_truck : The movement speed of the pallet truck. - Collision Type : An enum from the simlan_custom_msg package specifying the type of collision.","title":"Collision Action Server"},{"location":"simulation/scenario_manager/#collision-types","text":"There are three predefined collision types: Collision Type Enum Value Description HEAD_ON 0 Robots collide head-on. PALLET_TRUCK_SIDE 1 Pallet truck collides into the side of Jackal. JACKAL_SIDE 2 Jackal collides into the side of the pallet truck. Below are images illustrating the different collision types: Head-On Collision Pallet Truck Side Collision Jackal Side Collision","title":"Collision Types"},{"location":"simulation/scenario_manager/#running-a-scenario","text":"To execute a scenario, use the following command: ros2 launch scenario_execution_ros scenario_launch.py scenario:=<scenario_file> Replace <scenario_file> with a specific scenario file, such as: ros2 launch scenario_execution_ros scenario_launch.py scenario:=simulation/scenario_manager/scenarios/case1.osc or scenario 6 can be run from the scenarios.sh file like so: ./scenarios.sh 6 This runs a collision action client, executing multiple collision simulations with varying angles and speeds. Note: If the robots are not moving or something feels like it is not working, there can be ghost processes from an earlier run. Stop the program and run ./control.sh kill and try running again.","title":"Running a Scenario"},{"location":"simulation/scenario_manager/#time-to-collision-ttc-calculation","text":"The package includes a TTC node that logs the Time to Collision (TTC) and Closest Point of Arrival (CPA) for two robots using the Minkowski-Difference, assuming constant speed and direction.","title":"Time to Collision (TTC) Calculation"},{"location":"simulation/scenario_manager/#running-the-ttc-node","text":"ros2 run scenario_manager ttc This node calculates and logs: - \u23f1\ufe0f TTC : The time at which the closest approach occurs. - \ud83d\udccd CPA : The closest distance between the two robots, assuming constant speed and trajectory.","title":"Running the TTC Node"},{"location":"simulation/simlan_bringup/","text":"SIMLAN bringup This package is responsible for launching all things related to the simulation. The launch files in the package call launch files in all other packages to start everything up with the following command ros2 launch simlan_bringup full_sim.launch.py This is also called in the sim operation in control.sh: ./control.sh sim Launch arguments There are different launch arguments that can be changed to launch the simulation in different states. These can either be called from the terminal by appending the argument to the launch command above. rviz:='True' Another way to edit what is launched is to change the default value in full_sim.launch.py . launch_rviz_launch_argument = DeclareLaunchArgument(\"rviz\", default_value=\"False\", description=\"To launch rviz\") The launch arguments are then either added as a condition straight to a Node: condition=IfCondition(rviz) or passed downward to the launch file being called from the top-level launch file: launch_arguments={\"jackal_manual_control\":jackal_manual_control,}.items() If you use a specific argument configuration often, it is best to create a new launch file, on top of full_sim.launch.py . It can have the correct default values for all arguments and then just call on full_sim.launch.py . Diagram of launch structure This diagram is made in DrawIO and the PNG contains the XML code. Drop it into draw.io to make changes and add back to this readme.","title":"SIMLAN Bringup"},{"location":"simulation/simlan_bringup/#simlan-bringup","text":"This package is responsible for launching all things related to the simulation. The launch files in the package call launch files in all other packages to start everything up with the following command ros2 launch simlan_bringup full_sim.launch.py This is also called in the sim operation in control.sh: ./control.sh sim","title":"SIMLAN bringup"},{"location":"simulation/simlan_bringup/#launch-arguments","text":"There are different launch arguments that can be changed to launch the simulation in different states. These can either be called from the terminal by appending the argument to the launch command above. rviz:='True' Another way to edit what is launched is to change the default value in full_sim.launch.py . launch_rviz_launch_argument = DeclareLaunchArgument(\"rviz\", default_value=\"False\", description=\"To launch rviz\") The launch arguments are then either added as a condition straight to a Node: condition=IfCondition(rviz) or passed downward to the launch file being called from the top-level launch file: launch_arguments={\"jackal_manual_control\":jackal_manual_control,}.items() If you use a specific argument configuration often, it is best to create a new launch file, on top of full_sim.launch.py . It can have the correct default values for all arguments and then just call on full_sim.launch.py .","title":"Launch arguments"},{"location":"simulation/simlan_bringup/#diagram-of-launch-structure","text":"This diagram is made in DrawIO and the PNG contains the XML code. Drop it into draw.io to make changes and add back to this readme.","title":"Diagram of launch structure"},{"location":"simulation/simlan_gazebo_environment/models/forklift_robot/","text":"forklift_robot A simple Forklift Robot URDF and PROTO (for webots). Add it to your catkin workspace and run catkin build URDF One can see the URDF by running: roslaunch urdf_tutorial display.launch model:='$(find forklift_robot_description)/urdf/forklift_simple.urdf'","title":"forklift_robot"},{"location":"simulation/simlan_gazebo_environment/models/forklift_robot/#forklift_robot","text":"A simple Forklift Robot URDF and PROTO (for webots). Add it to your catkin workspace and run catkin build","title":"forklift_robot"},{"location":"simulation/simlan_gazebo_environment/models/forklift_robot/#urdf","text":"One can see the URDF by running: roslaunch urdf_tutorial display.launch model:='$(find forklift_robot_description)/urdf/forklift_simple.urdf'","title":"URDF"},{"location":"simulation/simlan_gazebo_environment/worlds/","text":"Gazebo Worlds <physics type=\"ode\"> <ode> <solver> <type>world</type> </solver> <constraints> <contact_max_correcting_vel>0.1</contact_max_correcting_vel> <contact_surface_layer>0.0001</contact_surface_layer> </constraints> </ode> <max_step_size>0.001</max_step_size> </physics> Two solvers: world step gives an accurate solution if it is able to solve the problem, while quick step depends on the number of iterations to reach an accurate enough solution. contact/collision parameters dampingFactor double Exponential velocity decay of the link velocity - takes the value and multiplies the - previous link velocity by (1-dampingFactor). maxVel double maximum contact correction velocity truncation term. minDepth double minimum allowable depth before contact correction impulse is applied maxContacts int Maximum number of contacts allowed between two entities. This value overrides the max_contacts element defined in physics. contact_max_correcting_vel : contact_max_correcting_vel This is the same parameter as the max_vel under collision->surface->contact. contact_max_correcting_vel sets max_vel globally. contact_surface_layer : contact_surface_layer This is the same parameter as the min_depth under collision->surface->contact. Note : We had issue getting global settings ( contact_max_correcting_vel and contact_surface_layer ) working. We therefore define object level minDepth and maxVel for each object \"quick\" solver parameters min_step_size The minimum time duration which advances with each time step of a variable time step solver. iters The number of iterations for the solver to run for each time step. objects: As of now there are many objects in the world file nad if you want to move them all it's quite cumbersome. Therefore the move_objects.py was added to be able to increment the poses by x amount if needed. See move_objects.py reference: Physics Parameters SDF format","title":"Gazebo Worlds"},{"location":"simulation/simlan_gazebo_environment/worlds/#gazebo-worlds","text":"<physics type=\"ode\"> <ode> <solver> <type>world</type> </solver> <constraints> <contact_max_correcting_vel>0.1</contact_max_correcting_vel> <contact_surface_layer>0.0001</contact_surface_layer> </constraints> </ode> <max_step_size>0.001</max_step_size> </physics> Two solvers: world step gives an accurate solution if it is able to solve the problem, while quick step depends on the number of iterations to reach an accurate enough solution.","title":"Gazebo Worlds"},{"location":"simulation/simlan_gazebo_environment/worlds/#contactcollision-parameters","text":"dampingFactor double Exponential velocity decay of the link velocity - takes the value and multiplies the - previous link velocity by (1-dampingFactor). maxVel double maximum contact correction velocity truncation term. minDepth double minimum allowable depth before contact correction impulse is applied maxContacts int Maximum number of contacts allowed between two entities. This value overrides the max_contacts element defined in physics. contact_max_correcting_vel : contact_max_correcting_vel This is the same parameter as the max_vel under collision->surface->contact. contact_max_correcting_vel sets max_vel globally. contact_surface_layer : contact_surface_layer This is the same parameter as the min_depth under collision->surface->contact. Note : We had issue getting global settings ( contact_max_correcting_vel and contact_surface_layer ) working. We therefore define object level minDepth and maxVel for each object \"quick\" solver parameters min_step_size The minimum time duration which advances with each time step of a variable time step solver. iters The number of iterations for the solver to run for each time step.","title":"contact/collision parameters"},{"location":"simulation/simlan_gazebo_environment/worlds/#objects","text":"As of now there are many objects in the world file nad if you want to move them all it's quite cumbersome. Therefore the move_objects.py was added to be able to increment the poses by x amount if needed. See move_objects.py","title":"objects:"},{"location":"simulation/simlan_gazebo_environment/worlds/#reference","text":"Physics Parameters SDF format","title":"reference:"},{"location":"simulation/static_agent_launcher/","text":"Launching GPSS cameras This package launches all the static agents, i.e., the cameras in the simulation. When the workspace is built, the camera_config.xacro file is updated with camera_ids and their corresponding intrinsic and extrinsic values. This can be seen in the build function in control.sh . The camera_config.xacro is then sent to the robot_state_publisher node and published to the /static_agent/robot_description topic. The ros_gz_sim package with the \"create\" executable then spawns the static_agents by subscribing to the /static_agent/robot_description topic and spawning each agent. gz_bridge for cameras Different configurations of the gz_bridge node were tested and evaluated against the simulated RTF (real time factor). In the earlier setups, a gz_bridge node was set up for each camera_stream (image, depth, semantic) for each camera_id. Three configurations were tested where different numbers of nodes were set up and their corresponding RTFs were captured and averaged over five tries. SN=single node for all cameras, 1NPC=1 node per camera_id, 1NPSPC = 1 node per stream per camera_id node setup number of cameras number of nodes camera streams avg RTF SN 9 1 image 17,9% 1NPC 9 9 image 16,7% SN 9 1 image depth semantic 5,14% 1NPC 9 9 image depth semantic 5,2% 1NPS 9 81 image depth semantic 4,7% It doesn't seem like a big difference, so we will stick with one node, i.e., SN. In the future it can be decided whether to keep the gz_bridge node as is or to generate a gz_bridge_camera.yaml file like the other generation scripts ( config_generation ). adding new cameras If you want to add new cameras use the jupyter notebook generate_extrensic_content.ipynb located in camera_utility . To view references of how an extrensic yaml file is structured, look at any file inside extrinsic folder . If we want to add new cameras in the same way as the existing ones we need to define an intrinsic and extrinsic yaml file with its camera name as file name, i.e. 500.yaml. To create an intrinsic yaml file, its just to copy an existing one and save the copy with the new camera name. For now, the intrinsic does not change. To create an extrinsic file you must change the \"rotation_matrix\", \"t_vec\", and \"position\". And these vary depending on where you want to place the camera and how it should be angled. To solve this, use method generate_extrinsic_complete(rpy, position) to generate a complete multi-string of the extrinsic yaml content. Easy to copy and paste into the new extrinsic file you want to create. It inputs: position : The gazebo position, actual position relative to whatever origin you are dealing with. rpy : A list containing the roll pitch yaw. This you need to calculate/experiment with yourself to find what angle you want the camera to be. Note: The angles are calculated using Radians, meaning PI=180 degrees, PI/2=90 degrees","title":"GPSS cameras"},{"location":"simulation/static_agent_launcher/#launching-gpss-cameras","text":"This package launches all the static agents, i.e., the cameras in the simulation. When the workspace is built, the camera_config.xacro file is updated with camera_ids and their corresponding intrinsic and extrinsic values. This can be seen in the build function in control.sh . The camera_config.xacro is then sent to the robot_state_publisher node and published to the /static_agent/robot_description topic. The ros_gz_sim package with the \"create\" executable then spawns the static_agents by subscribing to the /static_agent/robot_description topic and spawning each agent.","title":"Launching GPSS cameras"},{"location":"simulation/static_agent_launcher/#gz_bridge-for-cameras","text":"Different configurations of the gz_bridge node were tested and evaluated against the simulated RTF (real time factor). In the earlier setups, a gz_bridge node was set up for each camera_stream (image, depth, semantic) for each camera_id. Three configurations were tested where different numbers of nodes were set up and their corresponding RTFs were captured and averaged over five tries. SN=single node for all cameras, 1NPC=1 node per camera_id, 1NPSPC = 1 node per stream per camera_id node setup number of cameras number of nodes camera streams avg RTF SN 9 1 image 17,9% 1NPC 9 9 image 16,7% SN 9 1 image depth semantic 5,14% 1NPC 9 9 image depth semantic 5,2% 1NPS 9 81 image depth semantic 4,7% It doesn't seem like a big difference, so we will stick with one node, i.e., SN. In the future it can be decided whether to keep the gz_bridge node as is or to generate a gz_bridge_camera.yaml file like the other generation scripts ( config_generation ).","title":"gz_bridge for cameras"},{"location":"simulation/static_agent_launcher/#adding-new-cameras","text":"If you want to add new cameras use the jupyter notebook generate_extrensic_content.ipynb located in camera_utility . To view references of how an extrensic yaml file is structured, look at any file inside extrinsic folder . If we want to add new cameras in the same way as the existing ones we need to define an intrinsic and extrinsic yaml file with its camera name as file name, i.e. 500.yaml. To create an intrinsic yaml file, its just to copy an existing one and save the copy with the new camera name. For now, the intrinsic does not change. To create an extrinsic file you must change the \"rotation_matrix\", \"t_vec\", and \"position\". And these vary depending on where you want to place the camera and how it should be angled. To solve this, use method generate_extrinsic_complete(rpy, position) to generate a complete multi-string of the extrinsic yaml content. Easy to copy and paste into the new extrinsic file you want to create. It inputs: position : The gazebo position, actual position relative to whatever origin you are dealing with. rpy : A list containing the roll pitch yaw. This you need to calculate/experiment with yourself to find what angle you want the camera to be. Note: The angles are calculated using Radians, meaning PI=180 degrees, PI/2=90 degrees","title":"adding new cameras"},{"location":"simulation/visualize_real_data/","text":"Visualize Real Data Package The visualize_real_data package is designed to replay and visualize real-world trajectory data within RViz and Gazebo. This package takes recorded trajectory data (in JSON format) and corresponding images, processes them into ROS-compatible formats, and enables visualization and simulation replay in Gazebo. Preparing Data Before replaying any data, it has to be recorded. This is done by performing the steps below. Build the package First the package needs to be built, if this is not already done run: ./control.sh build Add the data Add your data files beneath the simulation/replay_data directory. Your structure should look like this: replay_data/ |\u2500\u2500 images/ | |\u2500\u2500 images | |\u2500\u2500 1770030000000.jpg #Example image 1 | |\u2500\u2500 1770030000100.jpg #Example image 2 |\u2500\u2500 example_trajectory.json # Example using image 1 and 2. |\u2500\u2500 README.md By default, the package expects to find the trajectory data inside the replay_data/ folder as .json , and the images inside the images/ folder as .jpg . The .json trajectories are expected on this form: { \"time_stamp\": 1770030000000, // Unix timestamp in milliseconds \"object_list\": [ [ 0, // Tracking ID 1, // X 2, // Y 3, // Z (most often 0) 4 // Orientation (does not have to be included, this is what is generated by orientation_faker) ], //... ] } Some things to note: The replay_data folder is gitignored, so you can safely save images there without uploading to git. The folder-names are configurable , as long as the settings in params.yaml match. You can: Rename the images/ folder to something else. Rename the trajectory JSON file to any other valid filename. Just be sure to update the corresponding fields ( images_folder , json_file_name ) in params.yaml . Config file When your data is in place, you may want to review the config parameters in params.yaml . This step is often optional, as default settings typically work well. There is currently an example trajectory file that will show an image of a dog and a cat (these would be your stitched images in practice) and with the positions from the trajectories as visualization markers. These are mostly there to check that things still work as intended without the need of external data. params.yaml Overview prepare_data section: The parameters here have to be set before prepare.launch.py is run. Changes made afterwards will not affect the preparation process (image->PointCloud2). Parameter Description Default Value pointcloud_topic Topic to publish the PointCloud2 (image data). pointcloud_topic images_folder Name of the folder containing images. images image_scale Resize factor for .jpg images (useful for RViz display). 0.04 set_frames If true , limits processing to frames_to_process ; otherwise, all available images will be used. false frames_to_process Number of frames to process if set_frames is true . 1 preprocess_all_data If true , preprocesses all available data before playback. May not work for extremely large recording windows. true fake_orientation If true , automatically fakes object orientations during data preparation (used by the scenario replayer). true list_json_files If true , lists available JSON files in the replay_data directory during preparation. false send_data section: Parameters here can be changed after the prepare-stage to alter the playback. You do not need to rebuild the package or re-run prepare.launch.py if you change something here. Parameter Description Default Value playback_rate Controls playback speed. Default is 1 (real-time based on extracted_fps ). 1 bag_name Name of the rosbag to play. If empty, the most recent one is used. (empty) scenario_replayer section: Parameter Description Default Value gazebo_teleport_service Service name for teleporting robots in Gazebo. /world/default/set_pose frame_id Frame in which entities are visualized. real_data use_cmd_vel If true , robots are driven with velocity commands; otherwise, teleportation is used to reenact the scenario. When set to true , it is important the simulation runs close to 100% real-time. false ignore_orientation If true , robot orientation is reset to identity (no rotation) regardless of source data. Using with cmd_vel may cause unexpected movement behavior. false shared section: Parameters here have to be set at the prepare-stage and can't be changed afterwards. Parameter Description Default Value frame_id Name of the frame in which all data is displayed. real_data namespace Namespace used for the node. visualize_real_data entity_topic Topic to publish the MarkerArray (trajectory data). entity_topic json_file_name Name of the file containing trajectory data. example_trajectory.json start_time Start time for data processing (optional filter). \"\" end_time End time for data processing (optional filter). \"\" initial_heading Initial heading/orientation in degrees (0 = pointing left). Used by orientation_faker. 0.0 frame_position x , y , z coordinates for visualization frame. x: 15.35, y: 5.7, z: -0.2 extracted_fps FPS of the extracted data for playback. 10.0 processing_time_limit Max time allowed per frame for consistent playback. If exceeded, a warning appears. 0.8 Launching the Processing Step Once configured, first run: ./control.sh replay_sim This starts the simulation with the correct rviz config. Then run either: ros2 launch visualize_real_data prepare.launch.py or ./control.sh prepare This will first start the orientation_faker node, which adds an orientation to the data based on the angle between points in the trajectory (this step can be ignored by changing the fake_orientation parameter). Afterwards, the data processing is started and generates a rosbag file stored in a rosbags/ folder inside the replay_data directory. NOTE: If orientation is already given, change the fake_orientation parameter inside the config to prevent it from overwriting the data. Rosbags are named using the name of the JSON file used and timestamps (e.g., my_recording_20250722_153045 if the JSON file is named my_recording.json ). Existing rosbags are never overwritten . You can delete old rosbags manually from the rosbags/ folder if needed. Sending Data After processing the data, you can send it to RViz: Make sure you\u2019ve added the following displays to RViz: PointCloud2 MarkerArray Run: ros2 launch visualize_real_data send.launch.py or ./control.sh replay_rviz This command sends the most recently created rosbag to the topics defined in params.yaml . To play an earlier dataset, you must manually delete newer rosbags as the latest one (by timestamp) is always selected automatically. Adjusting Playback Speed You can modify the playback_rate parameter in params.yaml to control how quickly the rosbag is replayed. A value of 1 reflects real-time playback based on extracted timestamps. Slower or faster playback is supported, but extreme values haven't been tested . Fixing RViz Usually the displays in RViz have to be configured to subscribe to the correct topics, and the PointCloud2 display's size has to match the image_scale defined in the params.yaml config file to look right. You can also change Alpha in the PointCloud display to see through the image. If the images aren't showing up, sometimes the QOS (Quality of Service) settings might be mismatched between the rosbag player and RViz . The intended QOS settings can be found in the recorder_qos.yaml file in the config folder together with the params.yaml file. These are the settings that the rosbag player uses, and the easiest fix is to make sure that RViz mirrors these settings for the respective displays. Summary Build package using CTRL + SHIFT + B VS Code task. Add the data you want to show in RViz in the replay_data folder Configure parameters under 'prepare_data', 'shared', and/or 'scenario_replayer' (optional) Run prepare.launch.py Open RViz2 and add the displays PointCloud2 and MarkerArray Make sure the QOS-settings match between the rosbag player and the displays Configure parameters under 'send_data' (optional) Run send.launch.py Subscribe to the relevant topics (as defined in params.yaml ) in RViz2 Now the data should be visualized. Warnings and Errors from the node Warnings and Errors thrown by this package are intended to inform you when something unexpected occurs during execution. In most cases, the process can continue without interruption, although the output may be affected. It is strongly recommended to resolve the underlying issue based on the warning/error message and then re-run the process to ensure reliable and consistent output.","title":"Visualize Real Data Package"},{"location":"simulation/visualize_real_data/#visualize-real-data-package","text":"The visualize_real_data package is designed to replay and visualize real-world trajectory data within RViz and Gazebo. This package takes recorded trajectory data (in JSON format) and corresponding images, processes them into ROS-compatible formats, and enables visualization and simulation replay in Gazebo.","title":"Visualize Real Data Package"},{"location":"simulation/visualize_real_data/#preparing-data","text":"Before replaying any data, it has to be recorded. This is done by performing the steps below.","title":"Preparing Data"},{"location":"simulation/visualize_real_data/#build-the-package","text":"First the package needs to be built, if this is not already done run: ./control.sh build","title":"Build the package"},{"location":"simulation/visualize_real_data/#add-the-data","text":"Add your data files beneath the simulation/replay_data directory. Your structure should look like this: replay_data/ |\u2500\u2500 images/ | |\u2500\u2500 images | |\u2500\u2500 1770030000000.jpg #Example image 1 | |\u2500\u2500 1770030000100.jpg #Example image 2 |\u2500\u2500 example_trajectory.json # Example using image 1 and 2. |\u2500\u2500 README.md By default, the package expects to find the trajectory data inside the replay_data/ folder as .json , and the images inside the images/ folder as .jpg . The .json trajectories are expected on this form: { \"time_stamp\": 1770030000000, // Unix timestamp in milliseconds \"object_list\": [ [ 0, // Tracking ID 1, // X 2, // Y 3, // Z (most often 0) 4 // Orientation (does not have to be included, this is what is generated by orientation_faker) ], //... ] } Some things to note: The replay_data folder is gitignored, so you can safely save images there without uploading to git. The folder-names are configurable , as long as the settings in params.yaml match. You can: Rename the images/ folder to something else. Rename the trajectory JSON file to any other valid filename. Just be sure to update the corresponding fields ( images_folder , json_file_name ) in params.yaml .","title":"Add the data"},{"location":"simulation/visualize_real_data/#config-file","text":"When your data is in place, you may want to review the config parameters in params.yaml . This step is often optional, as default settings typically work well. There is currently an example trajectory file that will show an image of a dog and a cat (these would be your stitched images in practice) and with the positions from the trajectories as visualization markers. These are mostly there to check that things still work as intended without the need of external data.","title":"Config file"},{"location":"simulation/visualize_real_data/#paramsyaml-overview","text":"","title":"params.yaml Overview"},{"location":"simulation/visualize_real_data/#prepare_data-section","text":"The parameters here have to be set before prepare.launch.py is run. Changes made afterwards will not affect the preparation process (image->PointCloud2). Parameter Description Default Value pointcloud_topic Topic to publish the PointCloud2 (image data). pointcloud_topic images_folder Name of the folder containing images. images image_scale Resize factor for .jpg images (useful for RViz display). 0.04 set_frames If true , limits processing to frames_to_process ; otherwise, all available images will be used. false frames_to_process Number of frames to process if set_frames is true . 1 preprocess_all_data If true , preprocesses all available data before playback. May not work for extremely large recording windows. true fake_orientation If true , automatically fakes object orientations during data preparation (used by the scenario replayer). true list_json_files If true , lists available JSON files in the replay_data directory during preparation. false","title":"prepare_data section:"},{"location":"simulation/visualize_real_data/#send_data-section","text":"Parameters here can be changed after the prepare-stage to alter the playback. You do not need to rebuild the package or re-run prepare.launch.py if you change something here. Parameter Description Default Value playback_rate Controls playback speed. Default is 1 (real-time based on extracted_fps ). 1 bag_name Name of the rosbag to play. If empty, the most recent one is used. (empty)","title":"send_data section:"},{"location":"simulation/visualize_real_data/#scenario_replayer-section","text":"Parameter Description Default Value gazebo_teleport_service Service name for teleporting robots in Gazebo. /world/default/set_pose frame_id Frame in which entities are visualized. real_data use_cmd_vel If true , robots are driven with velocity commands; otherwise, teleportation is used to reenact the scenario. When set to true , it is important the simulation runs close to 100% real-time. false ignore_orientation If true , robot orientation is reset to identity (no rotation) regardless of source data. Using with cmd_vel may cause unexpected movement behavior. false","title":"scenario_replayer section:"},{"location":"simulation/visualize_real_data/#shared-section","text":"Parameters here have to be set at the prepare-stage and can't be changed afterwards. Parameter Description Default Value frame_id Name of the frame in which all data is displayed. real_data namespace Namespace used for the node. visualize_real_data entity_topic Topic to publish the MarkerArray (trajectory data). entity_topic json_file_name Name of the file containing trajectory data. example_trajectory.json start_time Start time for data processing (optional filter). \"\" end_time End time for data processing (optional filter). \"\" initial_heading Initial heading/orientation in degrees (0 = pointing left). Used by orientation_faker. 0.0 frame_position x , y , z coordinates for visualization frame. x: 15.35, y: 5.7, z: -0.2 extracted_fps FPS of the extracted data for playback. 10.0 processing_time_limit Max time allowed per frame for consistent playback. If exceeded, a warning appears. 0.8","title":"shared section:"},{"location":"simulation/visualize_real_data/#launching-the-processing-step","text":"Once configured, first run: ./control.sh replay_sim This starts the simulation with the correct rviz config. Then run either: ros2 launch visualize_real_data prepare.launch.py or ./control.sh prepare This will first start the orientation_faker node, which adds an orientation to the data based on the angle between points in the trajectory (this step can be ignored by changing the fake_orientation parameter). Afterwards, the data processing is started and generates a rosbag file stored in a rosbags/ folder inside the replay_data directory. NOTE: If orientation is already given, change the fake_orientation parameter inside the config to prevent it from overwriting the data. Rosbags are named using the name of the JSON file used and timestamps (e.g., my_recording_20250722_153045 if the JSON file is named my_recording.json ). Existing rosbags are never overwritten . You can delete old rosbags manually from the rosbags/ folder if needed.","title":"Launching the Processing Step"},{"location":"simulation/visualize_real_data/#sending-data","text":"After processing the data, you can send it to RViz: Make sure you\u2019ve added the following displays to RViz: PointCloud2 MarkerArray Run: ros2 launch visualize_real_data send.launch.py or ./control.sh replay_rviz This command sends the most recently created rosbag to the topics defined in params.yaml . To play an earlier dataset, you must manually delete newer rosbags as the latest one (by timestamp) is always selected automatically.","title":"Sending Data"},{"location":"simulation/visualize_real_data/#adjusting-playback-speed","text":"You can modify the playback_rate parameter in params.yaml to control how quickly the rosbag is replayed. A value of 1 reflects real-time playback based on extracted timestamps. Slower or faster playback is supported, but extreme values haven't been tested .","title":"Adjusting Playback Speed"},{"location":"simulation/visualize_real_data/#fixing-rviz","text":"Usually the displays in RViz have to be configured to subscribe to the correct topics, and the PointCloud2 display's size has to match the image_scale defined in the params.yaml config file to look right. You can also change Alpha in the PointCloud display to see through the image. If the images aren't showing up, sometimes the QOS (Quality of Service) settings might be mismatched between the rosbag player and RViz . The intended QOS settings can be found in the recorder_qos.yaml file in the config folder together with the params.yaml file. These are the settings that the rosbag player uses, and the easiest fix is to make sure that RViz mirrors these settings for the respective displays.","title":"Fixing RViz"},{"location":"simulation/visualize_real_data/#summary","text":"Build package using CTRL + SHIFT + B VS Code task. Add the data you want to show in RViz in the replay_data folder Configure parameters under 'prepare_data', 'shared', and/or 'scenario_replayer' (optional) Run prepare.launch.py Open RViz2 and add the displays PointCloud2 and MarkerArray Make sure the QOS-settings match between the rosbag player and the displays Configure parameters under 'send_data' (optional) Run send.launch.py Subscribe to the relevant topics (as defined in params.yaml ) in RViz2 Now the data should be visualized.","title":"Summary"},{"location":"simulation/visualize_real_data/#warnings-and-errors-from-the-node","text":"Warnings and Errors thrown by this package are intended to inform you when something unexpected occurs during execution. In most cases, the process can continue without interruption, although the output may be affected. It is strongly recommended to resolve the underlying issue based on the warning/error message and then re-run the process to ensure reliable and consistent output.","title":"Warnings and Errors from the node"},{"location":"trajectories/","text":"python3 -m venv trajectory source trajectory/bin/activate pip install -r requirements.txt python -m ipykernel install --user --name=trajectory jupyter notebook --ip 0.0.0.0 --port 8888","title":"Index"}]}