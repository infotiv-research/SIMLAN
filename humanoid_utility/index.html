<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Hamid Ebadi" /><link rel="canonical" href="https://infotiv-research.github.io/SIMLAN/humanoid_utility/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Overview - SIMLAN, Simulation for Multi-Camera Robotics</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Overview";
        var mkdocs_page_input_path = "humanoid_utility/README.md";
        var mkdocs_page_url = "/SIMLAN/humanoid_utility/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/bash.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/python.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> SIMLAN, Simulation for Multi-Camera Robotics
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">SIMLAN, Simulation for Multi-Camera Robotics</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../dependencies/">Dependencies</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../contributing/">Contributing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../config_generation/">Config Generation</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Simulation</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/simlan_bringup/">SIMLAN Bringup</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/static_agent_launcher/">GPSS cameras</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/camera_bird_eye_view/">Camera Bird Eye View</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/aruco_localization/">Aruco Localization</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/bt_failsafe/">Failsafe</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/pallet_truck/pallet_truck_bringup/">Bringup</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/pallet_truck/">Pallet Truck</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/pallet_truck/pallet_truck_description/urdf/">URDF</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/pallet_truck/pallet_truck_control/">Control</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/pallet_truck/pallet_truck_navigation/">Navigation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/humanoid_robot/">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/humanoid_support_moveit_config/">Humanoid Moveit</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">Overview</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#pose-to-humanoid-motion">Pose to humanoid motion</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#terminology">Terminology:</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#dataset-generation">Dataset generation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#prebuild-datasets">Prebuild datasets</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#raw-dataset-shape">Raw dataset shape</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#loading-dataset">Loading Dataset</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#models">Models</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#pytorch">PyTorch</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#autogluon">AutoGluon</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#model-training">Model Training</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#model-evaluation">Model Evaluation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#model-prediction">Model Prediction</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#project-structure">Project Structure:</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#send-motion-commands">Send motion commands</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#model-structure">Model structure</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#the-theory">The Theory</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="pose_to_motion/autogluon/">Pose to Motion AutoGluon model</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/moveit2/">Panda MoveIt2</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/simlan_gazebo_environment/worlds/">Gazebo Worlds</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/raw_models/">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/raw_models/objects/">Object Modeling</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/raw_models/warehouse/">Warehouse Specification</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/object_mover/">Object Mover</a>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../credits/">Credits</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Appendix</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../CHANGELOG/">Changelog</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../resources/diagrams/">Diagrams</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">SIMLAN, Simulation for Multi-Camera Robotics</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Simulation</li>
      <li class="breadcrumb-item active">Overview</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/infotiv-research/SIMLAN/edit/master/docs/humanoid_utility/README.md">Edit on infotiv-research/SIMLAN</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="humanoid-motion-capture">Humanoid Motion Capture</h1>
<p>This project develops a system for translating human pose detection to humanoid robot motion in simulation environments. Using Google MediaPipe for pose landmark detection from camera input, the system maps detected human poses to corresponding joint movements executed by a humanoid robot in the Gazebo simulator. The implementation leverages ROS2 Ignition and MoveIt2 for motion planning and control, with a data generation pipeline that creates training pairs of pose landmarks and robot joint configurations. This approach provides a foundation for safety monitoring applications in industrial simulation (SIMLAN), where human pose analysis can be integrated for workplace incident detection. The work is based on a master's thesis "Human Motion Replay on a Simulated Humanoid Robot Using Pose Estimation" by Tove Casparsson and Siyu Yi, supervised by Hamid Ebadi, June 2025.</p>
<h2 id="pose-to-humanoid-motion">Pose to humanoid motion</h2>
<p>This project employs a deep neural network to learn the mapping between human pose and humanoid robot motion. The model takes 33 MediaPipe pose landmarks as input and predicts corresponding robot joint positions.</p>
<h2 id="terminology">Terminology:</h2>
<ul>
<li><strong>Forward Kinematics (FK)</strong>: The process of calculating the position and orientation of a robot's links given the values of its joint parameters (e.g., angles or displacements). In other words, FK answers the question: "Where is the robot's hand if I know all the joint values? (usually have one answer)"</li>
<li><strong>Inverse Kinematics (IK)</strong>: The process of determining the joint parameters (e.g., angles or displacements) required to achieve a desired position and orientation of the robot's links. In other words, IK answers the question: "What joint values will place the robot’s hand here? (usually have many answers)"</li>
<li><strong>Pose</strong> : 3D pose landmarks (MediaPipe) extracted by MediaPipe from a 2D image of human posture</li>
<li><strong>Motion</strong> : A kinematic instruction (joint parameters) sent to the robot (via MoveIt) for execution. While "kinematic instruction" would be a more accurate term, we continue to use "motion" for historical reasons (used in the word motion-capture), even though "motion" often refers to the difference/movement between two postures (e.g., posture2 - posture1).</li>
<li><strong>Motion Capture</strong>: Here it means using 2D images to find the motion (kinematic instruction/joint parameters) to instruct a humanoid robot to mimic the human posture.</li>
</ul>
<h2 id="dataset-generation">Dataset generation</h2>
<p>To build the synthetic dataset, first ensure that the cameras pointing to the humanoids are active in the <code>./config.sh</code>.</p>
<pre><code>CAMERA_ENABLED_IDS='500 501 502 503'
</code></pre>
<p>and rebuild the project with these commands:</p>
<pre><code>./control.sh build
</code></pre>
<p>The image below describes how the dataset generation system works.</p>
<p><img alt="Dataset generation overview" src="resources/dataset_generation.jpg" /></p>
<p>To create a humanoid dataset (paired pose data, motion data and reference images) in the <code>DATASET/TRAIN/</code> directory:</p>
<pre><code class="language-bash">./control.sh dataset TRAIN/
./control.sh dataset EVAL/
</code></pre>
<p>To find the implementation of how the dataset is created, go to <a href="./pre_processing/">pre_processing/</a> directory.</p>
<p>When the data is transformed into numpy/tabular format, each row in the CSV file represents <strong>multiple pose</strong> samples <strong>from each camera</strong> and its corresponding robot motion. The number of columns depends on the number of cameras, and it's possible to select what camera inputs you want to use; i.e., just one camera input or up to four camera inputs.</p>
<ul>
<li>The first columns are the 3D coordinates (x, y, z) for each of the 33 MediaPipe pose landmarks and <strong>for each camera</strong>, named like <code>cam500_0_x, cam501_0_x, cam502_0_x, cam503_0_x, cam500_0_y, cam501_0_y, cam502_0_y, cam503_0_y, cam500_0_z, cam501_0_z, cam502_0_z, cam503_0_z...</code>.</li>
<li>The remaining columns are the robot joint positions (motion targets) for that sample, with names like jRightShoulder_rotx, jLeftElbow_roty, etc.</li>
</ul>
<h2 id="prebuild-datasets">Prebuild datasets</h2>
<p>We use <code>20251028-DATASET-TRAINONLY-MULTI.zip</code> (place it in <code>DATASET</code> for training) and <code>20251028-DATASET-EVAL1000-MULTI.zip</code> (place it in <code>input/</code> directory for prediction) that are available at <a href="https://infotiv-my.sharepoint.com/:f:/r/personal/hamid_ebadi_infotiv_se/Documents/Humanoid?csf=1&amp;web=1&amp;e=M3xnWF">SharePoint</a> as our common datasets.</p>
<h2 id="raw-dataset-shape">Raw dataset shape</h2>
<pre><code>DATASET:
── TRAIN
    ├── camera_500
    │   ├── pose_data : 1.json , 2.json
    │   └── pose_images : 1.png , 2.png
    ├── camera_501
    │   ├── pose_data : 1.json , 1.json
    │   └── pose_images : 1.png, 2.png
    └── motion_data: 1.json, 2.json
</code></pre>
<h2 id="loading-dataset">Loading Dataset</h2>
<p>For training and prediction, we have a helper library that loads the data into these two primitive numpy arrays. All training must use these two data arrays with the following shape:</p>
<pre><code>motion_NP
-------------
scenario_id, motion_joint[1-47]
1, [JDATA_a]
2, [JDATA_b]

pose_NP
------------
scenario_id, cam_id, mp_landmark[1-99]
1, 500, [LDATA_a]
1, 501, [LDATA_b]
2, 500, [LDATA_c]
2, 501, [LDATA_d]
</code></pre>
<h2 id="models">Models</h2>
<h3 id="pytorch">PyTorch</h3>
<p>We have more control over the PyTorch model. It has its own implementation of dataset and model definition, located inside <a href="pose_to_motion/pytorch/utils.py">utils.py</a>. What is specific about using PyTorch is the possibility to use Optuna as a hyperparameter optimization tool. This tool helps find the best suitable set of hyperparameters given its training data. AutoGluon has its own internal optimization; thus we only use Optuna for PyTorch.</p>
<ul>
<li>Input Layer:  33 MediaPipe pose landmarks for each camera (x, y, z coordinates).</li>
<li>Hidden Layers: Multi-layer perceptron with pose normalization preprocessing</li>
<li>Output Layer: Robot joint position sequences for humanoid motion control, this results in a total of 47 joints to be outputted.</li>
<li>Training: Supervised learning on pose-motion paired datasets.</li>
</ul>
<h3 id="autogluon">AutoGluon</h3>
<p>We use <a href="https://auto.gluon.ai/stable/api/autogluon.tabular.TabularPredictor.html">Autogluon tabular predictor </a>. This model is trained as an ensemble (collection) of models where each separate model aims to predict a single joint given the complete input poses. The result from each model is then merged together and becomes a predicted list for each target joint.</p>
<h2 id="model-training">Model Training</h2>
<p>To train a model run the command below. The input size depends on the number of cameras you plan on using, meaning that if you only select one camera the input shape will take <strong>1</strong> camera into account. If you select <strong>4</strong> the model will have a <strong>4 times</strong> larger input size. To set it up in the way you want, go to <a href="config.sh"><code>config.sh</code></a>. The variables are:</p>
<ul>
<li><code>$model_instance</code>: If you want to reuse a model, specify its name here. Keep blank if you don't want to save.</li>
<li><code>$dataset_cameras</code>: The cameras you want to train on. This can be a list of cameras i.e. "500 501 502". For single training set this to one  ""</li>
<li><code>$model_type</code>: Possible selections: pytorch, autogluon</li>
</ul>
<p>Finally you specify what JSON directory you want to use as training data which becomes the second argument, below is an example of running train using the TRAIN/ dataset.</p>
<p>After a training run is complete, the model is saved inside of <code>pose_to_motion/{$model_type}/output/saved_model_states</code>. A summary report of the session is also generated and will be saved inside <code>pose_to_motion/{$model_type}/output/train_report.csv</code>.
Ensure that the <code>./config.sh</code> is properly configured.</p>
<pre><code># The cameras you want to train on. This can be a list of cameras i.e. &quot;500 501 502&quot;. For single training set this to one &quot;&quot;
dataset_cameras='500'

# Possible selections: pytorch, autogluon
model_type=&quot;pytorch&quot;

# If you want to reuse a model, specify its name here. Keep blank if you don't want to save.
model_instance=&quot;pytorch_test_pred_500&quot;

</code></pre>
<blockquote>
<p>Keep in mind that we use a separate virtual environment to install machine learning related pip packages called <code>mlenv</code> with a separate <a href="pose_to_motion/MLrequirements.txt"><code>requirements.txt</code></a>. Build the environment first using <code>./control.sh mlenv</code></p>
</blockquote>
<pre><code>
./control.sh mlenv
./control.sh train TRAIN/

</code></pre>
<h2 id="model-evaluation">Model Evaluation</h2>
<p>Running <code>control.sh eval</code> in the terminal runs the evaluation pipeline. This will trigger the selected <code>model_type</code>'s evaluation pipeline. To set it up in the way you want, go to <a href="config.sh"><code>config.sh</code></a>. The variables are:</p>
<ul>
<li><code>$model_instance</code>: If you want to reuse a model, specify its name here. Keep blank if you don't want to save.</li>
<li><code>$dataset_cameras</code>: The cameras you want to eval on. This can be a list of cameras i.e. "500 501 502". For single training set this to one  ""</li>
<li><code>$model_type</code>: Possible selections: pytorch, autogluon</li>
</ul>
<p>The evaluation will run metrics defined in <a href="pose_to_motion/metrics.py">metrics.py</a>; currently MSE and MAE are calculated. The results are then saved in a summary report located in <code>pose_to_motion/{$model_type}/outputs/eval_report.csv</code>.</p>
<p>To evaluate a trained model, there is a command that evaluates based on a set of metric. In the example below it evaluates on the EVAL/ JSON-dataset:</p>
<pre><code>
./control.sh eval EVAL/

</code></pre>
<p>As the result MSE and MAE values are printed.</p>
<h2 id="model-prediction">Model Prediction</h2>
<p>The input format needs to be structured as the template below. Please note the "dataset_name" can be whatever you want to describe the content of the data. The camera names are connected with the variable <code>dataset_cameras</code> inside <a href="/config.sh">config.sh</a>. Lastly, this folder needs to be placed inside the <a href="input/">input/</a> directory.</p>
<pre><code>DATASET_NAME_TEMPLATE_SOME_DESCRIPTOR/
    camera_500/
        file_1.(JSON, PNG, JPG, MP4)
        file_2.(JSON, PNG, JPG, MP4)
    camera_501/
        file_1.(JSON, PNG, JPG, MP4)
        file_2.(JSON, PNG, JPG, MP4)
    camera_502/
        file_1.(JSON, PNG, JPG, MP4)
        file_2.(JSON, PNG, JPG, MP4)
      camera_503/
        file_1.(JSON, PNG, JPG, MP4)
        file_2.(JSON, PNG, JPG, MP4)
</code></pre>
<p>The prediction pipeline uses these variables in the <code>config.sh</code>:</p>
<ul>
<li><code>humanoid_input_dir</code>: Input directory. Images or videos in this dir can be predicted.</li>
<li><code>humanoid_output_dir</code>: Output directory. pose files and predicted motions are placed here</li>
<li><code>dataset_cameras</code>: What cameras to take into concern for prediction.</li>
<li><code>model_instance</code>: What trained model instance to use.</li>
<li><code>replay_motion_namespace</code>: What humanoid you want to replay motions on.</li>
</ul>
<p>The prediction pipeline is implemented in a way where it expects the input dataset to follow the exact format as the model was trained on; see the note above for reference. It is possible to predict on three different types of data: <code>images</code>, <code>videos</code>, and raw <code>JSON</code> data. A good folder to put the data is inside the <code>input/</code> directory.</p>
<p>The input dataset gets processed into JSON format and is saved inside the <code>output/</code> directory, using <code>processes_input_data.py</code>. It creates the pose json files and MediaPipe images. <code>predict.py</code> inputs the pose json files and generates the predicted motions by saving them into a directory called <code>$humanoid_output_dir/dataset_name/predicted_motions/</code>. Running <code>control.sh predict $mode $dataset_name</code> inputs all poses for each camera in JSON format and for each pose_data outputs a predicted motion, which is replayable in RViz along with a ground truth pose_image. Finally, all motions are inputted sequentially into the motion viewer and visualized inside of Gazebo.</p>
<p>To run predict successfully, ensure that the variables in <code>config.sh</code> are set and place the data you want to predict inside <code>$humanoid_input_dir</code>. Then use the folder name of the dataset you want to predict as an argument along with the format of the data.</p>
<p>For testing make sure that</p>
<ul>
<li><code>input/TEST</code> (with DATASET_NAME_TEMPLATE_SOME_DESCRIPTOR explained above) is present (<strong><code>input/</code> and not <code>DATASET/</code></strong>)</li>
<li><code>input/VIDEOS/camera_500/20250611video.mp4</code></li>
</ul>
<p>To apply the predicted motion to your preferred humanoid adjust the <code>config.sh</code>:</p>
<pre><code class="language-bash">replay_motion_namespace=&quot;humanoid_2&quot;
dataset_cameras='500'
</code></pre>
<p>Finally run the command below:</p>
<pre><code>./control.sh predict TEST/
./control.sh predict VIDEOS/
./control.sh predict IMAGES/
</code></pre>
<h2 id="project-structure">Project Structure:</h2>
<ul>
<li>
<p><a href="humanoid_ml_requirements.txt"><code>humanoid_ml_requirements.txt</code></a>: Contains the humanoid machine learning requirements</p>
</li>
<li>
<p><a href="input"><code>input/</code></a> : This folder has the pose data to be predicted</p>
</li>
<li>
<p><a href="output"><code>output/</code></a> : This folder saves the predicted motion data and intermediate results</p>
</li>
<li>
<p><a href="DATASET/TRAIN"><code>/DATASET/TRAIN</code></a>: Contains motion, pose and image files generated by <code>./control.sh dataset TRAIN</code></p>
</li>
<li>
<p><code>motion_data</code> : Corresponding random motion (request)</p>
</li>
<li><code>camera/pose_data</code> : Mediapipe pose (result) for each camera</li>
<li>
<p><code>camera/pose_images</code>: Mediapipe annotated images and original images for each camera</p>
</li>
<li>
<p><a href="DATASET/EVAL"><code>/DATASET/EVAL</code></a>: Contains motion, pose and image files generated by <code>./control.sh dataset EVAL</code></p>
</li>
<li>
<p><code>motion_data</code> : Corresponding random motion (request)</p>
</li>
<li><code>camera/pose_data</code> : Mediapipe pose (result) for each camera</li>
<li>
<p><code>camera/pose_images</code>: Mediapipe annotated images and original images for each camera</p>
</li>
<li>
<p><a href="pre_processing"><code>pre_processing/</code></a>: contains all pre-processing files, used to convert the JSON data into tabular data. This will be the input of the model.</p>
</li>
<li>
<p><a href="process_input_data.py"><code>process_input_data.py</code></a>: Handles pose detection from actual images or videos using MediaPipe</p>
</li>
<li>
<p><a href="dataframe_converter.py"><code>dataframe_converter.py</code></a>: Contains helper functions to convert json files into dataframe/numpy format and is used by the models. Running its main will generate csv files that contains both pose and motion data from the json files. Inputs the cameras you want to use, the json dir, the csv filename to generate.</p>
</li>
<li>
<p><a href="pose_to_motion"><code>pose_to_motion/</code></a>: Contains the ML training pipeline for pose-to-motion prediction</p>
</li>
<li>
<p><a href="model.py"><code>model.py</code></a>: The main file containing implementations for training, evaluating, and predicting motions. This file acts as an orchestrator and inputs the data, what model to use, and what operation should be done. It is responsible for handling commands.</p>
</li>
<li>
<p><a href="pose_to_motion/autogluon"><code>autogluon/</code></a>: directory containing model implementation for autogluon model. Handles training, evaluation, and predicting motions.</p>
<ul>
<li><a href="pose_to_motion/autogluon/model.py"><code>framework.py</code></a>: Framework file, contains the implementation for train, evaluation, and prediction.</li>
<li><a href="pose_to_motion/autogluon/utils.py"><code>utils.py</code></a>: Utils file, contains helper methods, and model definition. The <code>model.py</code> use this.</li>
<li><a href="pose_to_motion/autogluon/output"><code>output/</code></a>: Contain reports of ran sessions and saved model states. Saved models will be found here</li>
<li><a href="pose_to_motion/autogluon/output/saved_model_states"><code>saved_model_states/</code></a>: Saved autogluon models are stored here</li>
<li><a href="pose_to_motion/autogluon/output/reports"><code>reports/</code></a>: Manually generated reports file containing training/evaluation run information. Good for reproducibility. The reports are generated by <code>generate_report.py</code> and the new report row is saved in <code>train_report.csv</code> or <code>eval_report.csv</code></li>
</ul>
</li>
<li>
<p><a href="pose_to_motion/pytorch"><code>pytorch/</code></a>: directory containing model implementation for pytorch model. Handles training, evaluation, and predicting motions.</p>
<ul>
<li><a href="pose_to_motion/pytorch/model.py"><code>framework.py</code></a>: Framework file, contains the implementation for train, evaluation, and prediction.</li>
<li><a href="pose_to_motion/pytorch/utils.py"><code>utils.py</code></a>: Utils file, contains helper methods, and model definition. The <code>model.py</code> use this.</li>
<li><a href="pose_to_motion/pytorch/output"><code>output/</code></a>: Contain reports of ran sessions and saved model states. Saved models will be found here</li>
<li><a href="pose_to_motion/pytorch/output/saved_model_states"><code>saved_model_states/</code></a>: Saved autogluon models are stored here</li>
<li><a href="pose_to_motion/pytorch/output/reports"><code>reports/</code></a>: Manually generated reports file containing training/evaluation run information. Good for reproducibility. The reports are generated by <code>generate_report.py</code> and the new report row is saved in <code>train_report.csv</code> or <code>eval_report.csv</code></li>
<li><a href="pose_to_motion/pytorch/output/optuna_studies"><code>optuna_studies/</code></a>: Specific for pytorch. Optuna studies are stored at this folder. See <a href="#optuna">optuna section</a> for more information about Optuna.</li>
</ul>
</li>
<li>
<p><a href="pose_to_motion/generate_report.py"><code>generate_report.py</code></a>:  Contains methods to generate report files, used after a training or evaluation session has been done for any selected model. These generated reports contain information for reproducing runs and how a model performed against metrics. A new report is generated every time we run train or eval on a model and every report is saved in a single CSV file where every row is a report. The files are named: <code>train_report.csv</code> or <code>eval_report.csv</code>.</p>
</li>
<li>
<p><a href="pose_to_motion/metrics.py"><code>metrics.py</code></a>: Contains all functions for calculating metrics for models for any selected model.</p>
</li>
<li>
<p><a href="pose_to_motion/humanoid_config.py"><code>humanoid_config.py</code></a>: it contains the humanoid configuration like the number of joints and the number of pose landmarks.</p>
</li>
</ul>
<h2 id="send-motion-commands">Send motion commands</h2>
<p>The package <a href="/simulation/random_motion_planner/">simulation/random_motion_planner/</a> allows us to execute motions to any humanoid robot using the MoveIt2 framework. It is set up in config.sh where you set the variable below, which is the robot namespace you want to control.</p>
<pre><code>replay_motion_namespace=&quot;humanoid_1&quot;
</code></pre>
<p>This package has two features. You can run it to execute a series of random motions which is useful when you want to create synthetic data. The other feature is that when the package is running for a given humanoid robot, you can send motions you want to execute directly. When the package runs it subscribes to a topic: <code>humanoid_X/execute_motion</code> which inputs a stringified message of a dict (joint_name, value).</p>
<p>The easiest way to use it is in python using a dictionary, stringifying it with json, and then publishing it to the topic:</p>
<pre><code>motion_dict = {JOINT_DATA}
json_str = json.dumps(motion_dict)
cmd = [
    &quot;ros2&quot;,
    &quot;topic&quot;,
    &quot;pub&quot;,
    &quot;--once&quot;,
    f&quot;{humanoid_namespace}/execute_motion&quot;,
    &quot;std_msgs/msg/String&quot;,
    f&quot;{{data: '{json_str}'}}&quot;,
]
subprocess.run(cmd, check=True)
</code></pre>
<h2 id="model-structure">Model structure</h2>
<p><code>model.py</code> is responsible for routing the training and prediction to the right model. When running <code>model.py train modelX=pytorch_model cams=[500,501]</code> from <code>./control.sh</code>, the pseudocode below in <code>model.py</code> loads the right model and passes the needed information for training and prediction to the model.</p>
<pre><code>import modelX
modelX.train(pose_NP, motion_NP, cams)
motion = modelX.train(pose_NP, cams)
pred= modelX.train(pose_NP, cams)
</code></pre>
<p>modelX can now have all information that it needs for training or prediction. Each model has to define at least the following interfaces:</p>
<pre><code>modelX.py:

def modelX.train():
    FL_POSE  = FLATTEN_POSE_BY_SCENARIO(pose_NP, cams)
    // 1, [LDATA_a], [LDATA_b]
    x = join(FL_POSE, motion_NP)
    // [LDATA_a], [LDATA_b], [JDATA_a]
    train with the data above
    SAVE_MODEL_AS(str(cams))

def modelX.predict(pose_NP, cams):
    FL_POSE  = FLATTEN_POSE_BY_SCENARIO(pose_NP, cams)
    LOAD_MODEL(str(cams), FL_POSE)
    WHATEVER
    return motion_np
</code></pre>
<p><img alt="" src="/resources/diagrams/humanoid_mocap_pipeline.drawio.png" /></p>
<h2 id="the-theory">The Theory</h2>
<ul>
<li><code>I</code> : an image. (<code>I_s</code>: from simulator, <code>I_r</code> from real world)</li>
<li><code>P</code> : a pose (mediapipe output)</li>
<li><code>M</code> : a motion (moveit2)</li>
</ul>
<p>then</p>
<ul>
<li><code>SIM(M) -&gt; I</code> : Simulator(gazebo) using inverse kinematics(moveit2) to convert the motion <code>M</code> to create image <code>I</code></li>
<li><code>PE(I) -&gt; P</code> : Pose estimator(mediapipe) takes the image <code>I</code>, to find human pose <code>P</code></li>
<li><code>Q(P) -&gt; M</code> : machine learning model <code>Q</code>, takes pose <code>P</code> and tries to replicate that pose estimator(mediapipe) pose using motion (moveit) <code>M</code></li>
</ul>
<p>Assumption: Pose estimator(mediapipe) performs good enough that it can detect human poses from both simulator and real-world domains:</p>
<ul>
<li><code>PE(I_r) -&gt; P_r</code></li>
<li><code>PE(I_s) -&gt; P_s</code></li>
</ul>
<p>Our goal is to</p>
<ul>
<li>pass random M to build the dataset of pairs :  <code>&lt;M,PE(SIM(M))&gt; = &lt;M,P&gt;</code></li>
<li>Use the dataset above to find <code>Q()</code> which is the inverse of this <code>PE(SIM())</code></li>
</ul>
<p>Now we can do motion capture (replicate real human movements) by <code>Q(PE(I_r))</code></p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../simulation/humanoid_support_moveit_config/" class="btn btn-neutral float-left" title="Humanoid Moveit"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="pose_to_motion/autogluon/" class="btn btn-neutral float-right" title="Pose to Motion AutoGluon model">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../simulation/humanoid_support_moveit_config/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="pose_to_motion/autogluon/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
