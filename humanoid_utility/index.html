<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Hamid Ebadi" /><link rel="canonical" href="https://infotiv-research.github.io/SIMLAN/humanoid_utility/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Humanoid Motion Capture - SIMLAN, Simulation for Multi-Camera Robotics</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Humanoid Motion Capture";
        var mkdocs_page_input_path = "humanoid_utility/README.md";
        var mkdocs_page_url = "/SIMLAN/humanoid_utility/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/bash.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/python.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> SIMLAN, Simulation for Multi-Camera Robotics
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">SIMLAN, Vision-Based Multi-Camera Robotics Simulation</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../dependencies/">Dependencies</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../contributing/">Contributing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../config_generation/">Config Generation</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Simulation</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/">Simulation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/simlan_gazebo_environment/worlds/">Gazebo Worlds</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/raw_models/">Models</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/raw_models/objects/">Object Modeling</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/raw_models/warehouse/">Warehouse Specification</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/simlan_bringup/">SIMLAN Bringup</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/static_agent_launcher/">GPSS cameras</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/camera_bird_eye_view/">Camera Bird Eye View</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/aruco_localization/">Aruco Localization</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/pallet_truck/pallet_truck_bringup/">Bringup</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/pallet_truck/">Pallet Truck</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/pallet_truck/pallet_truck_control/">Control</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/pallet_truck/pallet_truck_navigation/">Navigation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/bt_failsafe/">Failsafe</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/humanoid_robot/">Humanoid Robot</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/humanoid_support_moveit_config/">Humanoid Moveit</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">Humanoid Motion Capture</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#terminology">Terminology:</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#relevant-documents">Relevant Documents</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#dataset-generation">Dataset generation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#available-datasets">Available datasets</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#raw-dataset-shape">Raw dataset shape</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#cameras">Cameras</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#models">Models</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#pytorch">PyTorch</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#autogluon">AutoGluon</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#model-training">Model Training</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#model-evaluation">Model Evaluation</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#prediction-on-synthetic-data">Prediction on synthetic data</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#predicting-on-real-data-your-own-data">Predicting on real data/ your own data</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#project-structure">Project Structure:</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="pre_processing/">Humanoid Preprocessing</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="DEBUG/">Humanoid internal</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/motion_planner/">Humanoid motion planner</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="pose_to_motion/autogluon/">Pose to Motion AutoGluon model</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/moveit2/">Panda MoveIt2</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/object_mover/">Object Mover</a>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../credits/">Credits</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Appendix</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../ISSUES/">Issues</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../CHANGELOG/">Changelog</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../resources/diagrams/">Diagrams</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">SIMLAN, Simulation for Multi-Camera Robotics</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Simulation</li>
      <li class="breadcrumb-item active">Humanoid Motion Capture</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/infotiv-research/SIMLAN/edit/master/docs/humanoid_utility/README.md">Edit on infotiv-research/SIMLAN</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="humanoid-motion-capture">Humanoid Motion Capture</h1>
<p>This project develops a system for translating human pose detection to humanoid robot motion in simulation environments. Using Google MediaPipe for pose landmark detection from camera input, the system maps detected human poses to corresponding joint movements executed by a humanoid robot in the Gazebo simulator. The implementation leverages MoveIt2 for motion planning and control, with a data generation pipeline that creates training pairs of pose landmarks and robot joint configurations.</p>
<p>This project employs a deep neural network to learn the mapping between human pose and humanoid robot motion. The model takes 33 MediaPipe pose landmarks as input and predicts corresponding robot joint positions.</p>
<blockquote>
<p>The work is based on a master's thesis "Human Motion Replay on a Simulated Humanoid Robot Using Pose Estimation" by Tove Casparsson and Siyu Yi, supervised by Hamid Ebadi, June 2025.</p>
</blockquote>
<h2 id="terminology">Terminology:</h2>
<ul>
<li><strong>Forward Kinematics (FK)</strong>: The process of calculating the position and orientation of a robot's links given the values of its joint parameters (e.g., angles or displacements). In other words, FK answers the question: "Where is the robot's hand if I know all the joint values? (usually have one answer)"</li>
<li><strong>Inverse Kinematics (IK)</strong>: The process of determining the joint parameters (e.g., angles or displacements) required to achieve a desired position and orientation of the robot's links. In other words, IK answers the question: "What joint values will place the robot’s hand here? (usually have many answers)"</li>
<li><strong>Pose</strong> : 3D pose landmarks (MediaPipe) extracted by MediaPipe from a 2D image of human posture</li>
<li><strong>Motion</strong> : A kinematic instruction (joint parameters) sent to the robot (via MoveIt) for execution. While "kinematic instruction" would be a more accurate term, we continue to use "motion" for historical reasons (used in the word motion-capture), even though "motion" often refers to the difference/movement between two postures (e.g., posture2 - posture1).</li>
<li><strong>Motion Capture</strong>: Here it means using 2D images to find the motion (kinematic instruction/joint parameters) to instruct a humanoid robot to mimic the human posture.</li>
</ul>
<h2 id="relevant-documents">Relevant Documents</h2>
<ul>
<li><a href="DEBUG/">Debug</a> : for debugging purposes and development</li>
<li><a href="pre_processing/">Preprocessing</a> : details about pre processing of datasets</li>
<li><a href="/simulation/motion_planner/README.md">motion_planner</a> - To execute motions in the sim from a motion.json file.</li>
</ul>
<h2 id="dataset-generation">Dataset generation</h2>
<p>To build the synthetic dataset, first ensure that the cameras pointing to the humanoids are active in the <code>./config.sh</code>.</p>
<pre><code>CAMERA_ENABLED_IDS='500 501 502 503'
dataset_cameras='500 501 502 503'
</code></pre>
<p>and rebuild the project with these commands:</p>
<pre><code>./control.sh build
</code></pre>
<p>The image below describes how the dataset generation system works.</p>
<p><img alt="Dataset generation overview" src="resources/dataset_generation.jpg" /></p>
<p>To create a humanoid dataset (paired pose data, motion data and reference images) in the <code>DATASET/TRAIN/</code> directory:</p>
<p>As of now the dataset generation uses <code>humanoid_1</code> hardcoded. TODO: Fix so we can use "replay_motion_namespace".</p>
<pre><code class="language-bash">./control.sh dataset TRAIN/
./control.sh dataset EVAL/
</code></pre>
<p>To find the implementation of how the dataset is created i.e. mediapipe, go to <a href="pre_processing/media_to_pose_landmark.py">pre_processing/media_to_pose_landmark.py</a>.</p>
<h2 id="available-datasets">Available datasets</h2>
<p>We use <code>20251028-DATASET-TRAINONLY-MULTI.zip</code> (place it in <code>DATASET</code> for training) and <code>20251028-DATASET-EVAL1000-MULTI.zip</code> (place it in <code>input/</code> directory for prediction) that are available at <a href="https://infotiv-my.sharepoint.com/:f:/r/personal/hamid_ebadi_infotiv_se/Documents/Humanoid?csf=1&amp;web=1&amp;e=M3xnWF">SharePoint</a> as our common datasets.</p>
<h2 id="raw-dataset-shape">Raw dataset shape</h2>
<pre><code>DATASET:
── TRAIN
    ├── camera_500
    │   ├── pose_data : 1.json , 2.json
    │   └── pose_images : 1.png , 2.png
    ├── camera_501
    │   ├── pose_data : 1.json , 1.json
    │   └── pose_images : 1.png, 2.png
    └── motion_data: 1.json, 2.json
</code></pre>
<h2 id="cameras">Cameras</h2>
<p>Its important to know how the cameras are located. This is the camera setup for the cameras used for dataset creation, make sure to enable them in <code>config.sh</code>:</p>
<ul>
<li>500: Back side</li>
<li>501: Right side</li>
<li>502: Front side</li>
<li>503: Left side</li>
</ul>
<p>When selecting what cameras to use in "dataset_cameras" in <code>config.sh</code> it is important that the order remains the same. To avoid confusion always use the cameras in order: "500 501 502 503" meaning "Back Right Front Left".</p>
<h2 id="models">Models</h2>
<h3 id="pytorch">PyTorch</h3>
<p>We have more control over the PyTorch model. It has its own implementation of dataset and model definition, located inside <a href="pose_to_motion/pytorch/utils.py">utils.py</a>. What is specific about using PyTorch has the possibility to use Optuna is a hyperparameter optimization tool. This tool helps find the best suitable set of hyperparameters given its training data.</p>
<ul>
<li>Input Layer:  <strong>33</strong> MediaPipe pose landmarks for each camera (x, y, z coordinates).</li>
<li>Output Layer: Robot joint position sequences for humanoid motion control, this results in a total of 47 joints to be outputted.</li>
</ul>
<p>Currently the model hyperparameters (<code>HyperparameterDict</code>) in <code>humanoid_utility/pose_to_motion/pytorch/utils.py</code> are tuned using Optuna. Future model can use <a href="https://optuna.org/#code_examples">Optuna</a> for adjusting the  values.</p>
<h3 id="autogluon">AutoGluon</h3>
<p>We use <a href="https://auto.gluon.ai/stable/api/autogluon.tabular.TabularPredictor.html">Autogluon tabular predictor </a>. This model is trained as an ensemble (collection) of models where each separate model aims to predict a single joint given the complete input poses. The result from each model is then merged together and becomes a predicted list for each target joint.</p>
<h2 id="model-training">Model Training</h2>
<p>To train a model run the command below. The input size depends on the number of cameras you plan on using, meaning that if you only select one camera the input shape will take <strong>1</strong> camera into account. If you select <strong>4</strong> the model will have a <strong>4 times</strong> larger input size. To set it up in the way you want, go to <a href="config.sh"><code>config.sh</code></a>. The variables are:</p>
<ul>
<li><code>model_instance</code>: If you want to save a model, specify its name here. Keep blank if you don't want to save.</li>
<li><code>dataset_cameras</code>: The cameras you want to train on. This can be a list of cameras i.e. "500 501 502". For single training set this to one  ""</li>
<li><code>model_type</code>: Possible selections: "pytorch", "autogluon"</li>
</ul>
<blockquote>
<p>Keep in mind that we use a separate virtual environment to install machine learning related pip packages called <code>mlenv</code> with a separate <a href="pose_to_motion/MLrequirements.txt"><code>requirements.txt</code></a>. Build the environment first using <code>./control.sh mlenv</code>. This has to be done once and <code>./control.sh</code> enables the environment automatically</p>
</blockquote>
<p>Finally you specify what dataset directory (JSON) you want to use as training data which becomes the second argument, below is an example of running train using the <code>TRAIN/</code> dataset.</p>
<pre><code>./control.sh mlenv
./control.sh train TRAIN/
</code></pre>
<p>After a training run is complete, the model is saved inside of <code>pose_to_motion/{model_type}/output/saved_model_states</code>. A summary report of the session is also generated and will be saved inside <code>pose_to_motion/{model_type}/output/train_report.csv</code>.</p>
<h2 id="model-evaluation">Model Evaluation</h2>
<pre><code>./control.sh eval EVAL/
</code></pre>
<p>Running the command above in the terminal runs the evaluation pipeline.</p>
<p>The evaluation will run metrics defined in <a href="pose_to_motion/metrics.py">metrics.py</a>; currently MSE and MAE are calculated. The results are then saved in a summary report located in <code>pose_to_motion/{model_type}/outputs/eval_report.csv</code>.
As the result MSE and MAE values are printed.</p>
<h3 id="prediction-on-synthetic-data">Prediction on synthetic data</h3>
<p>The prediction pipeline uses these variables in the <code>config.sh</code>:</p>
<ul>
<li><code>dataset_cameras</code>: What cameras to take into concern for prediction. Should match the number of cameras the model was trained on.</li>
<li><code>model_instance</code>: What trained model instance to use.</li>
<li><code>replay_motion_namespace</code>: What humanoid you want to replay motions on. Default "humanoid_2".</li>
</ul>
<p>It is possible to predict on three different types of data: images, videos, and raw JSON data.</p>
<p>Run the command below to predict on different types of datasets:</p>
<pre><code>./control.sh predict JSON/ # This has the same &quot;EVAL/&quot;, &quot;TRAIN/&quot; directories.
./control.sh predict VIDEOS/ # As in input/VIDEOS/
./control.sh predict IMAGES/ # As in input/IMAGES/
</code></pre>
<p>The input for prediction needs to be structured as the template below.
The camera names are connected with the variable <code>dataset_cameras</code> inside <a href="/config.sh">config.sh</a>.</p>
<pre><code>input/JSON or VIDEOS or IMAGES
    camera_500/
        file_1.(JSON, PNG, JPG, MP4)
        file_2.(JSON, PNG, JPG, MP4)
    camera_501/
        file_1.(JSON, PNG, JPG, MP4)
        file_2.(JSON, PNG, JPG, MP4)
    camera_502/
        file_1.(JSON, PNG, JPG, MP4)
        file_2.(JSON, PNG, JPG, MP4)
      camera_503/
        file_1.(JSON, PNG, JPG, MP4)
        file_2.(JSON, PNG, JPG, MP4)
</code></pre>
<h3 id="predicting-on-real-data-your-own-data">Predicting on real data/ your own data</h3>
<p>The predict pipeline works on real data as well. It is for example possible to predict this dataset <a href="https://infotiv-my.sharepoint.com/:f:/r/personal/hamid_ebadi_infotiv_se/Documents/Humanoid?csf=1&amp;web=1&amp;e=M3xnWF">fit3d*.tar.gz</a> (or alternatively https://fit3d.imar.ro/fit3d.)</p>
<p>If you want to predict on your own videos and images you can follow these steps below. This example will use the gym exercise from 5 angles dataset from <a href="https://infotiv-my.sharepoint.com/:f:/r/personal/hamid_ebadi_infotiv_se/Documents/Humanoid?csf=1&amp;web=1&amp;e=M3xnWF">fit3d*.tar.gz</a> or alternatively https://fit3d.imar.ro/fit3d.</p>
<ol>
<li>This example assume you have a trained ML model on 4 cameras. Please follow <a href="humanoid_utility/README.md">humanoid_utility/README.md</a> for how to train a model.</li>
<li>download and extract the <a href="https://infotiv-my.sharepoint.com/:f:/r/personal/hamid_ebadi_infotiv_se/Documents/Humanoid?csf=1&amp;web=1&amp;e=M3xnWF">fit3d*.tar.gz</a> dataset.</li>
<li>Find a suitable video feed from multiple angles. You can find good examples in the folder "fit3d_train/train/s03/videos/".</li>
<li>In this folder you will find subfolders named "50591643", "58860488", "60457274", "65906101". These represent 4 different camera angles. It is the same when we name our "dataset_cameras" in <code>config.sh</code> as "500","501","502","503".</li>
<li>Going into any of the subfolders there are many different videos, this example uses "band_pull_apart.mp4"</li>
<li>add a new folder inside <code>input/</code> with whatever name you want. We will use the name <code>input/band_pull_apart/</code></li>
<li>For this guide we will rename the folders "50591643", "58860488", "60457274", "65906101" to "500","501","502","503".</li>
<li><strong>IMPORTANT</strong>: The angles for this example is not accurate since angles: "50591643" &amp; "58860488" both show back side and "60457274", "65906101" show the front. Thus there become a mismatch from our camera setup where we use back, right, front, left. The ML model will become confused by the angles if not taken into account.</li>
<li>Add subfolders inside <code>input/band_pull_apart/</code> named after the cameras and add respective video inside of it. This example has 4 videos:</li>
<li><code>band_pull_apart/camera_500/band_pull_apart.mp4</code></li>
<li><code>band_pull_apart/camera_50X/band_pull_apart.mp4</code></li>
<li>IMPORTANT: Make sure variable "dataset_cameras" match the names of the folders.</li>
<li>Now we can run the following command:</li>
</ol>
<pre><code>./control.sh predict band_pull_apart # based on input/band_pull_apart/
</code></pre>
<p>The videos will now be processed and the result is found in <code>output/band_pull_apart/</code>. Gazebo will also open and execute the motions from the videos.</p>
<h2 id="project-structure">Project Structure:</h2>
<ul>
<li>
<p><a href="pose_to_motion/humanoid_config.py"><code>humanoid_config.py</code></a>: it contains the humanoid configuration like the number of joints and the number of pose landmarks.</p>
</li>
<li>
<p><a href="humanoid_ml_requirements.txt"><code>humanoid_ml_requirements.txt</code></a>: Contains the humanoid machine learning requirements</p>
</li>
<li>
<p><a href="DATASET/TRAIN/"><code>/DATASET/TRAIN/</code></a>: Contains motion, pose and image files generated by <code>./control.sh dataset TRAIN</code></p>
</li>
<li>
<p><code>motion_data</code> : Corresponding random motion (request)</p>
</li>
<li><code>camera/pose_data</code> : Mediapipe pose (result) for each camera</li>
<li>
<p><code>camera/pose_images</code>: Mediapipe annotated images and original images for each camera</p>
</li>
<li>
<p><a href="DATASET/EVAL/"><code>/DATASET/EVAL/</code></a>: Same as <code>TRAIN/</code></p>
</li>
<li>
<p><a href="input"><code>input/</code></a> : This folder has the pose data to be predicted</p>
</li>
<li>
<p><a href="output"><code>output/</code></a> : This folder saves the predicted motion data and intermediate results</p>
</li>
<li>
<p><a href="pre_processing"><code>pre_processing/</code></a>: contains all pre-processing files, used to convert the JSON data into tabular data.</p>
</li>
<li>
<p><a href="media_to_pose_landmark.py"><code>media_to_pose_landmark.py</code></a>: Handles pose detection from actual images or videos using MediaPipe</p>
</li>
<li>
<p><a href="dataframe_json_bridge.py"><code>dataframe_json_bridge.py</code></a>: Contains helper functions to convert json files into dataframe/numpy format and is used by the models. Running its main will generate csv files that contains both pose and motion data from the json files. Inputs the cameras you want to use, the json dir, the csv filename to generate.</p>
</li>
<li>
<p><a href="pose_to_motion"><code>pose_to_motion/</code></a>: Contains the ML training pipeline for pose-to-motion prediction</p>
</li>
<li>
<p><a href="model.py"><code>model.py</code></a>: The main file containing implementations for training, evaluating, and predicting motions. This file acts as an orchestrator and inputs the data, what model to use, and what operation should be done.</p>
</li>
<li>
<p><a href="pose_to_motion/pytorch"><code>pytorch/</code></a>: directory containing model implementation for pytorch model. Handles training, evaluation, and predicting motions.</p>
<ul>
<li><a href="pose_to_motion/pytorch/model.py"><code>framework.py</code></a>: Framework file, contains the implementation for train, evaluation, and prediction.</li>
<li><a href="pose_to_motion/pytorch/utils.py"><code>utils.py</code></a>: Utils file, contains helper methods, and model definition. The <code>model.py</code> use this.</li>
<li><a href="pose_to_motion/pytorch/output"><code>output/</code></a>: Contain reports of ran sessions and saved model states. Saved models will be found here</li>
<li><a href="pose_to_motion/pytorch/output/saved_model_states"><code>saved_model_states/</code></a>: Saved autogluon models are stored here</li>
<li><a href="pose_to_motion/pytorch/output/reports"><code>reports/</code></a>: Manually generated reports file containing training/evaluation run information. Good for reproducibility. The reports are generated by <code>generate_report.py</code> and the new report row is saved in <code>train_report.csv</code> or <code>eval_report.csv</code></li>
<li><a href="pose_to_motion/pytorch/output/optuna_studies"><code>optuna_studies/</code></a>: Specific for pytorch. Optuna studies are stored at this folder. See <a href="#optuna">optuna section</a> for more information about Optuna.</li>
</ul>
</li>
<li>
<p><a href="pose_to_motion/autogluon"><code>autogluon/</code></a>: directory containing model implementation for autogluon model. Handles training, evaluation, and predicting motions. Has similar files as pytorch.</p>
<ul>
<li><a href="pose_to_motion/autogluon/model.py"><code>framework.py</code></a></li>
<li><a href="pose_to_motion/autogluon/utils.py"><code>utils.py</code></a></li>
<li><a href="pose_to_motion/autogluon/output"><code>output/</code></a></li>
<li><a href="pose_to_motion/autogluon/output/saved_model_states"><code>saved_model_states/</code></a></li>
<li><a href="pose_to_motion/autogluon/output/reports"><code>reports/</code></a></li>
</ul>
</li>
<li>
<p><a href="pose_to_motion/generate_report.py"><code>generate_report.py</code></a>:  Contains methods to generate report files, used after a training or evaluation session has been done for any selected model. These generated reports contain information for reproducing runs and how a model performed against metrics. A new report is generated every time we run train or eval on a model and every report is saved in a single CSV file where every row is a report. The files are named: <code>train_report.csv</code> or <code>eval_report.csv</code>.</p>
</li>
<li>
<p><a href="pose_to_motion/metrics.py"><code>metrics.py</code></a>: Contains all functions for calculating metrics for models for any selected model.</p>
</li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../simulation/humanoid_support_moveit_config/" class="btn btn-neutral float-left" title="Humanoid Moveit"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="pre_processing/" class="btn btn-neutral float-right" title="Humanoid Preprocessing">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../simulation/humanoid_support_moveit_config/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="pre_processing/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
