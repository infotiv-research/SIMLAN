<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Hamid Ebadi" /><link rel="canonical" href="https://infotiv-research.github.io/SIMLAN/humanoid_utility/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Overview - SIMLAN, Simulation for Multi-Camera Robotics</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Overview";
        var mkdocs_page_input_path = "humanoid_utility/README.md";
        var mkdocs_page_url = "/SIMLAN/humanoid_utility/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/bash.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/c.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> SIMLAN, Simulation for Multi-Camera Robotics
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">SIMLAN, Simulation for Multi-Camera Robotics</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../dependencies/">Dependencies</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../contributing/">Contributing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../config_generation/">Config Generation</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Simulation</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/simlan_gazebo_environment/worlds/">Gazebo Worlds</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/static_agent_launcher/">GPSS cameras</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/camera_bird_eye_view/">Camera Bird Eye View</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/aruco_localization/">Aruco Localization</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/bt_failsafe/">Failsafe</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/simlan_bringup/">SIMLAN Bringup</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">Pallet Truck</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../simulation/pallet_truck/">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../simulation/pallet_truck/pallet_truck_bringup/">Bringup</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../simulation/pallet_truck/pallet_truck_description/urdf/">URDF</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../simulation/pallet_truck/pallet_truck_control/">Control</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../simulation/pallet_truck/pallet_truck_navigation/">Navigation</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">Humanoid Robot</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../simulation/humanoid_robot/">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../simulation/humanoid_support_moveit_config/">Humanoid Moveit</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/moveit2/">Panda MoveIt2</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/object_mover/">Object Mover</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Humanoid Motion Capture Utility</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">Overview</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#terminology">Terminology:</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#dataset">Dataset</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#translation-of-a-detected-pose-to-humanoid-motion-command">Translation of a detected pose to humanoid motion command​</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#neural-network-design">Neural network design</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#project-structure">Project Structure:</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#dataset-generation">Dataset generation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#dataset-preprocessing">Dataset preprocessing</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#model-training">Model Training</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#autogluon">Autogluon</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#pytorch">Pytorch</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#optuna">Optuna</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#commands">Commands</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#model-evaluation">Model Evaluation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#model-prediction">Model Prediction</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#misc-ignore-what-comes-next">Misc. (IGNORE WHAT COMES NEXT)</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#notes">Notes</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="pose_to_motion/autogluon/">Pose to Motion AutoGluon model</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Models</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/raw_models/">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/raw_models/objects/">Object Modeling</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../simulation/raw_models/warehouse/">Warehouse Specification</a>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../credits/">Credits</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Appendix</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../CHANGELOG/">Changelog</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../resources/ISSUES/">Issues</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../resources/diagrams/">Diagrams</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">SIMLAN, Simulation for Multi-Camera Robotics</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Humanoid Motion Capture Utility</li>
      <li class="breadcrumb-item active">Overview</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/infotiv-research/SIMLAN/edit/master/docs/humanoid_utility/README.md">Edit on infotiv-research/SIMLAN</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="humanoid-motion-capture">Humanoid Motion Capture</h1>
<p>This project develops a system for translating human pose detection to humanoid robot motion in simulation environments. Using Google MediaPipe for pose landmark detection from camera input, the system maps detected human poses to corresponding joint movements executed by a humanoid robot in Gazebo simulator. The implementation leverages ROS2 Ignition and MoveIt2 for motion planning and control, with a data generation pipeline that creates training pairs of pose landmarks and robot joint configurations. This approach provides a foundation for safety monitoring applications in industrial simulation (SIMLAN), where human pose analysis can be integrated for workplace incident detection. The work is based on master thesis "Human Motion Replay on a Simulated Humanoid Robot Using Pose Estimation" by Tove Casparsson and Siyu Yi, Supervised by Hamid Ebadi, June 2025.</p>
<p><img alt="Pose translation" src="resources/pose_translation.jpg" /></p>
<h2 id="terminology">Terminology:</h2>
<ul>
<li><strong>Forward Kinematics (FK)</strong>: The process of calculating the position and orientation of a robot's links given the values of its joint parameters (e.g., angles or displacements). In other words, FK answers the question: "Where is the robot's hand if I know all the joint values? (usually have one answer)"</li>
<li><strong>Inverse Kinematics (IK)</strong>: The process of determining the joint parameters (e.g., angles or displacements) required to achieve a desired position and orientation of the robot's links. In other words, IK answers the question: "What joint values will place the robot’s hand here? (usually have many answers)"</li>
<li><strong>Pose</strong> : 3D pose landmarks (MediaPipe) extracted by Mediapipe from an 2D image of human posture</li>
<li><strong>Motion</strong> : A kinematic instruction (joint parameters) sent to the robot (via MoveIt) for execution. While "kinematic instruction" would be a more accurate term, we continue to use "motion" for historical reasons (used in the word motion-capture), even though , "motion" often refers to the difference/movement between two postures (e.g., posture2 - posture1).</li>
<li><strong>Motion Capture</strong>: Here it means using 2D images to find the motion(kinematic instruction/joint parameters) to instruct a humanoid robot to mimic the human posture.</li>
</ul>
<h2 id="dataset">Dataset</h2>
<p>To find the implementation of how the dataset is created, go to <a href="./pre_processing/">pre_prossessing/ directory</a>. We have two dataformats for multi and single camera prediction. </p>
<p>For the single-camera dataset, each row in the CSV file represents <strong>one pose</strong> sample and its corresponding robot motion from a specific camera.
- The first columns are the 3D coordinates (x, y, z) for each of the 33 MediaPipe pose landmarks, named like <code>cam500_0_x, cam500_0_y, cam500_0_z,...</code>.
- The remaining columns are the robot joint positions (motion targets) for that sample, with names like <code>jRightShoulder_rotx</code>, <code>jLeftElbow_roty</code>, etc.</p>
<p>For the multi-camera dataset, each row in the CSV file represents <strong>multiple poses</strong> samples <strong>from each camera</strong> and its corresponding robot motion.
- the first columns are the 3D coordinates (x, y, z) for each of the 33 MediaPipe pose landmarks and <strong>for each camera</strong>, named like <code>cam500_0_x, cam501_0_x, cam502_0_x, cam503_0_x, cam500_0_y, cam501_0_y, cam502_0_y, cam503_0_y, cam500_0_z, cam501_0_z, cam502_0_z, cam503_0_z...</code>.
- The remaining columns are the robot joint positions (motion targets) for that sample, with names like jRightShoulder_rotx, jLeftElbow_roty, etc.</p>
<h3 id="translation-of-a-detected-pose-to-humanoid-motion-command">Translation of a detected pose to humanoid motion command​</h3>
<p>This project employs a deep neural network to learn the mapping between human pose and humanoid robot motion. The model takes 33 MediaPipe pose landmarks as input and predicts corresponding robot joint positions.</p>
<h3 id="neural-network-design">Neural network design</h3>
<ul>
<li>Input Layer:  33 MediaPipe pose landmarks for each camera (x, y, z coordinates). </li>
<li>Hidden Layers: Multi-layer perceptron with pose normalization preprocessing</li>
<li>Output Layer: Robot joint position sequences for humanoid motion control, this results in a total of 47 joints to be outputed.</li>
<li>Training: Supervised learning on pose-motion paired datasets. </li>
</ul>
<h2 id="project-structure">Project Structure:</h2>
<ul>
<li><a href="humanoid_ml_requirements.txt"><code>humanoid_ml_requirements.txt</code></a>: Contains the humanoid machine learning requirements</li>
<li><a href="input"><code>input/</code></a> : This folder has the pose data to be predicted</li>
<li><a href="output"><code>output/</code></a> : This folder saves the predicted motion data and intermediate results</li>
<li><a href="DATASET_RAW/TRAIN"><code>/DATASET_RAW/TRAIN</code></a>: Contains motion, pose and image files generated by <code>./control.sh dataset TRAIN</code></li>
<li><code>motion_data</code> : Corresponding random motion (request)</li>
<li><code>camera/pose_data</code> : Mediapipe pose (result) for each camera</li>
<li><code>camera/pose_images</code>: Mediapipe annotated images and original images for each camera</li>
<li><a href="DATASET_RAW/EVAL"><code>/DATASET_RAW/EVAL</code></a>: Contains motion, pose and image files generated by <code>./control.sh dataset EVAL</code></li>
<li><code>motion_data</code> : Corresponding random motion (request)</li>
<li><code>camera/pose_data</code> : Mediapipe pose (result) for each camera</li>
<li><code>camera/pose_images</code>: Mediapipe annotated images and original images for each camera</li>
<li><a href="pre_processing"><code>pre_processing/</code></a>: contains all pre-processing files, used to convert the JSON data into tabular data. This will be the input of the model.</li>
<li><a href="mp_detection.py"><code>mp_detection.py</code></a>: Handles pose detection from actual images or videos using MediaPipe</li>
<li><a href="load_json_data.py"><code>load_json_data.py</code></a>: Build a csv file that contains both pose and motion data from the json files and outputs a default csv format.  </li>
<li>
<p><a href="csvtowide.py"><code>csvtowide.py</code></a>: Convert the default csv format into single and -multi camera csv files. (this data is the actual model input). </p>
</li>
<li>
<p><a href="pose_to_motion"><code>pose_to_motion/</code></a>: Contains the ML training pipeline for pose-to-motion prediction</p>
</li>
<li>
<p><a href="pose_to_motion/autogluon"><code>autogluon/</code></a>: directory containing model implementation for autogluon model. Handles training, evaluation, and predicting motions.</p>
<ul>
<li><a href="pose_to_motion/autogluon/model.py"><code>model.py</code></a>: Main file, contains the implementation for train, evaluation, and prediction.</li>
<li><a href="pose_to_motion/autogluon/utils.py"><code>utils.py</code></a>: Utils file, contains helper methods, and model definition. The <code>model.py</code> use this.</li>
<li><a href="pose_to_motion/autogluon/output"><code>output/</code></a>: Contain reports of ran sessions and saved model states. Saved models will be found here </li>
<li><a href="pose_to_motion/autogluon/output/saved_model_states"><code>saved_model_states/</code></a>: Saved autogluon models are stored here  </li>
<li><a href="pose_to_motion/autogluon/output/reports"><code>reports/</code></a>: Manually generated reports file containing training/evaluation run information. Good for reprodusability. The reports are generated by <code>generate_report.py</code> and the new report row is saved in <code>train_report.csv</code> or <code>eval_report.csv</code></li>
</ul>
</li>
<li>
<p><a href="pose_to_motion/pytorch"><code>pytorch/</code></a>: directory containing model implementation for pytorch model. Handles training, evaluation, and predicting motions.</p>
<ul>
<li><a href="pose_to_motion/pytorch/model.py"><code>model.py</code></a>: Main file, contains the implementation for train, evaluation, and prediction.</li>
<li><a href="pose_to_motion/pytorch/utils.py"><code>utils.py</code></a>: Utils file, contains helper methods, and model definition. The <code>model.py</code> use this.</li>
<li><a href="pose_to_motion/pytorch/output"><code>output/</code></a>: Contain reports of ran sessions and saved model states. Saved models will be found here </li>
<li><a href="pose_to_motion/pytorch/output/saved_model_states"><code>saved_model_states/</code></a>: Saved autogluon models are stored here  </li>
<li><a href="pose_to_motion/pytorch/output/reports"><code>reports/</code></a>: Manually generated reports file containing training/evaluation run information. Good for reprodusability. The reports are generated by <code>generate_report.py</code> and the new report row is saved in <code>train_report.csv</code> or <code>eval_report.csv</code></li>
<li><a href="pose_to_motion/pytorch/output/optuna_studies"><code>optuna_studies/</code></a>: Specific for pytorch. Optuna studies are stored at this folder. See <a href="#optuna">optuna section</a> for more information about Optuna.</li>
</ul>
</li>
<li><a href="pose_to_motion/generate_report.py"><code>generate_report.py</code></a>:  Contains methods to generate report files, used after a training or evaluation session has been done for any selected model. These generated reports contain information for reproducing runs and how a model performed against metrics. A new report is generated every time we run train or eval on a model and every report is saved in a single CSV file where every row is a report. The files are named: <code>train_report.csv</code> or <code>eval_report.csv</code>.</li>
<li><a href="pose_to_motion/metrics.py"><code>metrics.py</code></a>: Contains all functions for calculating metrics for models for any selected model. </li>
<li><a href="pose_to_motion/humanoid_config.py"><code>humanoid_config.py</code></a>: it contains the humanoid configuration like the number of joints and the number of pose landmarks.</li>
</ul>
<h2 id="dataset-generation">Dataset generation</h2>
<p>The below image describes how the dataset generation system works.</p>
<p><img alt="Dataset generation overview" src="resources/dataset_generation.jpg" /></p>
<p>To create a humanoid dataset ( paired pose data, motion data and reference images) in the <code>DATASET_RAW/TRAIN</code> directory:</p>
<pre><code class="language-bash">./control.sh dataset TRAIN/
</code></pre>
<h2 id="dataset-preprocessing">Dataset preprocessing</h2>
<p>The diagram below shows the overall workflow of this project. Pose and motion data are first collected from JSON files and combined into a single CSV file. These processed csv files are ready to be fed into a model of your choice, Autogluon or pytorch.  See below to find more information about the dataset or more about the <a href="#models">models</a>. </p>
<p><img alt="Overall workflow" src="resources/ML.drawio.png" /></p>
<blockquote>
<p>The dataset, input and outputs are in <code>humanoid_utility/DATASET_RAW/</code> directory.</p>
</blockquote>
<p>The resulting files are stored as parallel <code>.json</code> files in multi-camera folders <code>camera_*/pose_data</code>, <code>camera_*/motion_data</code>, <code>camera_*/pose_images</code>.
To avoid creation of <code>pose_images</code> that are only used as the ground truth and debugging (specially if you are building your training data), comment out then call to <code>self.save_pose_image()</code> in <code>camera_viewer.py</code>.</p>
<p>Finally to merge them all in tabular <code>.csv</code> file run the following command.</p>
<pre><code>./control.sh convert2csv
</code></pre>
<p>Results in merging:</p>
<ul>
<li><code>DATASET_PROCESSED/train.csv</code> to <code>DATASET_PROCESSED/TRAIN/single_train_cam501.csv</code></li>
<li><code>DATASET_PROCESSED/train.csv</code> to <code>DATASET_PROCESSED/TRAIN/single_train_cam502.csv</code></li>
<li><code>DATASET_PROCESSED/train.csv</code> to <code>DATASET_PROCESSED/TRAIN/single_train_cam500.csv</code></li>
<li><code>DATASET_PROCESSED/train.csv</code> to <code>DATASET_PROCESSED/TRAIN/single_train_cam503.csv</code></li>
<li><code>DATASET_PROCESSED/train.csv</code> to <code>DATASET_PROCESSED/TRAIN/multi_train.csv</code></li>
<li><code>DATASET_PROCESSED/eval.csv</code> to <code>DATASET_PROCESSED/EVAL/single_eval_cam500.csv</code></li>
<li><code>DATASET_PROCESSED/eval.csv</code> to <code>DATASET_PROCESSED/EVAL/single_eval_cam501.csv</code></li>
<li><code>DATASET_PROCESSED/eval.csv</code> to <code>DATASET_PROCESSED/EVAL/single_eval_cam502.csv</code></li>
<li><code>DATASET_PROCESSED/eval.csv</code> to <code>DATASET_PROCESSED/EVAL/single_eval_cam503.csv</code></li>
<li><code>DATASET_PROCESSED/eval.csv</code> to <code>DATASET_PROCESSED/EVAL/multi_eval.csv</code></li>
</ul>
<p>You can replay_motion each motion data separately: <code>./control.sh replay_motion DATASET_PROCESSED/EVAL/motion_data/AAAAAAA_motion.json</code></p>
<h2 id="model-training">Model Training</h2>
<h3 id="autogluon">Autogluon</h3>
<p>We use <a href="https://auto.gluon.ai/stable/api/autogluon.tabular.TabularPredictor.html">Autogluon tabular predictor </a>. This model is trained as an ensemble (collection) of models where each separate model aims to predict a single joint given the complete input poses. The result from each model is then merged together and becomes a predicted list for each target joint. </p>
<h3 id="pytorch">Pytorch</h3>
<p>We have more control over the Pytorch model. It has its own implementation of its dataset and model definition, located inside of <a href="pose_to_motion/pytorch/utils.py">its utils dir</a> folder. What is specific about using the pytorch is the possibility to use optuna as a hyperparameter optimization tool. This tools help find the best suitable set of hyperparams given its training data. Autogluon has its own internal optimization thus we only use this for pytorch. </p>
<h2 id="optuna">Optuna</h2>
<p>Optuna is a hyperparameter optimization framework available as a python library. It acts as a wrapper around an existing training framework where it uses the same setup of dataset, train_loop and hyperparameters, so it has the same context as when you would run it yourself. Optuna starts by defining an objective function. The goal of the objective function is either to minimize or maximize a value. You specify this value yourself, for this setup we have typically used the validation MSE to use as a goal to minimize. We also need to define the scope of values for the hyperparameters. We define for every hyperparameter, a range of values which Optuna is allowed to select from. i.e. BATCHSIZE being in range(4,128). When we have done this for every param we want to find the optimal value for, we start the start optuna by running a "study". Optuna runs N number of training sessions, each independent and uses internal algorithms to find the best set of values for the list of hyperparameters. In the end we get the training session that scored the best and can use its hyperparameters.  </p>
<p>Note: We only use this for pytorch since autogluon has its own hyperparameter optimization. This is why we only see studies inside <code>pytorch/optuna_studies/</code>. </p>
<h3 id="commands">Commands</h3>
<blockquote>
<p>Keep in mind that we use a separate virtual environment to install machine learning related pip packages called <code>mlenv</code>  with a separate <a href="pose_to_motion/MLrequirements.txt"><code>requirements.txt</code></a>. Build the environment first using <code>./control.sh mlenv</code></p>
</blockquote>
<p>Running <code>control.sh single_train</code> or <code>control.sh multi_train</code> in the terminal runs the training pipeline. Single train expects data from 1 camera source whereas multi train can use unlimited amount of camera input. Both models use 1 target set of joints. </p>
<p>The interface to modify the runtime arguments, you modify variables inside of <code>config.sh</code>. The runtime variables you can change are these:
- <strong>model_type</strong>. Possible selections: pytorch, autogluon
- <strong>pose_to_motion_model_name</strong>. If you want to reuse a model, specify its name here. Keep blank if you dont want to save.
- <strong>pose_to_motion_session_name</strong>. Optional if you want to describe your session.</p>
<p>After a training run is complete, the model is saved inside of <code>pose_to_motion/{$model_type}/output/saved_model_states</code>. A summary report of the session is also generated and will be saved inside <code>pose_to_motion/{$model_type}/output/train_report.csv</code></p>
<p>There are two options for using the data and training a model. The resulting model is saved in <code>{$model_type}/output/saved_model_states</code>. Training a model using 1 camera source, reads data from <code>DATASET_PROCESSED/TRAIN/single_train_cam5XX.csv</code> and trains our model.</p>
<pre><code>./control.sh single_train
</code></pre>
<p>Training a model using multiple camera sources, reads data from <code>DATASET_PROCESSED/TRAIN/multi_train.csv</code> and trains our model:</p>
<pre><code>./control.sh multi_train
</code></pre>
<h2 id="model-evaluation">Model Evaluation</h2>
<p>Running <code>control.sh eval</code> in the terminal runs the evaluation pipeline. This will trigger the selected <code>model_type</code>'s evaluation pipeline. The same run time variables found in <code>config.sh</code>, used for training is used for evaluation where the key variables <strong>model_type</strong> and <strong>pose_to_motion_model_name</strong>. </p>
<p>The evaluation will run metrics defined inside of <a href="pose_to_motion/metrics.py">metrics.py</a>, currently MSE and MAE will be calculated. The results is then saved in a summary report localted in <code>pose_to_motion/{$model_type}/outputs/eval_report.csv</code>. </p>
<p>To evaluate a trained model, there is a command that evaluates based on a set of metric:</p>
<pre><code>./control.sh eval
</code></pre>
<h2 id="model-prediction">Model Prediction</h2>
<p>Running <code>control.sh predict</code> will run input all poses for each camera, in JSON format and for each pose_data, output a predicted motion, which is replayable in RViz along with a ground truth pose_image.   </p>
<p>The following command uses MediaPipe to create pose data from an image or a video in <code>input/</code> to populate <code>output/</code> with mediapipe landmark pose and images.</p>
<pre><code>./control.sh image_pipeline
./control.sh video_pipeline
</code></pre>
<p>To use the model on each generated pose, run the following command to generate predicted motions which will be saved inside <code>predict_data/</code>, inside of the input folder. </p>
<pre><code>./control.sh single_prediction output
</code></pre>
<p>To use the model on each pose inside of any <code>DATASET_RAW</code> directory for example: <code>DATASET_RAW/EVAL/camera_500/pose_data</code>, run the following command below for single camera using the /EVAL dataset and camera 500 as input:</p>
<pre><code>./control.sh single_prediction DATASET_RAW/EVAL
</code></pre>
<p>This will save the predicted motions to <code>DATASET_RAW/EVAL/predict_data</code>. Afterwards it opens up a image viewer with the ground truth mediapipe detection as well to quickly check the performance of our ml model:</p>
<p>To use it for multi camera use the command below to predict motions. This works the same as <code>single_prediction</code> but instead use multi-camera datasets.</p>
<pre><code>./control.sh multi_prediction DATASET_RAW/EVAL
</code></pre>
<h2 id="misc-ignore-what-comes-next">Misc. (IGNORE WHAT COMES NEXT)</h2>
<h3 id="notes">Notes</h3>
<ul>
<li>It is a "feature" (not a bug) that some cameras cannot detect the pose (NaN values). It helps to be still able to detect pose when a part of body is masked.</li>
<li>Mediapipe cannot reliable detect the depth, back and front</li>
<li>Experiment different options for NaN values (average of other rows, zero, etc)</li>
</ul>
<h1 id="preprocessing">Preprocessing</h1>
<p><img alt="pose_landmarks_index.png" src="resources/pose_landmarks_index.png" /></p>
<p>https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker</p>
<p>Three steps of normalization (feature engineer) of mediapipe landmarks:
- Position normalization
- Scale normalization
- Rotation normalization</p>
<p><img alt="normalization.png" src="resources/Normalisation.drawio.png" /></p>
<h2 id="issues-and-future-works">Issues and Future Works</h2>
<h3 id="current-performance-metrics">Current Performance Metrics</h3>
<p>The upgraded multi-camera system demonstrates the following evaluation results:
- <strong>Mean Squared Error (MSE)</strong>: 0.206
- <strong>Root Mean Squared Error (RMSE)</strong>: 0.454</p>
<h3 id="known-limitations">Known Limitations</h3>
<ul>
<li>MediaPipe exhibits difficulty distinguishing between front and back orientations, leading to potential left/right body side confusion in pose detection</li>
<li>Current preprocessing includes normalization for single-camera data (using hip midpoint as origin), but multi-camera normalization requires further development</li>
<li>Depth estimation reliability remains limited in MediaPipe's 2D-to-3D pose conversion</li>
</ul>
<h3 id="future-development-areas">Future Development Areas</h3>
<ul>
<li>Enhance multi-camera data preprocessing with normalization, also probably replace the <code>pose_landmarks</code> into <code>pose_world_landmarks</code> in <code>camera_viewer.py</code> </li>
<li>Replace pre-processing with pose-processing, and pay attention to whether Mediapipe's <code>static mode</code> is enabled.</li>
<li>Consider applying feature engineering to enable the model to gain a deeper and more accurate understanding of the data. </li>
</ul>
<h2 id="the-theory">The Theory</h2>
<ul>
<li></li>
<li><code>I</code> : an image. (<code>I_s</code>: from simulator, <code>I_r</code> from real world)</li>
<li><code>P</code> : a pose (mediapipe output)</li>
<li><code>M</code> : a motion (moveit2)</li>
</ul>
<p>then
- <code>SIM(M) -&gt; I</code> : Simulator(gazebo) using inverse kinematics(moveit2) to convert the motion <code>M</code> to create image <code>I</code>
- <code>PE(I) -&gt; P</code> : Pose estimator(mediapipe) takes the image <code>I</code>, to find human pose <code>P</code>
- <code>Q(P) -&gt; M</code> : machine learning model <code>Q</code>, takes pose <code>P</code> and tries to replicate that pose estimator(mediapipe) pose using motion (moveit) <code>M</code></p>
<p>Assumption: Pose estimator(mediapipe) performs good enough that it can detect human poses from both simulator and real-world domains:
- <code>PE(I_r) -&gt; P_r</code>
- <code>PE(I_s) -&gt; P_s</code></p>
<p>Our goal is to
- pass random M to build the dataset of pairs :  <code>&lt;M,PE(SIM(M))&gt; = &lt;M,P&gt;</code> 
- Use the dataset above to find <code>Q()</code> which is the inverse of this <code>PE(SIM())</code></p>
<p>No we can do motion capture (replicate real human movements) by <code>Q(PE(I_r))</code> </p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../simulation/object_mover/" class="btn btn-neutral float-left" title="Object Mover"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="pose_to_motion/autogluon/" class="btn btn-neutral float-right" title="Pose to Motion AutoGluon model">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../simulation/object_mover/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="pose_to_motion/autogluon/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
